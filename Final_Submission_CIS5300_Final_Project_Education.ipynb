{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkhtAfcljF6z"
      },
      "source": [
        "# **CIS5300 Final Project:** *From Bart to Edge - Education*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djRg9FI4p18L"
      },
      "source": [
        "## **1. Load and Preprocess (Must Run!)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8WI_aBQwFJ3"
      },
      "source": [
        "### **1.1 Download Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGQnkdD2u302",
        "outputId": "c412a018-ccb1-4572-e10e-5fc4e67a389b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting optimum\n",
            "  Downloading optimum-1.23.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting auto-gptq\n",
            "  Downloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Collecting coloredlogs (from optimum)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from optimum) (1.13.1)\n",
            "Requirement already satisfied: transformers>=4.29 in /usr/local/lib/python3.10/dist-packages (from optimum) (4.46.3)\n",
            "Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from optimum) (2.5.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.1.1)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.2.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.5)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.13.2)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->optimum) (3.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->optimum) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.29->optimum) (0.20.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->optimum) (3.0.2)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimum-1.23.3-py3-none-any.whl (424 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.1/424.1 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading auto_gptq-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gekko-1.2.1-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, sacremoses, rouge, portalocker, humanfriendly, gekko, fsspec, dill, colorama, sacrebleu, multiprocess, coloredlogs, bitsandbytes, datasets, optimum, auto-gptq\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed auto-gptq-0.7.1 bitsandbytes-0.45.0 colorama-0.4.6 coloredlogs-15.0.1 datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 gekko-1.2.1 humanfriendly-10.0 multiprocess-0.70.16 optimum-1.23.3 portalocker-3.0.0 rouge-1.0.1 sacrebleu-2.4.3 sacremoses-0.1.1 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets optimum auto-gptq sentencepiece bitsandbytes sacremoses sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njsj37jhsbTY",
        "outputId": "7256eff2-11ed-4d13-f5f2-3291e93457a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prKdB5wTwPme"
      },
      "source": [
        "### **1.2 Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sykKxOCOp1j7"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import random\n",
        "import torch\n",
        "from torch.ao.quantization import (\n",
        "    prepare_qat,\n",
        "    convert,\n",
        "    QConfig,\n",
        "    default_observer,\n",
        "    default_per_channel_weight_observer,\n",
        "    get_default_qat_qconfig,\n",
        "    default_weight_observer\n",
        ")\n",
        "\n",
        "from transformers import MBart50Tokenizer, MBartForConditionalGeneration, M2M100Tokenizer, M2M100ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, TrainerCallback, AutoModelForCausalLM, AutoTokenizer, pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding, GenerationConfig\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "from itertools import product\n",
        "import psutil\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import threading\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_237ElHjwXeo"
      },
      "source": [
        "### **1.3 Download and Initialize Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pw23gEf8jTA"
      },
      "source": [
        "#### **1.3.1 mBart**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246,
          "referenced_widgets": [
            "ff9faaa4372d40beaa2a0d0379718a0f",
            "237bd9946efd4341a6e737f90619415a",
            "ff7fc8fad35e4bc4b5af6a66b02eeab0",
            "5239a3204f50437ebb9660e77889d360",
            "0ef3082589fe48e9b5115130f8d8170e",
            "8932ac5b234b49b9b16ce7387312d1ca",
            "2de78c4b2daf4434b269beb08fedf2b8",
            "6418a89bb07449d3986fb0985d0b0d91",
            "f16c2010a4cc4f078d864e06a29b2705",
            "9935c83b26de424bb8f16a38514fc549",
            "ec0fc12c1e46475fa8db9a2e9079a793",
            "940ba0154c42408a95c031dfdedc6fc7",
            "b1436bb01a074f5ea26d88d1f3f8ea99",
            "b97c38b90de242979b0f58f63302cbd4",
            "af7df096521e4df89427ead7b509db3c",
            "5bac5314524242719b10b66c56285fb7",
            "48608fc7fd874e84a7f09b6eaa1f03d3",
            "2c37188e2ad64e4f800b7fade3946d76",
            "17a750d1c77c41609b83822f8297f161",
            "5f4d477e70b04a458c9920401aaa4baf",
            "002770a697294cd2a1e62ad9ebb56f0f",
            "f33c8631b8a541c496a0cd096c8583a5",
            "d2e19031aa804962a444cdf147a61337",
            "9b310f903a104c0689a057b53a1884aa",
            "fe730cae5da84d12ae15061c7dfc6dce",
            "87e418e793af4804b4de3b745521baa9",
            "6c598d02efab4b74a9eebdfbfc0c77cf",
            "64ba765e64f4444cadcada2c98c37424",
            "e128e776b30a485b97bbec619a18edf4",
            "62aa05a2bf9c4d00966c87e6d4a04bbe",
            "c1e2ca8d05984c42bef78914d63a0b58",
            "c085a6ca4c134ca7a141ed9d17c26583",
            "11f06f842c274672aec268b7b8a1bde3",
            "5068215f603144bf992b9646ce7e7f36",
            "1893af19547248e5a5c1d8f3263502ec",
            "457d792d667e4e07a5e0b59090b4daaf",
            "e9cf9a5ccbc543b49417ff8b3908b811",
            "0946552c0e9e4d1e8a7dceecbfeebd66",
            "5df9ea3969824c11b6b80583197649c0",
            "c29bf138bef446e6bc23bf794ad7e97f",
            "110a246bcdf74da598f3b58bdadb33b1",
            "91f81fe060674705b1d06053069c84a1",
            "85fd8a9632fb4e8294cdfea09c3894ab",
            "8442e1702c114f16abb5b2d2bf87c33f",
            "1e417de88dc94e84a224b85173154e13",
            "f48780158e314db5a2f1a0a54751f034",
            "8576425312624e3681aa02e18744428d",
            "a278515e304e439e96873a80009734af",
            "9e011d5071cb4136bd358fc6eb4036f9",
            "9af86396852a41bdb9af03cbc5e6abae",
            "a9bfe5364ca0430390a15f6d8241b333",
            "db51b11fe06c4234b0773f4e354b00b4",
            "aca03c1006944492bebbb331c64c1a41",
            "eb037255f7ad4907962ebb141696eefe",
            "293eec9edcad4b02880f2367ca370f3d",
            "0ca9d1a2be2945fe8492088c529f21c9",
            "536d1b905a8b4e328125bfa37aee69c7",
            "25c10fbc8a504e50b6e8859ebe172753",
            "dd57a84a69f149a1ba7ab9feb8d66c3a",
            "3491cd83bc924065a32138c4af6f8a6c",
            "2b0da7e5d0ad4af6bae9757dec680f7e",
            "e0904ce568ea43939edcea1745630a51",
            "913c744b886b433ea1ad7c7718b82972",
            "5467128fefd748b7b9c8aebba9b64a16",
            "171cced3c611442784354d9f94f5c278",
            "1b0d3f7633c54f99b2ef3ba24deeae98"
          ]
        },
        "id": "24zK1v_srGwN",
        "outputId": "7e625c83-8724-4569-8807-45dc6c59bf97"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff9faaa4372d40beaa2a0d0379718a0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "940ba0154c42408a95c031dfdedc6fc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2e19031aa804962a444cdf147a61337",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5068215f603144bf992b9646ce7e7f36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e417de88dc94e84a224b85173154e13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ca9d1a2be2945fe8492088c529f21c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mBart and tokenizers initialized successfully.\n"
          ]
        }
      ],
      "source": [
        "# Model names\n",
        "mbart_model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "\n",
        "# Source and target languages\n",
        "SRC_LANG = \"zh_CN\"\n",
        "TGT_LANG = \"en_XX\"\n",
        "\n",
        "# Initialize mBART model and tokenizer\n",
        "tokenizer_mbart = MBart50Tokenizer.from_pretrained(mbart_model_name)\n",
        "model_mbart = MBartForConditionalGeneration.from_pretrained(mbart_model_name)\n",
        "tokenizer_mbart.src_lang = SRC_LANG\n",
        "tokenizer_mbart.tgt_lang = TGT_LANG\n",
        "\n",
        "# Ensure all parameters in the original model are trainable\n",
        "for param in model_mbart.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "# LoRA configuration for mBART\n",
        "mbart_lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,  # Low-rank dimension\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.0,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "# Apply LoRA to mBART model\n",
        "model_mbart_lora = copy.deepcopy(model_mbart)\n",
        "model_mbart_lora = get_peft_model(model_mbart_lora, mbart_lora_config)\n",
        "\n",
        "# Verify that only LoRA-specific layers are trainable\n",
        "for name, param in model_mbart_lora.named_parameters():\n",
        "    if any(layer in name for layer in mbart_lora_config.target_modules):\n",
        "        param.requires_grad = True  # LoRA layers should be trainable\n",
        "    else:\n",
        "        param.requires_grad = False  # Non-LoRA layers should be frozen\n",
        "\n",
        "\n",
        "# mBart with Layer Freezing\n",
        "model_mbart_freeze = copy.deepcopy(model_mbart)\n",
        "\n",
        "# Layer Freezing Configuration\n",
        "freeze_encoder_layers = 8  # Number of encoder layers to freeze\n",
        "freeze_decoder_layers = 8  # Number of decoder layers to freeze\n",
        "\n",
        "# Apply Layer Freezing to the base mBART model\n",
        "for name, param in model_mbart_freeze.named_parameters():\n",
        "    # Freeze encoder layers\n",
        "    if \"encoder.layers\" in name:\n",
        "        layer_num = int(name.split(\".\")[3])  # Extract layer index\n",
        "        if layer_num < freeze_encoder_layers:\n",
        "            param.requires_grad = False  # Freeze parameters\n",
        "    # Freeze decoder layers\n",
        "    elif \"decoder.layers\" in name:\n",
        "        layer_num = int(name.split(\".\")[3])  # Extract layer index\n",
        "        if layer_num < freeze_decoder_layers:\n",
        "            param.requires_grad = False  # Freeze parameters\n",
        "    else:\n",
        "        param.requires_grad = True  # Keep other parameters trainable\n",
        "\n",
        "\n",
        "print(\"mBart and tokenizers initialized successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFo0H42qtLyV"
      },
      "outputs": [],
      "source": [
        "def print_model_parameters(model, model_name=\"Model\"):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Print summaries\n",
        "    print(f\"{model_name} - Total Parameters: {total_params}\")\n",
        "    print(f\"{model_name} - Trainable Parameters (All): {trainable_params}\")\n",
        "    print(f\"{model_name} - Percentage of Parameters to Fine-tune (All): {100 * trainable_params / total_params:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfM1I_XerqQO",
        "outputId": "538c49c0-2143-4985-9694-61d9b83d7394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mBART (original) - Total Parameters: 610879488\n",
            "mBART (original) - Trainable Parameters (All): 610879488\n",
            "mBART (original) - Percentage of Parameters to Fine-tune (All): 100.00%\n",
            "\n",
            "mBART (LoRA) - Total Parameters: 612059136\n",
            "mBART (LoRA) - Trainable Parameters (All): 76750848\n",
            "mBART (LoRA) - Percentage of Parameters to Fine-tune (All): 12.54%\n",
            "\n",
            "mBART (Layer Freezing) - Total Parameters: 610879488\n",
            "mBART (Layer Freezing) - Trainable Parameters (All): 375736320\n",
            "mBART (Layer Freezing) - Percentage of Parameters to Fine-tune (All): 61.51%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display parameter summaries for the original and LoRA-enhanced models\n",
        "print_model_parameters(model_mbart, \"mBART (original)\")\n",
        "print_model_parameters(model_mbart_lora, \"mBART (LoRA)\")\n",
        "print_model_parameters(model_mbart_freeze, \"mBART (Layer Freezing)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XklaqI9z8qAy"
      },
      "source": [
        "#### **1.3.2 M2M100**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "whhn7rwd8vbj",
        "outputId": "9aca5ad8-22e3-4fd4-cda0-791de51a4f2f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7af8893e04a46e29d04ae08c2806042",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "289ae24bf9ce490baeed866f57f00b77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7300849ff8684b879070d59ee4e9f3cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25bdc40f610642cebc8dd76b3b6a051e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a476fcce9e28421182dfc3eacab09150",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "375fd77fc6ed47fc8d4fd3eec3061df9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7143f6dc06a44bc4bf284d8aac65533e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M2M100 model and tokenizer initialized with LoRA successfully.\n"
          ]
        }
      ],
      "source": [
        "# Model names\n",
        "mtm_model_name = \"facebook/m2m100_418M\"\n",
        "\n",
        "# Initialize M2M100 model and tokenizer\n",
        "tokenizer_mtm = M2M100Tokenizer.from_pretrained(mtm_model_name)\n",
        "model_mtm = M2M100ForConditionalGeneration.from_pretrained(mtm_model_name)\n",
        "\n",
        "# Create a deep copy of the original model for LoRA\n",
        "model_mtm_lora = copy.deepcopy(model_mtm)\n",
        "\n",
        "# LoRA configuration for M2M100\n",
        "mtm_lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,  # Low-rank dimension\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.0,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Target LoRA modules (check M2M100 architecture)\n",
        ")\n",
        "\n",
        "# Apply LoRA to M2M100 model\n",
        "model_mtm_lora = get_peft_model(model_mtm_lora, mtm_lora_config)\n",
        "\n",
        "# Ensure only LoRA layers are trainable\n",
        "for name, param in model_mtm_lora.named_parameters():\n",
        "    if any(layer in name for layer in mtm_lora_config.target_modules):\n",
        "        param.requires_grad = True  # LoRA layers\n",
        "    else:\n",
        "        param.requires_grad = False  # Non-LoRA layers\n",
        "\n",
        "\n",
        "# Create a deep copy of the original model for Layer Freezing\n",
        "model_mtm_freeze = copy.deepcopy(model_mtm)\n",
        "\n",
        "# Layer Freezing Configuration\n",
        "freeze_encoder_layers = 8  # Number of encoder layers to freeze\n",
        "freeze_decoder_layers = 8  # Number of decoder layers to freeze\n",
        "\n",
        "# Apply Layer Freezing to M2M100 model\n",
        "for name, param in model_mtm_freeze.named_parameters():\n",
        "    # Freeze encoder layers\n",
        "    if \"encoder.layers\" in name:\n",
        "        layer_num = int(name.split(\".\")[3])  # Extract the layer index\n",
        "        if layer_num < freeze_encoder_layers:\n",
        "            param.requires_grad = False  # Freeze parameters\n",
        "    # Freeze decoder layers\n",
        "    elif \"decoder.layers\" in name:\n",
        "        layer_num = int(name.split(\".\")[3])  # Extract the layer index\n",
        "        if layer_num < freeze_decoder_layers:\n",
        "            param.requires_grad = False  # Freeze parameters\n",
        "    else:\n",
        "        param.requires_grad = True  # Keep other parameters trainable\n",
        "\n",
        "\n",
        "print(\"M2M100 model and tokenizer initialized with LoRA successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaZIrpfx-0eL",
        "outputId": "4b268ed2-ec1d-4f16-d797-5711aa5ce04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "M2M100 (original) - Total Parameters: 483905536\n",
            "M2M100 (original) - Trainable Parameters (All): 483905536\n",
            "M2M100 (original) - Percentage of Parameters to Fine-tune (All): 100.00%\n",
            "\n",
            "M2M100 (LoRA) - Total Parameters: 485085184\n",
            "M2M100 (LoRA) - Trainable Parameters (All): 76750848\n",
            "M2M100 (LoRA) - Percentage of Parameters to Fine-tune (All): 15.82%\n",
            "\n",
            "M2M100 (Layer Freezing) - Total Parameters: 483905536\n",
            "M2M100 (Layer Freezing) - Trainable Parameters (All): 248762368\n",
            "M2M100 (Layer Freezing) - Percentage of Parameters to Fine-tune (All): 51.41%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display parameter summaries\n",
        "print_model_parameters(model_mtm, \"M2M100 (original)\")\n",
        "print_model_parameters(model_mtm_lora, \"M2M100 (LoRA)\")\n",
        "print_model_parameters(model_mtm_freeze, \"M2M100 (Layer Freezing)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXhU2VXTwiwH"
      },
      "source": [
        "### **1.4 Load and Preprocess Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VEx8b6vaw7n"
      },
      "outputs": [],
      "source": [
        "def load_data_from_txt(txt_file_path, sample_size=100000, train_ratio=0.8, dev_ratio=0.1, encoding='utf-8'):\n",
        "    # Check if the file exists\n",
        "    if not os.path.isfile(txt_file_path):\n",
        "        print(\"The specified .txt file does not exist.\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Initialize lists to hold English and Chinese sentences\n",
        "    english_sentences = []\n",
        "    chinese_sentences = []\n",
        "\n",
        "    try:\n",
        "        # Read the text file line by line\n",
        "        with open(txt_file_path, 'r', encoding=encoding) as f:\n",
        "            lines = [line.strip() for line in f]\n",
        "\n",
        "        # Separate English and Chinese sentences by alternating lines\n",
        "        for i in range(0, len(lines) - 1, 2):\n",
        "            english_sentences.append(lines[i])\n",
        "            chinese_sentences.append(lines[i + 1])\n",
        "\n",
        "        # Ensure equal number of English and Chinese sentences\n",
        "        min_length = min(len(english_sentences), len(chinese_sentences))\n",
        "        english_sentences = english_sentences[:min_length]\n",
        "        chinese_sentences = chinese_sentences[:min_length]\n",
        "\n",
        "        # Create a DataFrame for paired sentences\n",
        "        data = pd.DataFrame({\"english_text\": english_sentences, \"chinese_text\": chinese_sentences})\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        # Adjust sample size if data size is less than requested sample size\n",
        "        sample_size = min(sample_size, len(data))\n",
        "        data = data.sample(n=sample_size, random_state=42)\n",
        "\n",
        "        # Calculate the number of samples for each split\n",
        "        train_end = int(sample_size * train_ratio)\n",
        "        dev_end = train_end + int(sample_size * dev_ratio)\n",
        "\n",
        "        # Split the dataset into train, dev, and test sets\n",
        "        train_data = data[:train_end]\n",
        "        dev_data = data[train_end:dev_end]\n",
        "        test_data = data[dev_end:]\n",
        "\n",
        "        print(f\"Loaded train data with {len(train_data)} samples.\")\n",
        "        print(f\"Loaded dev data with {len(dev_data)} samples.\")\n",
        "        print(f\"Loaded test data with {len(test_data)} samples.\")\n",
        "\n",
        "        return train_data, dev_data, test_data\n",
        "\n",
        "    except UnicodeDecodeError as e:\n",
        "        print(f\"Error reading the file: {e}\")\n",
        "        return None, None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXAywKJrZHg2"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data, tokenizer, max_length=50, src_lang=None, tgt_lang=None):\n",
        "    \"\"\"\n",
        "    Preprocesses the data for tokenization.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Input data containing source and target texts.\n",
        "        tokenizer: Tokenizer to use for processing.\n",
        "        max_length (int): Maximum sequence length.\n",
        "        src_lang (str): Source language code (e.g., 'zh' for Chinese). Set to None if not required.\n",
        "        tgt_lang (str): Target language code (e.g., 'en' for English). Set to None if not required.\n",
        "\n",
        "    Returns:\n",
        "        Dataset: Tokenized dataset ready for model input.\n",
        "    \"\"\"\n",
        "    dataset = Dataset.from_pandas(data)\n",
        "\n",
        "    def tokenize_function(batch):\n",
        "        # Optionally set language codes for M2M100 or similar models\n",
        "        if src_lang is not None:\n",
        "            tokenizer.src_lang = src_lang\n",
        "        if tgt_lang is not None:\n",
        "            tokenizer.tgt_lang = tgt_lang\n",
        "\n",
        "        # Tokenize the source text\n",
        "        inputs = tokenizer(\n",
        "            batch[\"chinese_text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        )\n",
        "\n",
        "        # Tokenize the target text\n",
        "        targets = tokenizer(\n",
        "            batch[\"english_text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=max_length\n",
        "        )\n",
        "\n",
        "        # Prepare labels and replace padding tokens with -100 for loss calculation\n",
        "        batch[\"input_ids\"] = inputs[\"input_ids\"]\n",
        "        batch[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
        "        batch[\"labels\"] = [\n",
        "            [(token if token != tokenizer.pad_token_id else -100) for token in label]\n",
        "            for label in targets[\"input_ids\"]\n",
        "        ]\n",
        "        return batch\n",
        "\n",
        "    # Tokenize and reformat the dataset\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"english_text\", \"chinese_text\"])\n",
        "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    return tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N79RbBPjrORm",
        "outputId": "21a0d925-ab34-41ac-c2ff-75c607ffe20f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded train data with 80000 samples.\n",
            "Loaded dev data with 10000 samples.\n",
            "Loaded test data with 10000 samples.\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the dataset\n",
        "data_file_path = \"/content/drive/MyDrive/CIS5800/Bilingual/Education/Bi-Education.txt\"\n",
        "train_data, dev_data, test_data = load_data_from_txt(data_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozU3DRKlwtH1"
      },
      "source": [
        "### **1.5 Initialize Evaluation Metrics: BLEU**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhRrgxxtwr6q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "f26ebe4e-a943-47ca-fea1-3377dfe548b8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (<ipython-input-24-459752d69b64>, line 55)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-24-459752d69b64>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    if model = \"alpaca\":\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
          ]
        }
      ],
      "source": [
        "def format_prompt(chinese_text):\n",
        "    return f'''Below is a Chinese sentence. Translate it into English.\n",
        "\n",
        "### Instruction:\n",
        "{chinese_text}\n",
        "\n",
        "### Response:\n",
        "'''\n",
        "\n",
        "def evaluate_bleu_score(model, tokenizer, test_data, max_length=50, device=\"cuda\", model_type = \"mbart\"):\n",
        "    \"\"\"\n",
        "    Evaluates the BLEU score of a given model on a test dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The translation model to evaluate (e.g., trained mBART model).\n",
        "    - tokenizer: The tokenizer corresponding to the model.\n",
        "    - test_data: The test dataset containing \"chinese_text\" and \"english_text\" columns.\n",
        "    - max_length: Maximum sequence length for translation generation.\n",
        "    - device: The device to run the evaluation on (\"cuda\" or \"cpu\").\n",
        "\n",
        "    Returns:\n",
        "    - average_bleu: The average BLEU score for the test set.\n",
        "    \"\"\"\n",
        "    # Move model to the specified device\n",
        "    model = model.to(device)\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    # Prepare the test sentences\n",
        "    test_chinese_sentences = test_data[\"chinese_text\"].tolist()\n",
        "    test_english_references = test_data[\"english_text\"].tolist()\n",
        "\n",
        "    if Alpaca:\n",
        "      test_chinese_sentences = [format_prompt(sentence) for sentence in test_chinese_sentences]\n",
        "\n",
        "    bleu_scores = []\n",
        "\n",
        "    # Progress bar for tracking\n",
        "    for src_sentence, ref_sentence in tqdm(zip(test_chinese_sentences, test_english_references),\n",
        "                                           total=len(test_chinese_sentences),\n",
        "                                           desc=\"Calculating BLEU Scores\"):\n",
        "        # Tokenize and prepare the input with max_length\n",
        "        inputs = tokenizer(src_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
        "\n",
        "        # Generate translation\n",
        "        with torch.no_grad():\n",
        "          if model_type == \"mabrt\":\n",
        "            generated_tokens = model.generate(**inputs, max_length=max_length)\n",
        "\n",
        "          elif model_type == \"m2m\":\n",
        "            generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"en\"])\n",
        "\n",
        "        # Decode generated tokens to text\n",
        "        generated_sentence = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "        if model = \"alpaca\":\n",
        "          generated_sentence = generated_sentence.replace('</s>', '')\n",
        "\n",
        "        # Tokenize reference and hypothesis for BLEU calculation\n",
        "        reference = [ref_sentence.split()]  # Reference should be a list of lists for sentence_bleu\n",
        "        hypothesis = generated_sentence.split()\n",
        "\n",
        "        # Compute BLEU score with smoothing\n",
        "        bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=smooth_fn)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    # Calculate and return the average BLEU score for the test set\n",
        "    average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    print(f\"Average BLEU score for the test set: {average_bleu:.4f}\")\n",
        "\n",
        "    return average_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWB_nGyqUeSz"
      },
      "outputs": [],
      "source": [
        "def error_evaluate(model, tokenizer, test_data, save_path, max_length=50, device=\"cuda\", bleu_threshold=0.1, model_type = \"mabrt\"):\n",
        "    \"\"\"\n",
        "    Evaluate a translation model on a test set using BLEU score.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Translation model (e.g., mBART).\n",
        "    - tokenizer: Tokenizer corresponding to the model.\n",
        "    - test_data: Test dataset as a DataFrame with 'chinese_text' and 'english_text' columns.\n",
        "    - save_path: Path to save the incorrect examples as a CSV file.\n",
        "    - max_length: Maximum sequence length for generated translations (default=50).\n",
        "    - device: Device to run the evaluation ('cuda' or 'cpu').\n",
        "    - bleu_threshold: BLEU score threshold to determine correctness (default=0.1).\n",
        "\n",
        "    Returns:\n",
        "    - results_df: DataFrame containing evaluation results.\n",
        "    - errors_df: DataFrame containing incorrect examples.\n",
        "    \"\"\"\n",
        "    # Move model to the specified device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Initialize smoothing function\n",
        "    smooth_fn = SmoothingFunction().method1\n",
        "\n",
        "    # Results storage\n",
        "    results = []\n",
        "\n",
        "    # Process test data\n",
        "    for idx, row in tqdm(test_data.iterrows(), total=len(test_data), desc=\"Evaluating BLEU\"):\n",
        "        chinese_input = row[\"chinese_text\"]\n",
        "        ground_truth = row[\"english_text\"]\n",
        "\n",
        "        # Step 1: Tokenize and move input to device\n",
        "        inputs = tokenizer(chinese_input, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        # Step 2: Generate translation\n",
        "        with torch.no_grad():\n",
        "          if model_type == \"mabrt\":\n",
        "            translated_tokens = model.generate(\n",
        "                **inputs, max_length=max_length, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
        "            )\n",
        "          elif model_type == \"m2m\":\n",
        "            translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"en\"])\n",
        "\n",
        "        prediction = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "        # Step 3: Calculate BLEU score\n",
        "        reference = [ground_truth.split()]  # Reference sentence split into tokens\n",
        "        hypothesis = prediction.split()     # Hypothesis sentence split into tokens\n",
        "        bleu_score = sentence_bleu(reference, hypothesis, smoothing_function=smooth_fn)\n",
        "\n",
        "        # Step 4: Determine correctness\n",
        "        is_correct = bleu_score >= bleu_threshold\n",
        "\n",
        "        # Step 5: Append results\n",
        "        results.append({\n",
        "            \"chinese_input\": chinese_input,\n",
        "            \"ground_truth\": ground_truth,\n",
        "            \"prediction\": prediction,\n",
        "            \"bleu_score\": round(bleu_score, 3),\n",
        "            \"correct\": is_correct\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    results_df = pd.DataFrame(results)\n",
        "\n",
        "    # Filter incorrect translations\n",
        "    errors_df = results_df[~results_df[\"correct\"]]\n",
        "\n",
        "    # Print and save results\n",
        "    print(\"\\nAll Results:\")\n",
        "    print(results_df)\n",
        "\n",
        "    print(\"\\nError Examples:\")\n",
        "    print(errors_df)\n",
        "\n",
        "    # Save errors to CSV file\n",
        "    errors_df.to_csv(save_path, index=False)\n",
        "    print(f\"Error examples saved to '{save_path}'\")\n",
        "\n",
        "    return results_df, errors_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhCijy_CqYHV"
      },
      "source": [
        "### **1.6 GPU and Memory Monitoring**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScJcEeM3qxwf",
        "outputId": "b39fa912-69b3-4647-acce-2fe3d9ea6900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory created or already exists: /content/drive/MyDrive/CIS5800/Log/m2m\n",
            "Directory created or already exists: /content/drive/MyDrive/CIS5800/Log\n",
            "Directory created or already exists: /content/drive/MyDrive/CIS5800/Log/mBART\n",
            "Directory created or already exists: /content/drive/MyDrive/CIS5800/Log/mBART\n"
          ]
        }
      ],
      "source": [
        "file_path_m2m = \"/content/drive/MyDrive/CIS5800/Log/m2m/memory_usage.log\"\n",
        "file_path_m2m_lora = \"/content/drive/MyDrive/CIS5800/Log/memory_usage_lora.log\"\n",
        "\n",
        "file_path_mBART = \"/content/drive/MyDrive/CIS5800/Log/mBART/memory_usage.log\"\n",
        "file_path_mBART_lora = \"/content/drive/MyDrive/CIS5800/Log/mBART/memory_usage_lora.log\"\n",
        "\n",
        "paths = [file_path_m2m, file_path_m2m_lora, file_path_mBART, file_path_mBART_lora]\n",
        "\n",
        "for file_path in paths:\n",
        "  directory_path = os.path.dirname(file_path)\n",
        "  if not os.path.exists(directory_path):\n",
        "    os.makedirs(directory_path, exist_ok=True)\n",
        "  print(f\"Directory created or already exists: {directory_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ0S4c2xqfHV"
      },
      "outputs": [],
      "source": [
        "# Define a function to monitor memory usage\n",
        "\n",
        "class MemoryUsageLogger(TrainerCallback):\n",
        "    def __init__(self, log_file):\n",
        "        self.log_file = log_file\n",
        "        # Initialize the log file and write headers\n",
        "        with open(log_file, \"w\") as f:\n",
        "            f.write(\"Step,Timestamp,Time (s),RAM Usage (GB),GPU Memory Usage (GB)\\n\")\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        # Calculate memory usage\n",
        "        process = psutil.Process(os.getpid())\n",
        "        ram_usage = process.memory_info().rss / (1024 ** 3)  # Convert to GB\n",
        "\n",
        "        gpu_memory = 0\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_memory = torch.cuda.memory_allocated() / (1024 ** 3)  # Convert to GB\n",
        "\n",
        "        # Log memory usage\n",
        "        current_time = time.time() - self.start_time\n",
        "        timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
        "        with open(self.log_file, \"a\") as f:\n",
        "            f.write(f\"{state.global_step},{timestamp},{current_time:.2f},{ram_usage:.2f},{gpu_memory:.2f}\\n\")\n",
        "\n",
        "        # Optional: Print to console for real-time monitoring\n",
        "        print(f\"Step: {state.global_step}, Timestamp: {timestamp}, RAM: {ram_usage:.2f} GB, GPU: {gpu_memory:.2f} GB\")\n",
        "\n",
        "\n",
        "# Analyze logs to find top 15 memory usages\n",
        "def analyze_logs(log_file, output_file):\n",
        "    \"\"\"\n",
        "    Analyzes the memory log file to find the top 15 GPU and RAM usages\n",
        "    along with their timestamps and saves the results to a file.\n",
        "\n",
        "    Args:\n",
        "        log_file (str): Path to the input log file.\n",
        "        output_file (str): Path to the output file.\n",
        "    \"\"\"\n",
        "    # Check if the log file exists\n",
        "    if not os.path.exists(log_file):\n",
        "        raise FileNotFoundError(f\"Log file '{log_file}' does not exist.\")\n",
        "\n",
        "    # Read the log file into a DataFrame\n",
        "    df = pd.read_csv(log_file)\n",
        "\n",
        "    # Find the top 15 GPU memory usages\n",
        "    top_gpu_usage = df.nlargest(15, \"GPU Memory Usage (GB)\")\n",
        "\n",
        "    # Find the top 15 RAM usages\n",
        "    top_ram_usage = df.nlargest(15, \"RAM Usage (GB)\")\n",
        "\n",
        "    # Combine the results\n",
        "    combined = pd.concat([top_gpu_usage, top_ram_usage]).drop_duplicates().sort_values(by=\"Time (s)\")\n",
        "\n",
        "    # Ensure output file exists or create it\n",
        "    if not os.path.exists(output_file):\n",
        "        with open(output_file, \"w\") as f:\n",
        "            f.write(\"\")  # Create an empty file if it doesn't exist\n",
        "\n",
        "    # Save the results to the output file\n",
        "    combined.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"Analysis complete. Top 15 memory logs saved to: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KdUmj4Ayhkd"
      },
      "source": [
        "## **2. Baseline Evaluation (50,000 Samples)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMZQWk2lttXo"
      },
      "source": [
        "### **2.1 Load Dataset (50,000)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "id": "XBpNkKCWt2b3",
        "outputId": "fc107207-2703-44af-fa5a-ee218b95eda8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded train data with 40000 samples.\n",
            "Loaded dev data with 5000 samples.\n",
            "Loaded test data with 5000 samples.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e816fff464a43419154e6e00b6b1f1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d13525771bf4465eab17147bcee2f715",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ee5c10c834d49d8950fb133771f57bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fd4030a53f9496d8c48891b6597ea0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c55df4c13d24e24b2563d08184268de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56f6640600f24f8ba223c01de241e340",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             english_text  \\\n",
            "234806  Pottery Barn: If I was a genie and could give ...   \n",
            "41549   First, during slewing, the voltage at the sour...   \n",
            "169822  Locke: I think that's overdoing it just a bit ...   \n",
            "313751  Amanda: I think I'll stick to a scoop of class...   \n",
            "32640   And it is expected that other interested parti...   \n",
            "\n",
            "                                          chinese_text  \n",
            "234806  家居用品陶瓷谷仓：设想如果我是一个天才，我能给于你理想的工作，工作会是什么？ 工作会在哪里？  \n",
            "41549                      第一种，在回摆过程中，在M5和M8源处的电压可能降低。  \n",
            "169822                 中文意思：洛克：我认为这做得有一点点过分，还是在他头上绑一条…  \n",
            "313751              阿曼达：我想我还是要一勺经典香草口味的除非你想和我一起吃一个香蕉船。  \n",
            "32640                    他还希望其他有关当事人能投放竟标也许能阻止新闻集团的计划。  \n",
            "                                             english_text  \\\n",
            "173397  He will see to it was that your plans quickly ...   \n",
            "296479  Plan, monitor & assure all machineries in good...   \n",
            "24319   There is a central vein in the center of the m...   \n",
            "371615  That's weighing remote military conjectures ag...   \n",
            "309857  Geocell is a new geosynthetics. It is efficaci...   \n",
            "\n",
            "                                    chinese_text  \n",
            "173397                         他将留意你的计划是否很快付诸实行。  \n",
            "296479                        计划、监控并确保所有设备的有效利用。  \n",
            "24319                          髓质中央有一大的静脉，称中央静脉。  \n",
            "371615  那不过是牵强附会的军事假设，而现在的事实却是无辜者正在惨遭杀害，这怎能同日而语!  \n",
            "309857          土工格室作为一种新型的土工合成材料，能有效的加固无粘性的风积沙。  \n",
            "                                             english_text  \\\n",
            "449881  The sergeant suggested reducing the rations by...   \n",
            "233204  A U.N. relief plane brought water, tents, cook...   \n",
            "13482   The Working Group suggests to either broaden t...   \n",
            "320630  Although his Ministry had a graphic design ate...   \n",
            "111437  \"The World of Cinemagic\"provides rare behind-t...   \n",
            "\n",
            "                                             chinese_text  \n",
            "449881             那位副官想出了一计：偷偷地用一个小勺子来量米，这样便可以减少分量，节省粮食。  \n",
            "233204      一架联合国救援飞机为灾情最严重的日惹省班图尔地区带去水、帐篷、烹饪器具以及成套的卫生用品。  \n",
            "13482   工作组建议要么拓宽存款的定义，将其它类型的存款包括在内，而不仅仅包含目前的消费及公司存款，要...  \n",
            "320630                          虽然该部有一个视觉设计室，但主要用来创作宣传材料。  \n",
            "111437   “电影魔术世界”使游客们能看到一些罕见的幕后情况，能有影响地参与那情节紧张的好莱坞电影技术领域。  \n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the dataset\n",
        "train_data, dev_data, test_data = load_data_from_txt(data_file_path, sample_size = 50000)\n",
        "\n",
        "tokenized_data_mbart_train = preprocess_data(train_data, tokenizer_mbart)\n",
        "tokenized_data_mbart_dev = preprocess_data(dev_data, tokenizer_mbart)\n",
        "tokenized_data_mbart_test = preprocess_data(test_data, tokenizer_mbart)\n",
        "\n",
        "tokenized_data_mtm_train = preprocess_data(train_data, tokenizer_mtm, max_length=20, src_lang=\"zh\", tgt_lang=\"en\")\n",
        "tokenized_data_mtm_dev = preprocess_data(dev_data, tokenizer_mtm, max_length=20, src_lang=\"zh\", tgt_lang=\"en\")\n",
        "tokenized_data_mtm_test = preprocess_data(test_data, tokenizer_mtm, max_length=20, src_lang=\"zh\", tgt_lang=\"en\")\n",
        "\n",
        "print(train_data.head())\n",
        "print(dev_data.head())\n",
        "print(test_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4Urv1Ng5VIN"
      },
      "source": [
        "### **2.2 Regular mBart**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZfQD1si5NOi",
        "outputId": "2de2bdaa-2b44-42c9-91c1-a3cdd89b1a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translation: I love machine learning, I love natural language processing\n"
          ]
        }
      ],
      "source": [
        "# Prepare input text\n",
        "text = \"我爱机器学习，我爱自然语言处理\"\n",
        "inputs = tokenizer_mbart(text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate translation\n",
        "translated_tokens = model_mbart.generate(**inputs, forced_bos_token_id=tokenizer_mbart.lang_code_to_id[\"en_XX\"])\n",
        "translation = tokenizer_mbart.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Translation:\", translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27hPoCj6SBI0",
        "outputId": "c5b663e7-068d-46d1-83fd-a9a3da25294d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Scores: 100%|██████████| 5000/5000 [51:06<00:00,  1.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score for the test set: 0.1169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "average_bleu = evaluate_bleu_score(model=model_mbart, tokenizer=tokenizer_mbart, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc4zyFWvOnwD",
        "outputId": "4e52b32f-efc3-45d8-928e-6377a83c9927"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating BLEU: 100%|██████████| 5000/5000 [47:01<00:00,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All Results:\n",
            "                                         chinese_input  \\\n",
            "0                              雅典娜: 我们开始彼此统一，以感激和欢乐一起。   \n",
            "1                             GDP被公认为衡量一个国家经济实力的最重要指标。   \n",
            "2                                捷克共和国的扬?卡万是目前的联大会议主席。   \n",
            "3     大麻籽提供了所有必需的氨基酸，这意味着它们含有的蛋白质能够与肉类、鸡蛋和奶制品中的蛋白质相媲美。   \n",
            "4                                       黑核桃的果仁很难从壳中取出。   \n",
            "...                                                ...   \n",
            "4995                                休和托尼大吵了一架，现在谁也不理谁。   \n",
            "4996                                 音乐与舞曲的节拍，会让你精神高昂。   \n",
            "4997                     如果您的设置全部正确，那么您应该得到类似图33所示的情况。   \n",
            "4998                       我们都要做好自己工作，并且要在工作中找到精神上的乐趣。   \n",
            "4999                             记得爸爸妈妈忘记带食物的那次的野餐 吗 ?   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     Ariana: We come together in the oneness, in th...   \n",
            "1     GDP by fair the mainest index that thinks to m...   \n",
            "2     Jan Kavan of the Czech Republic is the current...   \n",
            "3     Hemp seeds provide all of the essential amino ...   \n",
            "4     Black walnut kernels are difficult to get out ...   \n",
            "...                                                 ...   \n",
            "4995  Sue and Tony had a bust-up and aren't speaking...   \n",
            "4996  It can be music or dance beat, anything that w...   \n",
            "4997  If you provided all of the correct settings, y...   \n",
            "4998  We must cultivate our own garden and find the ...   \n",
            "4999  Do you remember that picnic when Mother and Pa...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     Athena: We began to unite together in gratitud...       0.029    False  \n",
            "1     GDP is generally regarded as the most importan...       0.031    False  \n",
            "2     Jan Kavan of the Czech Republic is the current...       0.612     True  \n",
            "3     Cannabis seeds provide all the necessary amino...       0.089    False  \n",
            "4     The berries of black nuts are difficult to ext...       0.096    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4995  Hugh and Tony had a big fight, and now no one ...       0.224     True  \n",
            "4996  The music and dance beats will make you feel v...       0.074    False  \n",
            "4997  If all settings are correct, you should get a ...       0.089    False  \n",
            "4998  We have to do our job well and find spiritual ...       0.040    False  \n",
            "4999  Do you remember the picnic where your parents ...       0.104     True  \n",
            "\n",
            "[5000 rows x 5 columns]\n",
            "\n",
            "Error Examples:\n",
            "                                         chinese_input  \\\n",
            "0                              雅典娜: 我们开始彼此统一，以感激和欢乐一起。   \n",
            "1                             GDP被公认为衡量一个国家经济实力的最重要指标。   \n",
            "3     大麻籽提供了所有必需的氨基酸，这意味着它们含有的蛋白质能够与肉类、鸡蛋和奶制品中的蛋白质相媲美。   \n",
            "4                                       黑核桃的果仁很难从壳中取出。   \n",
            "5               为美国国债最大的海外持有国，中国在债券市场上有着其它债权国不可比拟的重要性。   \n",
            "...                                                ...   \n",
            "4990                                阿甘意识到乔伊已不再给咖啡馆打工了。   \n",
            "4993                       他说，\"毫无疑问将会出现某种让新总统受到考验的事件\"。   \n",
            "4996                                 音乐与舞曲的节拍，会让你精神高昂。   \n",
            "4997                     如果您的设置全部正确，那么您应该得到类似图33所示的情况。   \n",
            "4998                       我们都要做好自己工作，并且要在工作中找到精神上的乐趣。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     Ariana: We come together in the oneness, in th...   \n",
            "1     GDP by fair the mainest index that thinks to m...   \n",
            "3     Hemp seeds provide all of the essential amino ...   \n",
            "4     Black walnut kernels are difficult to get out ...   \n",
            "5     As the largest foreign holder of US Treasurys,...   \n",
            "...                                                 ...   \n",
            "4990  Gunther realizes Joey's not working at the cof...   \n",
            "4993  There will be some event that will, in all lik...   \n",
            "4996  It can be music or dance beat, anything that w...   \n",
            "4997  If you provided all of the correct settings, y...   \n",
            "4998  We must cultivate our own garden and find the ...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     Athena: We began to unite together in gratitud...       0.029    False  \n",
            "1     GDP is generally regarded as the most importan...       0.031    False  \n",
            "3     Cannabis seeds provide all the necessary amino...       0.089    False  \n",
            "4     The berries of black nuts are difficult to ext...       0.096    False  \n",
            "5     As the largest overseas holder of US Treasurie...       0.094    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4990  Agan realizes that Joe is no longer working fo...       0.043    False  \n",
            "4993  \"There is no doubt that there will be some kin...       0.061    False  \n",
            "4996  The music and dance beats will make you feel v...       0.074    False  \n",
            "4997  If all settings are correct, you should get a ...       0.089    False  \n",
            "4998  We have to do our job well and find spiritual ...       0.040    False  \n",
            "\n",
            "[3214 rows x 5 columns]\n",
            "Error examples saved to '/content/drive/MyDrive/CIS5800/Error/mBart_base'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                                         chinese_input  \\\n",
              " 0                              雅典娜: 我们开始彼此统一，以感激和欢乐一起。   \n",
              " 1                             GDP被公认为衡量一个国家经济实力的最重要指标。   \n",
              " 2                                捷克共和国的扬?卡万是目前的联大会议主席。   \n",
              " 3     大麻籽提供了所有必需的氨基酸，这意味着它们含有的蛋白质能够与肉类、鸡蛋和奶制品中的蛋白质相媲美。   \n",
              " 4                                       黑核桃的果仁很难从壳中取出。   \n",
              " ...                                                ...   \n",
              " 4995                                休和托尼大吵了一架，现在谁也不理谁。   \n",
              " 4996                                 音乐与舞曲的节拍，会让你精神高昂。   \n",
              " 4997                     如果您的设置全部正确，那么您应该得到类似图33所示的情况。   \n",
              " 4998                       我们都要做好自己工作，并且要在工作中找到精神上的乐趣。   \n",
              " 4999                             记得爸爸妈妈忘记带食物的那次的野餐 吗 ?   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     Ariana: We come together in the oneness, in th...   \n",
              " 1     GDP by fair the mainest index that thinks to m...   \n",
              " 2     Jan Kavan of the Czech Republic is the current...   \n",
              " 3     Hemp seeds provide all of the essential amino ...   \n",
              " 4     Black walnut kernels are difficult to get out ...   \n",
              " ...                                                 ...   \n",
              " 4995  Sue and Tony had a bust-up and aren't speaking...   \n",
              " 4996  It can be music or dance beat, anything that w...   \n",
              " 4997  If you provided all of the correct settings, y...   \n",
              " 4998  We must cultivate our own garden and find the ...   \n",
              " 4999  Do you remember that picnic when Mother and Pa...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     Athena: We began to unite together in gratitud...       0.029    False  \n",
              " 1     GDP is generally regarded as the most importan...       0.031    False  \n",
              " 2     Jan Kavan of the Czech Republic is the current...       0.612     True  \n",
              " 3     Cannabis seeds provide all the necessary amino...       0.089    False  \n",
              " 4     The berries of black nuts are difficult to ext...       0.096    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4995  Hugh and Tony had a big fight, and now no one ...       0.224     True  \n",
              " 4996  The music and dance beats will make you feel v...       0.074    False  \n",
              " 4997  If all settings are correct, you should get a ...       0.089    False  \n",
              " 4998  We have to do our job well and find spiritual ...       0.040    False  \n",
              " 4999  Do you remember the picnic where your parents ...       0.104     True  \n",
              " \n",
              " [5000 rows x 5 columns],\n",
              "                                          chinese_input  \\\n",
              " 0                              雅典娜: 我们开始彼此统一，以感激和欢乐一起。   \n",
              " 1                             GDP被公认为衡量一个国家经济实力的最重要指标。   \n",
              " 3     大麻籽提供了所有必需的氨基酸，这意味着它们含有的蛋白质能够与肉类、鸡蛋和奶制品中的蛋白质相媲美。   \n",
              " 4                                       黑核桃的果仁很难从壳中取出。   \n",
              " 5               为美国国债最大的海外持有国，中国在债券市场上有着其它债权国不可比拟的重要性。   \n",
              " ...                                                ...   \n",
              " 4990                                阿甘意识到乔伊已不再给咖啡馆打工了。   \n",
              " 4993                       他说，\"毫无疑问将会出现某种让新总统受到考验的事件\"。   \n",
              " 4996                                 音乐与舞曲的节拍，会让你精神高昂。   \n",
              " 4997                     如果您的设置全部正确，那么您应该得到类似图33所示的情况。   \n",
              " 4998                       我们都要做好自己工作，并且要在工作中找到精神上的乐趣。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     Ariana: We come together in the oneness, in th...   \n",
              " 1     GDP by fair the mainest index that thinks to m...   \n",
              " 3     Hemp seeds provide all of the essential amino ...   \n",
              " 4     Black walnut kernels are difficult to get out ...   \n",
              " 5     As the largest foreign holder of US Treasurys,...   \n",
              " ...                                                 ...   \n",
              " 4990  Gunther realizes Joey's not working at the cof...   \n",
              " 4993  There will be some event that will, in all lik...   \n",
              " 4996  It can be music or dance beat, anything that w...   \n",
              " 4997  If you provided all of the correct settings, y...   \n",
              " 4998  We must cultivate our own garden and find the ...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     Athena: We began to unite together in gratitud...       0.029    False  \n",
              " 1     GDP is generally regarded as the most importan...       0.031    False  \n",
              " 3     Cannabis seeds provide all the necessary amino...       0.089    False  \n",
              " 4     The berries of black nuts are difficult to ext...       0.096    False  \n",
              " 5     As the largest overseas holder of US Treasurie...       0.094    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4990  Agan realizes that Joe is no longer working fo...       0.043    False  \n",
              " 4993  \"There is no doubt that there will be some kin...       0.061    False  \n",
              " 4996  The music and dance beats will make you feel v...       0.074    False  \n",
              " 4997  If all settings are correct, you should get a ...       0.089    False  \n",
              " 4998  We have to do our job well and find spiritual ...       0.040    False  \n",
              " \n",
              " [3214 rows x 5 columns])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_evaluate(model_mbart, tokenizer_mbart, test_data, \"/content/drive/MyDrive/CIS5800/Error/mBart_base\", max_length=50, device=\"cuda\", bleu_threshold=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GILSJNwtBxpr"
      },
      "source": [
        "### **2.3 Regular M2M100**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiSchVAUX0L1",
        "outputId": "b30deb6e-2749-48a0-8cd9-dfaf2a17b224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I love machine learning, I love natural language processing.\n"
          ]
        }
      ],
      "source": [
        "tokenizer_mtm.src_lang = \"zh\"\n",
        "inputs = tokenizer_mtm(\"我爱机器学习，我爱自然语言处理\", return_tensors=\"pt\")\n",
        "model_mtm.to(\"cpu\")\n",
        "outputs = model_mtm.generate(**inputs, forced_bos_token_id=tokenizer_mtm.lang_code_to_id[\"en\"])\n",
        "\n",
        "print(tokenizer_mtm.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_seqnfBMf6R0",
        "outputId": "a1641280-111e-4157-ac37-29402a8c978b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Scores: 100%|██████████| 5000/5000 [47:56<00:00,  1.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score for the test set: 0.0347\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "average_bleu = evaluate_bleu_score(model=model_mtm, tokenizer=tokenizer_mtm, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_Q2NLAxVYnQ",
        "outputId": "4f845e59-76ae-41ae-d12c-0babd887154f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating BLEU: 100%|██████████| 5000/5000 [42:22<00:00,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All Results:\n",
            "                                          chinese_input  \\\n",
            "0                那位副官想出了一计：偷偷地用一个小勺子来量米，这样便可以减少分量，节省粮食。   \n",
            "1         一架联合国救援飞机为灾情最严重的日惹省班图尔地区带去水、帐篷、烹饪器具以及成套的卫生用品。   \n",
            "2     工作组建议要么拓宽存款的定义，将其它类型的存款包括在内，而不仅仅包含目前的消费及公司存款，要...   \n",
            "3                             虽然该部有一个视觉设计室，但主要用来创作宣传材料。   \n",
            "4      “电影魔术世界”使游客们能看到一些罕见的幕后情况，能有影响地参与那情节紧张的好莱坞电影技术领域。   \n",
            "...                                                 ...   \n",
            "4995                                 用英文缮制的所有单据须一次寄交我行。   \n",
            "4996                            那些有作为者，那些成大事者，谁没有过开始呢 ?   \n",
            "4997  友谊的一个基本原则是不评判朋友、接受他的缺点，而对朋友的工作表现提出批评性的反馈意见则与此相矛盾。   \n",
            "4998                                 这些鱼种还被出口为国家换来大量外汇。   \n",
            "4999                               吸脂手术是一种吸取不需要部分脂肪的手术。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     The sergeant suggested reducing the rations by...   \n",
            "1     A U.N. relief plane brought water, tents, cook...   \n",
            "2     The Working Group suggests to either broaden t...   \n",
            "3     Although his Ministry had a graphic design ate...   \n",
            "4     \"The World of Cinemagic\"provides rare behind-t...   \n",
            "...                                                 ...   \n",
            "4995  All documents made out in english must sent to...   \n",
            "4996  As those who are into a major event, and who n...   \n",
            "4997  A basic rule of friendship is being non-judgem...   \n",
            "4998  These fingerling still are exported to be a co...   \n",
            "4999  Liposuction is the removal of fat by suction t...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     The deputy officer found out a sum: secretly w...       0.012    False  \n",
            "1     A United Nations rescue aircraft brought water...       0.124     True  \n",
            "2     The work group recommends either extending the...       0.272     True  \n",
            "3     Although the department has a visual design ro...       0.019    False  \n",
            "4     The “Film Magic World” allows visitors to see ...       0.014    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4995  All documents made in English must be submitte...       0.084    False  \n",
            "4996  Those who have acts, those who have become gre...       0.036    False  \n",
            "4997  One of the basic principles of friendship is n...       0.041    False  \n",
            "4998  These species are also exported to the country...       0.094    False  \n",
            "4999  Fat absorption surgery is an absorption that d...       0.017    False  \n",
            "\n",
            "[5000 rows x 5 columns]\n",
            "\n",
            "Error Examples:\n",
            "                                          chinese_input  \\\n",
            "0                那位副官想出了一计：偷偷地用一个小勺子来量米，这样便可以减少分量，节省粮食。   \n",
            "3                             虽然该部有一个视觉设计室，但主要用来创作宣传材料。   \n",
            "4      “电影魔术世界”使游客们能看到一些罕见的幕后情况，能有影响地参与那情节紧张的好莱坞电影技术领域。   \n",
            "5            张曼玉馅饼及冒名顶替者，由苏格兰乐队，他们1992年专辑斗篷和匕首湿湿湿假定的名称。   \n",
            "6     现产各种系列车型的离合器和分离轴承，产品销往澳、美、东南亚各国及全国各地，获得客户一至好评，...   \n",
            "...                                                 ...   \n",
            "4995                                 用英文缮制的所有单据须一次寄交我行。   \n",
            "4996                            那些有作为者，那些成大事者，谁没有过开始呢 ?   \n",
            "4997  友谊的一个基本原则是不评判朋友、接受他的缺点，而对朋友的工作表现提出批评性的反馈意见则与此相矛盾。   \n",
            "4998                                 这些鱼种还被出口为国家换来大量外汇。   \n",
            "4999                               吸脂手术是一种吸取不需要部分脂肪的手术。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     The sergeant suggested reducing the rations by...   \n",
            "3     Although his Ministry had a graphic design ate...   \n",
            "4     \"The World of Cinemagic\"provides rare behind-t...   \n",
            "5     Maggie Pie & The Impostors was the name assume...   \n",
            "6     At present, it is producing various models of ...   \n",
            "...                                                 ...   \n",
            "4995  All documents made out in english must sent to...   \n",
            "4996  As those who are into a major event, and who n...   \n",
            "4997  A basic rule of friendship is being non-judgem...   \n",
            "4998  These fingerling still are exported to be a co...   \n",
            "4999  Liposuction is the removal of fat by suction t...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     The deputy officer found out a sum: secretly w...       0.012    False  \n",
            "3     Although the department has a visual design ro...       0.019    False  \n",
            "4     The “Film Magic World” allows visitors to see ...       0.014    False  \n",
            "5     Zhang Manjou Cake and the famous supporters ar...       0.023    False  \n",
            "6     Currently produced various series of vehicles ...       0.088    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4995  All documents made in English must be submitte...       0.084    False  \n",
            "4996  Those who have acts, those who have become gre...       0.036    False  \n",
            "4997  One of the basic principles of friendship is n...       0.041    False  \n",
            "4998  These species are also exported to the country...       0.094    False  \n",
            "4999  Fat absorption surgery is an absorption that d...       0.017    False  \n",
            "\n",
            "[3652 rows x 5 columns]\n",
            "Error examples saved to '/content/drive/MyDrive/CIS5800/Error/m2m_base'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                                          chinese_input  \\\n",
              " 0                那位副官想出了一计：偷偷地用一个小勺子来量米，这样便可以减少分量，节省粮食。   \n",
              " 1         一架联合国救援飞机为灾情最严重的日惹省班图尔地区带去水、帐篷、烹饪器具以及成套的卫生用品。   \n",
              " 2     工作组建议要么拓宽存款的定义，将其它类型的存款包括在内，而不仅仅包含目前的消费及公司存款，要...   \n",
              " 3                             虽然该部有一个视觉设计室，但主要用来创作宣传材料。   \n",
              " 4      “电影魔术世界”使游客们能看到一些罕见的幕后情况，能有影响地参与那情节紧张的好莱坞电影技术领域。   \n",
              " ...                                                 ...   \n",
              " 4995                                 用英文缮制的所有单据须一次寄交我行。   \n",
              " 4996                            那些有作为者，那些成大事者，谁没有过开始呢 ?   \n",
              " 4997  友谊的一个基本原则是不评判朋友、接受他的缺点，而对朋友的工作表现提出批评性的反馈意见则与此相矛盾。   \n",
              " 4998                                 这些鱼种还被出口为国家换来大量外汇。   \n",
              " 4999                               吸脂手术是一种吸取不需要部分脂肪的手术。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     The sergeant suggested reducing the rations by...   \n",
              " 1     A U.N. relief plane brought water, tents, cook...   \n",
              " 2     The Working Group suggests to either broaden t...   \n",
              " 3     Although his Ministry had a graphic design ate...   \n",
              " 4     \"The World of Cinemagic\"provides rare behind-t...   \n",
              " ...                                                 ...   \n",
              " 4995  All documents made out in english must sent to...   \n",
              " 4996  As those who are into a major event, and who n...   \n",
              " 4997  A basic rule of friendship is being non-judgem...   \n",
              " 4998  These fingerling still are exported to be a co...   \n",
              " 4999  Liposuction is the removal of fat by suction t...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     The deputy officer found out a sum: secretly w...       0.012    False  \n",
              " 1     A United Nations rescue aircraft brought water...       0.124     True  \n",
              " 2     The work group recommends either extending the...       0.272     True  \n",
              " 3     Although the department has a visual design ro...       0.019    False  \n",
              " 4     The “Film Magic World” allows visitors to see ...       0.014    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4995  All documents made in English must be submitte...       0.084    False  \n",
              " 4996  Those who have acts, those who have become gre...       0.036    False  \n",
              " 4997  One of the basic principles of friendship is n...       0.041    False  \n",
              " 4998  These species are also exported to the country...       0.094    False  \n",
              " 4999  Fat absorption surgery is an absorption that d...       0.017    False  \n",
              " \n",
              " [5000 rows x 5 columns],\n",
              "                                           chinese_input  \\\n",
              " 0                那位副官想出了一计：偷偷地用一个小勺子来量米，这样便可以减少分量，节省粮食。   \n",
              " 3                             虽然该部有一个视觉设计室，但主要用来创作宣传材料。   \n",
              " 4      “电影魔术世界”使游客们能看到一些罕见的幕后情况，能有影响地参与那情节紧张的好莱坞电影技术领域。   \n",
              " 5            张曼玉馅饼及冒名顶替者，由苏格兰乐队，他们1992年专辑斗篷和匕首湿湿湿假定的名称。   \n",
              " 6     现产各种系列车型的离合器和分离轴承，产品销往澳、美、东南亚各国及全国各地，获得客户一至好评，...   \n",
              " ...                                                 ...   \n",
              " 4995                                 用英文缮制的所有单据须一次寄交我行。   \n",
              " 4996                            那些有作为者，那些成大事者，谁没有过开始呢 ?   \n",
              " 4997  友谊的一个基本原则是不评判朋友、接受他的缺点，而对朋友的工作表现提出批评性的反馈意见则与此相矛盾。   \n",
              " 4998                                 这些鱼种还被出口为国家换来大量外汇。   \n",
              " 4999                               吸脂手术是一种吸取不需要部分脂肪的手术。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     The sergeant suggested reducing the rations by...   \n",
              " 3     Although his Ministry had a graphic design ate...   \n",
              " 4     \"The World of Cinemagic\"provides rare behind-t...   \n",
              " 5     Maggie Pie & The Impostors was the name assume...   \n",
              " 6     At present, it is producing various models of ...   \n",
              " ...                                                 ...   \n",
              " 4995  All documents made out in english must sent to...   \n",
              " 4996  As those who are into a major event, and who n...   \n",
              " 4997  A basic rule of friendship is being non-judgem...   \n",
              " 4998  These fingerling still are exported to be a co...   \n",
              " 4999  Liposuction is the removal of fat by suction t...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     The deputy officer found out a sum: secretly w...       0.012    False  \n",
              " 3     Although the department has a visual design ro...       0.019    False  \n",
              " 4     The “Film Magic World” allows visitors to see ...       0.014    False  \n",
              " 5     Zhang Manjou Cake and the famous supporters ar...       0.023    False  \n",
              " 6     Currently produced various series of vehicles ...       0.088    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4995  All documents made in English must be submitte...       0.084    False  \n",
              " 4996  Those who have acts, those who have become gre...       0.036    False  \n",
              " 4997  One of the basic principles of friendship is n...       0.041    False  \n",
              " 4998  These species are also exported to the country...       0.094    False  \n",
              " 4999  Fat absorption surgery is an absorption that d...       0.017    False  \n",
              " \n",
              " [3652 rows x 5 columns])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_evaluate(model_mtm, tokenizer_mtm, test_data, \"/content/drive/MyDrive/CIS5800/Error/m2m_base\", max_length=50, device=\"cuda\", bleu_threshold=0.1, model_type = \"m2m\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm909v2MW6VE"
      },
      "source": [
        "## **3. Hyperparameter Tuning (10,000 Samples)**\n",
        "In this section, we performed hyperparamter tuning during fine-tuning on both model to see the best combination and performance. Notably, due to the limitation of computational resources, we performed a grid search on a smaller dataset (10000 samples) compared with previous part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx6APgdyXlgH"
      },
      "source": [
        "### **3.1 Load Dataset (10,000)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GA9GkBo1XlBx",
        "outputId": "9cd8a1ff-20e9-4d34-8cbc-2e92ad521735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded train data with 8000 samples.\n",
            "Loaded dev data with 1000 samples.\n",
            "Loaded test data with 1000 samples.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d6a0f80d7e742f28251c2ba2f7210c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4521bcf3996b4a7fa8cf1f0cd228a08e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "31f6b9b2f9254c28aefa3dde62ca5a3a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c7835035ee64e53ac35f6f69d387d90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2681281f3c5494d9bb2a4a495fb8f4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2786976531a04a52ae4425cdc5c43bed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             english_text  \\\n",
            "234806  At night, the bin is lit up a neon light point...   \n",
            "41549   Semantic opacity in Japanese mainly results fr...   \n",
            "169822  Therefore, the commodity market price is deter...   \n",
            "313751  Certain diets can also cause bad breath, inclu...   \n",
            "32640   Your body needs more of this mineral during pr...   \n",
            "\n",
            "                                             chinese_text  \n",
            "234806  他还提到，在夜晚，垃圾桶的开口处有霓虹灯光照亮，但是到目前为止，还没有人因为这个垃圾桶会说话...  \n",
            "41549             日语中主要使用语义模糊词语、多义词和语义概括(笼统)词语等来表达不明确的语义。  \n",
            "169822                           因此，该商品的市场销售价格，是由国外厂商决定的。  \n",
            "313751                       某些饮食也会导致不好的口气，包括那些高蛋白质和高糖食品。  \n",
            "32640                    你的身体为了赶上怀孕期间扩大的血液容量，需要更多的这种化学元素。  \n",
            "                                             english_text  \\\n",
            "196016  After the Firmament's collapse， radiation even...   \n",
            "301736  Marx foresaw constant internecine warfare amon...   \n",
            "439742  And in their prayers for you their hearts will...   \n",
            "313104  AS usually cause serious cardiovascular diseas...   \n",
            "273830  The determination of 24 hours urine protein? u...   \n",
            "\n",
            "                                   chinese_text  \n",
            "196016          冰天崩溃后，射线最终使人类身体变小，并逐渐缩短他们的生命长度。  \n",
            "301736  马克思预见到资本家会不断自相残杀，结果将造成越来越少的人控制着越来越大的帝国。  \n",
            "439742      14他们也因神极大的恩赐，显在你们心里，就切切地想念你们，为你们祈祷。  \n",
            "313104              AS常常引起心绞痛、心肌梗死、血栓等严重的心血管疾病。  \n",
            "273830                      24h 尿蛋白测定:采用考马斯亮蓝法。  \n",
            "                                             english_text  \\\n",
            "23402   Mount Desert Island in Acadia National Park is...   \n",
            "62827   Ether, glyme, diglyme, nitromethane, or other ...   \n",
            "230383  The processor manipulates the data, storing th...   \n",
            "206133  Low-overhead data collection by other devices ...   \n",
            "49689   And now, friends, fellow citizens of Gettysbur...   \n",
            "\n",
            "                                             chinese_text  \n",
            "23402                            位于阿卡迪亚国家公园中的山沙岛是最为著名的一个。  \n",
            "62827                  乙醚、乙二肟、二乙二肟，硝基甲烷或类似的适当的极性溶剂，可用作溶剂。  \n",
            "230383                               处理器处理没数据，将结果存回到存储器中。  \n",
            "206133                      低开销的数据收集的其他设备支持在特殊情况下 (如树路由)。  \n",
            "49689   朋友们，葛底斯堡和宾夕法尼亚州的同胞们，来自远方各州的同胞们，在这临别之际，我再一次恳请你们...  \n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the dataset\n",
        "train_data, dev_data, test_data = load_data_from_txt(data_file_path, sample_size = 10000)\n",
        "\n",
        "tokenized_data_mbart_train = preprocess_data(train_data, tokenizer_mbart)\n",
        "tokenized_data_mbart_dev = preprocess_data(dev_data, tokenizer_mbart)\n",
        "tokenized_data_mbart_test = preprocess_data(test_data, tokenizer_mbart)\n",
        "\n",
        "tokenized_data_mtm_train = preprocess_data(train_data, tokenizer_mtm, max_length=20, src_lang=\"zh\", tgt_lang=\"en\")\n",
        "tokenized_data_mtm_dev = preprocess_data(dev_data, tokenizer_mtm, max_length=20, src_lang=\"zh\", tgt_lang=\"en\")\n",
        "tokenized_data_mtm_test = preprocess_data(test_data, tokenizer_mtm, max_length=20, src_lang=\"zh\", tgt_lang=\"en\")\n",
        "\n",
        "print(train_data.head())\n",
        "print(dev_data.head())\n",
        "print(test_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d8JQtZ2p29a"
      },
      "source": [
        "### **3.2 mBart**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VmUXr3R0z3J"
      },
      "source": [
        "#### **3.2.1 Hyperparameter Tuning mBart with Regular Fine-Tine Technique**\n",
        "**Best Configuration:learning_rate = 2e-05, batch_size = 16, epochs = 1, validation_loss = 1.9151597023010254**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XHV_tsNf0zI9",
        "outputId": "00baa20d-e77f-4010-d1bd-07d200a5f3e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing configuration: LR=2e-05, Batch Size=8, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 02:53, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.956300</td>\n",
              "      <td>1.918204</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.918204426765442, 'eval_runtime': 5.1355, 'eval_samples_per_second': 194.722, 'eval_steps_per_second': 24.34, 'epoch': 1.0}\n",
            "Testing configuration: LR=2e-05, Batch Size=8, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 08:41, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.986700</td>\n",
              "      <td>1.939724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.484500</td>\n",
              "      <td>1.985877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.215000</td>\n",
              "      <td>2.056471</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.0564706325531006, 'eval_runtime': 5.206, 'eval_samples_per_second': 192.087, 'eval_steps_per_second': 24.011, 'epoch': 3.0}\n",
            "Testing configuration: LR=2e-05, Batch Size=16, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 02:11, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.946600</td>\n",
              "      <td>1.915160</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9151597023010254, 'eval_runtime': 4.4799, 'eval_samples_per_second': 223.221, 'eval_steps_per_second': 14.063, 'epoch': 1.0}\n",
            "Testing configuration: LR=2e-05, Batch Size=16, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 06:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.961100</td>\n",
              "      <td>1.926324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.578800</td>\n",
              "      <td>1.952229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.367700</td>\n",
              "      <td>1.996490</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.996490240097046, 'eval_runtime': 4.4775, 'eval_samples_per_second': 223.339, 'eval_steps_per_second': 14.07, 'epoch': 3.0}\n",
            "Testing configuration: LR=3e-05, Batch Size=8, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 02:54, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.976000</td>\n",
              "      <td>1.933671</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9336711168289185, 'eval_runtime': 5.145, 'eval_samples_per_second': 194.365, 'eval_steps_per_second': 24.296, 'epoch': 1.0}\n",
            "Testing configuration: LR=3e-05, Batch Size=8, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 08:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.027600</td>\n",
              "      <td>1.972491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.382600</td>\n",
              "      <td>2.039524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.009900</td>\n",
              "      <td>2.165004</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.1650044918060303, 'eval_runtime': 5.1408, 'eval_samples_per_second': 194.524, 'eval_steps_per_second': 24.315, 'epoch': 3.0}\n",
            "Testing configuration: LR=3e-05, Batch Size=16, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 02:11, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.956600</td>\n",
              "      <td>1.923363</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9233630895614624, 'eval_runtime': 4.4727, 'eval_samples_per_second': 223.576, 'eval_steps_per_second': 14.085, 'epoch': 1.0}\n",
            "Testing configuration: LR=3e-05, Batch Size=16, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 06:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.985600</td>\n",
              "      <td>1.946692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.480700</td>\n",
              "      <td>1.986397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.186100</td>\n",
              "      <td>2.070349</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.0703492164611816, 'eval_runtime': 4.4899, 'eval_samples_per_second': 222.722, 'eval_steps_per_second': 14.032, 'epoch': 3.0}\n",
            "Testing configuration: LR=5e-05, Batch Size=8, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 02:53, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.017900</td>\n",
              "      <td>1.971304</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9713040590286255, 'eval_runtime': 5.194, 'eval_samples_per_second': 192.531, 'eval_steps_per_second': 24.066, 'epoch': 1.0}\n",
            "Testing configuration: LR=5e-05, Batch Size=8, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 08:42, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.113400</td>\n",
              "      <td>2.046349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.268500</td>\n",
              "      <td>2.138797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.750800</td>\n",
              "      <td>2.369820</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:05]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.3698198795318604, 'eval_runtime': 5.1351, 'eval_samples_per_second': 194.739, 'eval_steps_per_second': 24.342, 'epoch': 3.0}\n",
            "Testing configuration: LR=5e-05, Batch Size=16, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 02:11, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.984200</td>\n",
              "      <td>1.946302</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9463016986846924, 'eval_runtime': 4.4694, 'eval_samples_per_second': 223.741, 'eval_steps_per_second': 14.096, 'epoch': 1.0}\n",
            "Testing configuration: LR=5e-05, Batch Size=16, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-6079f8c1da14>:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 06:35, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.041800</td>\n",
              "      <td>1.996488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.352300</td>\n",
              "      <td>2.062188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.929100</td>\n",
              "      <td>2.219889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.2198894023895264, 'eval_runtime': 4.4756, 'eval_samples_per_second': 223.432, 'eval_steps_per_second': 14.076, 'epoch': 3.0}\n",
            "Best Configuration:\n",
            "{'learning_rate': 2e-05, 'batch_size': 16, 'epochs': 1}\n",
            "Best Evaluation Results:\n",
            "{'eval_loss': 1.9151597023010254, 'eval_runtime': 4.4799, 'eval_samples_per_second': 223.221, 'eval_steps_per_second': 14.063, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameter grid\n",
        "learning_rates = [2e-5, 3e-5, 5e-5]\n",
        "batch_sizes = [8, 16]\n",
        "epochs = [1, 3]\n",
        "\n",
        "# Store the best result\n",
        "best_result = None\n",
        "best_params = {}\n",
        "\n",
        "# Iterate over all hyperparameter combinations\n",
        "for lr, batch_size, epoch in itertools.product(learning_rates, batch_sizes, epochs):\n",
        "    print(f\"Testing configuration: LR={lr}, Batch Size={batch_size}, Epochs={epoch}\")\n",
        "\n",
        "    # Deep copy the initial model to ensure fresh start\n",
        "    model_mbart_tune = copy.deepcopy(model_mbart)\n",
        "    model_mbart_tune.config.pad_token_id = tokenizer_mbart.pad_token_id  # Set pad token ID\n",
        "\n",
        "    # Set a unique output directory for each configuration\n",
        "    output_dir = f\"./results/lr_{lr}_bs_{batch_size}_ep_{epoch}\"\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"epoch\",      # Evaluate once per epoch\n",
        "        logging_strategy=\"steps\",         # Log training loss every few steps\n",
        "        logging_steps=100,\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epoch,\n",
        "        weight_decay=0.01,\n",
        "        remove_unused_columns=False,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",                 # Disable W&B integration\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model_mbart_tune,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data_mbart_train,\n",
        "        eval_dataset=tokenized_data_mbart_dev,\n",
        "        tokenizer=tokenizer_mbart,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "    # Track the best configuration\n",
        "    if best_result is None or eval_results['eval_loss'] < best_result['eval_loss']:\n",
        "        best_result = eval_results\n",
        "        best_params = {\n",
        "            'learning_rate': lr,\n",
        "            'batch_size': batch_size,\n",
        "            'epochs': epoch\n",
        "        }\n",
        "\n",
        "# Print the best configuration\n",
        "print(\"Best Configuration:\")\n",
        "print(best_params)\n",
        "print(\"Best Evaluation Results:\")\n",
        "print(best_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVG55jwJkbtn"
      },
      "source": [
        "#### **3.2.2 Hyperparameter Tuning mBart with LoRA**\n",
        "Since a well-performing set of hyperparameters (learning_rate = 2e-05, batch_size = 16, epochs = 1) has already been identified during the above fine-tuning, it is reasonable to use these as a starting point for LoRA hyperparameter tuning. This is because using previously tuned traditional parameters provides a stable training baseline, reduces the search space, and allows focused optimization of LoRA-specific parameters, which have a greater impact on performance.\n",
        "\n",
        "**Best configuration:  r=8, alpha=64, dropout=0.0, validation loss = 1.967467**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "41lIoOprkihZ",
        "outputId": "d8ff5a94-a83c-4ac4-b43e-b07c9fe9cc38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing LoRA configuration: r=8, alpha=16, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.269500</td>\n",
              "      <td>2.186194</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.18619441986084, 'eval_runtime': 24.0318, 'eval_samples_per_second': 41.611, 'eval_steps_per_second': 5.201, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=8, alpha=16, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.218100</td>\n",
              "      <td>2.136166</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.1361663341522217, 'eval_runtime': 23.9919, 'eval_samples_per_second': 41.681, 'eval_steps_per_second': 5.21, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=8, alpha=16, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.228000</td>\n",
              "      <td>2.146366</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.1463656425476074, 'eval_runtime': 24.1613, 'eval_samples_per_second': 41.389, 'eval_steps_per_second': 5.174, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=8, alpha=32, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.072600</td>\n",
              "      <td>2.003432</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.003432035446167, 'eval_runtime': 24.0927, 'eval_samples_per_second': 41.506, 'eval_steps_per_second': 5.188, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=8, alpha=32, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.070800</td>\n",
              "      <td>2.004421</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.004420518875122, 'eval_runtime': 24.0156, 'eval_samples_per_second': 41.64, 'eval_steps_per_second': 5.205, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=8, alpha=32, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.072200</td>\n",
              "      <td>2.005456</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.005455732345581, 'eval_runtime': 24.0918, 'eval_samples_per_second': 41.508, 'eval_steps_per_second': 5.188, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=8, alpha=64, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 05:59, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.033000</td>\n",
              "      <td>1.967467</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9674665927886963, 'eval_runtime': 24.0202, 'eval_samples_per_second': 41.632, 'eval_steps_per_second': 5.204, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=8, alpha=64, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.031000</td>\n",
              "      <td>1.968252</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9682523012161255, 'eval_runtime': 24.0101, 'eval_samples_per_second': 41.649, 'eval_steps_per_second': 5.206, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=8, alpha=64, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.032000</td>\n",
              "      <td>1.968983</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9689826965332031, 'eval_runtime': 24.02, 'eval_samples_per_second': 41.632, 'eval_steps_per_second': 5.204, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=16, alpha=16, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.280500</td>\n",
              "      <td>2.197836</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.197836399078369, 'eval_runtime': 24.0847, 'eval_samples_per_second': 41.52, 'eval_steps_per_second': 5.19, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=16, alpha=16, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.280000</td>\n",
              "      <td>2.202169</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.2021687030792236, 'eval_runtime': 24.1264, 'eval_samples_per_second': 41.448, 'eval_steps_per_second': 5.181, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=16, alpha=16, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.290800</td>\n",
              "      <td>2.214560</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.2145602703094482, 'eval_runtime': 24.0837, 'eval_samples_per_second': 41.522, 'eval_steps_per_second': 5.19, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=16, alpha=32, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.076000</td>\n",
              "      <td>2.006214</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.006213665008545, 'eval_runtime': 24.1193, 'eval_samples_per_second': 41.461, 'eval_steps_per_second': 5.183, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=16, alpha=32, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.073900</td>\n",
              "      <td>2.007113</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.007113456726074, 'eval_runtime': 23.9851, 'eval_samples_per_second': 41.693, 'eval_steps_per_second': 5.212, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=16, alpha=32, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.075300</td>\n",
              "      <td>2.008230</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.0082297325134277, 'eval_runtime': 24.0578, 'eval_samples_per_second': 41.567, 'eval_steps_per_second': 5.196, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=16, alpha=64, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.039500</td>\n",
              "      <td>1.973194</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9731940031051636, 'eval_runtime': 24.043, 'eval_samples_per_second': 41.592, 'eval_steps_per_second': 5.199, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=16, alpha=64, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.036900</td>\n",
              "      <td>1.973644</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9736441373825073, 'eval_runtime': 24.127, 'eval_samples_per_second': 41.447, 'eval_steps_per_second': 5.181, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=16, alpha=64, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:02, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.037400</td>\n",
              "      <td>1.973854</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9738541841506958, 'eval_runtime': 24.1835, 'eval_samples_per_second': 41.35, 'eval_steps_per_second': 5.169, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=32, alpha=16, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.260900</td>\n",
              "      <td>2.176215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.176215410232544, 'eval_runtime': 24.1779, 'eval_samples_per_second': 41.36, 'eval_steps_per_second': 5.17, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=32, alpha=16, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:04, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.266600</td>\n",
              "      <td>2.187254</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.187253713607788, 'eval_runtime': 24.1739, 'eval_samples_per_second': 41.367, 'eval_steps_per_second': 5.171, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=32, alpha=16, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:03, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.276200</td>\n",
              "      <td>2.197995</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.197995185852051, 'eval_runtime': 24.2015, 'eval_samples_per_second': 41.32, 'eval_steps_per_second': 5.165, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=32, alpha=32, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.075400</td>\n",
              "      <td>2.005779</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.0057790279388428, 'eval_runtime': 24.1308, 'eval_samples_per_second': 41.441, 'eval_steps_per_second': 5.18, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=32, alpha=32, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:04, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.073200</td>\n",
              "      <td>2.006605</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.0066046714782715, 'eval_runtime': 24.2807, 'eval_samples_per_second': 41.185, 'eval_steps_per_second': 5.148, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=32, alpha=32, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:03, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.074700</td>\n",
              "      <td>2.007794</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.0077943801879883, 'eval_runtime': 24.105, 'eval_samples_per_second': 41.485, 'eval_steps_per_second': 5.186, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=32, alpha=64, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.041200</td>\n",
              "      <td>1.974903</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:23]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.974902868270874, 'eval_runtime': 24.1341, 'eval_samples_per_second': 41.435, 'eval_steps_per_second': 5.179, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=32, alpha=64, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:03, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.038600</td>\n",
              "      <td>1.975254</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9752542972564697, 'eval_runtime': 24.2397, 'eval_samples_per_second': 41.255, 'eval_steps_per_second': 5.157, 'epoch': 1.0}\n",
            "Testing LoRA configuration: r=32, alpha=64, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-64-377d6b3748e3>:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 06:04, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.039600</td>\n",
              "      <td>1.975913</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 1.9759129285812378, 'eval_runtime': 24.3065, 'eval_samples_per_second': 41.141, 'eval_steps_per_second': 5.143, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 2e-05\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "\n",
        "r_values = [8, 16, 32]  # Low-rank dimensions\n",
        "alpha_values = [16, 32, 64]  # Scaling factors\n",
        "dropout_values = [0.0, 0.1, 0.2]  # Dropout rates\n",
        "\n",
        "for r, alpha, dropout in product(r_values, alpha_values, dropout_values):\n",
        "    print(f\"Testing LoRA configuration: r={r}, alpha={alpha}, dropout={dropout}\")\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "        inference_mode=False,\n",
        "        r=r,\n",
        "        lora_alpha=alpha,\n",
        "        lora_dropout=dropout,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    )\n",
        "\n",
        "    # Apply LoRA\n",
        "    model_mbart_lora_tune = copy.deepcopy(model_mbart)\n",
        "    model_mbart_lora_tune = get_peft_model(model_mbart_lora_tune, lora_config)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/lora_r_{r}_alpha_{alpha}_dropout_{dropout}\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        weight_decay=0.01,\n",
        "        remove_unused_columns=False,\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=100,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Train and evaluate\n",
        "    trainer = Trainer(\n",
        "        model=model_mbart_lora_tune,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data_mbart_train,\n",
        "        eval_dataset=tokenized_data_mbart_dev,\n",
        "        tokenizer=tokenizer_mbart,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_results}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJR8LtahC5m2"
      },
      "source": [
        "#### **3.2.3 Hyperparameter Tuning mBart with Layer Freezing**\n",
        "**Best Configuration: Encoder layers frozen = 8, Decoder layers frozen = 8, eval_loss =  1.9147837162017822**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e-B_3uDnDq_n",
        "outputId": "1671bace-9909-49f5-c095-929159d19607"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Layer Freezing configuration: encoder_layers=8, decoder_layers=8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f29569041e67>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.980300</td>\n",
              "      <td>1.914784</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results for encoder=8, decoder=8: {'eval_loss': 1.9147837162017822, 'eval_runtime': 24.4993, 'eval_samples_per_second': 40.817, 'eval_steps_per_second': 5.102, 'epoch': 1.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=8, decoder_layers=9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f29569041e67>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:28, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.982700</td>\n",
              "      <td>1.917383</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results for encoder=8, decoder=9: {'eval_loss': 1.917382836341858, 'eval_runtime': 24.5331, 'eval_samples_per_second': 40.761, 'eval_steps_per_second': 5.095, 'epoch': 1.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=8, decoder_layers=10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f29569041e67>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:19, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.985700</td>\n",
              "      <td>1.920089</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results for encoder=8, decoder=10: {'eval_loss': 1.9200888872146606, 'eval_runtime': 24.4048, 'eval_samples_per_second': 40.976, 'eval_steps_per_second': 5.122, 'epoch': 1.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=9, decoder_layers=8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f29569041e67>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:29, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.983200</td>\n",
              "      <td>1.918116</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results for encoder=9, decoder=8: {'eval_loss': 1.9181159734725952, 'eval_runtime': 24.4886, 'eval_samples_per_second': 40.835, 'eval_steps_per_second': 5.104, 'epoch': 1.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=9, decoder_layers=9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f29569041e67>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:22, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.985900</td>\n",
              "      <td>1.920977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results for encoder=9, decoder=9: {'eval_loss': 1.9209765195846558, 'eval_runtime': 24.4172, 'eval_samples_per_second': 40.955, 'eval_steps_per_second': 5.119, 'epoch': 1.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=9, decoder_layers=10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f29569041e67>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.989300</td>\n",
              "      <td>1.923922</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results for encoder=9, decoder=10: {'eval_loss': 1.9239217042922974, 'eval_runtime': 24.4221, 'eval_samples_per_second': 40.947, 'eval_steps_per_second': 5.118, 'epoch': 1.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=10, decoder_layers=8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f29569041e67>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.987300</td>\n",
              "      <td>1.922695</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results for encoder=10, decoder=8: {'eval_loss': 1.9226946830749512, 'eval_runtime': 24.4811, 'eval_samples_per_second': 40.848, 'eval_steps_per_second': 5.106, 'epoch': 1.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=10, decoder_layers=9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f29569041e67>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:16, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.990600</td>\n",
              "      <td>1.926136</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results for encoder=10, decoder=9: {'eval_loss': 1.926135540008545, 'eval_runtime': 24.4352, 'eval_samples_per_second': 40.925, 'eval_steps_per_second': 5.116, 'epoch': 1.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=10, decoder_layers=10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-65-f29569041e67>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 08:08, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.994200</td>\n",
              "      <td>1.929239</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results for encoder=10, decoder=10: {'eval_loss': 1.9292387962341309, 'eval_runtime': 24.4241, 'eval_samples_per_second': 40.943, 'eval_steps_per_second': 5.118, 'epoch': 1.0}\n",
            "\n",
            "Best Configuration:\n",
            "Encoder layers frozen: 8\n",
            "Decoder layers frozen: 8\n",
            "Best Evaluation Results: {'eval_loss': 1.9147837162017822, 'eval_runtime': 24.4993, 'eval_samples_per_second': 40.817, 'eval_steps_per_second': 5.102, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameter search space\n",
        "freeze_encoder_layers_values = [8, 9, 10]  # Number of encoder layers to freeze\n",
        "freeze_decoder_layers_values = [8, 9, 10]  # Number of decoder layers to freeze\n",
        "\n",
        "learning_rate = 2e-05\n",
        "batch_size = 16\n",
        "epochs = 1\n",
        "\n",
        "# Variables to track the best configuration\n",
        "best_config = None\n",
        "best_eval_result = None\n",
        "best_metric = float(\"inf\")  # Assuming lower is better (e.g., validation loss)\n",
        "\n",
        "for freeze_encoder_layers, freeze_decoder_layers in product(freeze_encoder_layers_values, freeze_decoder_layers_values):\n",
        "    print(f\"Testing Layer Freezing configuration: encoder_layers={freeze_encoder_layers}, decoder_layers={freeze_decoder_layers}\")\n",
        "\n",
        "    # Create a new model copy for Layer Freezing\n",
        "    model_mbart_freeze_tune = copy.deepcopy(model_mbart)\n",
        "\n",
        "    # Apply Layer Freezing\n",
        "    for name, param in model_mbart_freeze_tune.named_parameters():\n",
        "        # Freeze encoder layers\n",
        "        if \"encoder.layers\" in name:\n",
        "            layer_num = int(name.split(\".\")[3])  # Extract layer index\n",
        "            if layer_num < freeze_encoder_layers:\n",
        "                param.requires_grad = False\n",
        "        # Freeze decoder layers\n",
        "        elif \"decoder.layers\" in name:\n",
        "            layer_num = int(name.split(\".\")[3])  # Extract layer index\n",
        "            if layer_num < freeze_decoder_layers:\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            param.requires_grad = True  # Keep other parameters trainable\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/freeze_encoder_{freeze_encoder_layers}_decoder_{freeze_decoder_layers}\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        weight_decay=0.01,\n",
        "        remove_unused_columns=False,\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=100,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Train and evaluate\n",
        "    trainer = Trainer(\n",
        "        model=model_mbart_freeze_tune,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data_mbart_train,\n",
        "        eval_dataset=tokenized_data_mbart_dev,\n",
        "        tokenizer=tokenizer_mbart,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results for encoder={freeze_encoder_layers}, decoder={freeze_decoder_layers}: {eval_results}\")\n",
        "\n",
        "    # Update best configuration if the current result is better\n",
        "    current_metric = eval_results[\"eval_loss\"]  # Replace with the appropriate key from eval_results\n",
        "    if current_metric < best_metric:\n",
        "        best_metric = current_metric\n",
        "        best_config = (freeze_encoder_layers, freeze_decoder_layers)\n",
        "        best_eval_result = eval_results\n",
        "\n",
        "# Print the best configuration and its results\n",
        "print(\"\\nBest Configuration:\")\n",
        "print(f\"Encoder layers frozen: {best_config[0]}\")\n",
        "print(f\"Decoder layers frozen: {best_config[1]}\")\n",
        "print(f\"Best Evaluation Results: {best_eval_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvEilsRXY4FV"
      },
      "source": [
        "### **3.3 M2M100**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikWMcrgpZT2b"
      },
      "source": [
        "#### **3.3.1 Hyperparameter Tuning M2M100 with Regular Fine-Tune Technique**\n",
        "**Best Configuration: learning_rate = 2e-05, batch_size = 16, epochs = 3, validation loss = 2.655961751937866**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nuiYU3HZY9cs",
        "outputId": "d7e133f9-743b-4acc-d24a-a656e9ea127a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing configuration: LR=2e-05, Batch Size=8, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 02:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.838000</td>\n",
              "      <td>2.673186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.6731860637664795, 'eval_runtime': 4.2035, 'eval_samples_per_second': 237.898, 'eval_steps_per_second': 29.737, 'epoch': 1.0}\n",
            "Testing configuration: LR=2e-05, Batch Size=8, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 06:45, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.861400</td>\n",
              "      <td>2.673662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.301300</td>\n",
              "      <td>2.664213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.040600</td>\n",
              "      <td>2.685887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.6858866214752197, 'eval_runtime': 4.2191, 'eval_samples_per_second': 237.017, 'eval_steps_per_second': 29.627, 'epoch': 3.0}\n",
            "Testing configuration: LR=2e-05, Batch Size=16, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 01:08, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.859500</td>\n",
              "      <td>2.698718</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.6987178325653076, 'eval_runtime': 2.248, 'eval_samples_per_second': 444.833, 'eval_steps_per_second': 28.024, 'epoch': 1.0}\n",
            "Testing configuration: LR=2e-05, Batch Size=16, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.854500</td>\n",
              "      <td>2.677058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.429400</td>\n",
              "      <td>2.647158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.166500</td>\n",
              "      <td>2.655962</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.655961751937866, 'eval_runtime': 2.2501, 'eval_samples_per_second': 444.42, 'eval_steps_per_second': 27.998, 'epoch': 3.0}\n",
            "Testing configuration: LR=3e-05, Batch Size=8, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 02:15, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.834000</td>\n",
              "      <td>2.665765</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.665764570236206, 'eval_runtime': 4.2025, 'eval_samples_per_second': 237.952, 'eval_steps_per_second': 29.744, 'epoch': 1.0}\n",
            "Testing configuration: LR=3e-05, Batch Size=8, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 06:47, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.905500</td>\n",
              "      <td>2.720311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.217300</td>\n",
              "      <td>2.704587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.852800</td>\n",
              "      <td>2.739367</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7393667697906494, 'eval_runtime': 4.2274, 'eval_samples_per_second': 236.553, 'eval_steps_per_second': 29.569, 'epoch': 3.0}\n",
            "Testing configuration: LR=3e-05, Batch Size=16, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 01:08, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.843300</td>\n",
              "      <td>2.678771</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.6787710189819336, 'eval_runtime': 2.2424, 'eval_samples_per_second': 445.948, 'eval_steps_per_second': 28.095, 'epoch': 1.0}\n",
            "Testing configuration: LR=3e-05, Batch Size=16, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:26, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.865500</td>\n",
              "      <td>2.689855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.335700</td>\n",
              "      <td>2.675825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.992300</td>\n",
              "      <td>2.694527</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.6945269107818604, 'eval_runtime': 2.26, 'eval_samples_per_second': 442.477, 'eval_steps_per_second': 27.876, 'epoch': 3.0}\n",
            "Testing configuration: LR=5e-05, Batch Size=8, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 02:15, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.872600</td>\n",
              "      <td>2.706617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7066168785095215, 'eval_runtime': 4.1999, 'eval_samples_per_second': 238.103, 'eval_steps_per_second': 29.763, 'epoch': 1.0}\n",
            "Testing configuration: LR=5e-05, Batch Size=8, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3000/3000 06:46, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.016500</td>\n",
              "      <td>2.827892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.143600</td>\n",
              "      <td>2.793673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.615500</td>\n",
              "      <td>2.830040</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:04]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8300395011901855, 'eval_runtime': 4.2502, 'eval_samples_per_second': 235.281, 'eval_steps_per_second': 29.41, 'epoch': 3.0}\n",
            "Testing configuration: LR=5e-05, Batch Size=16, Epochs=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 01:08, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.853500</td>\n",
              "      <td>2.682950</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.682950258255005, 'eval_runtime': 2.282, 'eval_samples_per_second': 438.206, 'eval_steps_per_second': 27.607, 'epoch': 1.0}\n",
            "Testing configuration: LR=5e-05, Batch Size=16, Epochs=3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-20-1c4afd065de1>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.929700</td>\n",
              "      <td>2.764360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.234000</td>\n",
              "      <td>2.734500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.754300</td>\n",
              "      <td>2.768544</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7685439586639404, 'eval_runtime': 2.2224, 'eval_samples_per_second': 449.962, 'eval_steps_per_second': 28.348, 'epoch': 3.0}\n",
            "Best Configuration:\n",
            "{'learning_rate': 2e-05, 'batch_size': 16, 'epochs': 3}\n",
            "Best Evaluation Results:\n",
            "{'eval_loss': 2.655961751937866, 'eval_runtime': 2.2501, 'eval_samples_per_second': 444.42, 'eval_steps_per_second': 27.998, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "learning_rates = [2e-5, 3e-5, 5e-5]\n",
        "batch_sizes = [8, 16]\n",
        "epochs = [1, 3]\n",
        "\n",
        "best_result = None\n",
        "best_params = {}\n",
        "\n",
        "for lr, batch_size, epoch in itertools.product(\n",
        "    learning_rates, batch_sizes, epochs\n",
        "):\n",
        "    print(f\"Testing configuration: LR={lr}, Batch Size={batch_size}, Epochs={epoch}\")\n",
        "\n",
        "    model_mtm_tune = copy.deepcopy(model_mtm)\n",
        "\n",
        "    output_dir = f\"./results/lr_{lr}_bs_{batch_size}_ep_{epoch}\"\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=100,\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epoch,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        remove_unused_columns=False,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model_mtm_tune,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data_mtm_train,\n",
        "        eval_dataset=tokenized_data_mtm_dev,\n",
        "        tokenizer=tokenizer_mtm\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "    if best_result is None or eval_results['eval_loss'] < best_result['eval_loss']:\n",
        "        best_result = eval_results\n",
        "        best_params = {\n",
        "            'learning_rate': lr,\n",
        "            'batch_size': batch_size,\n",
        "            'epochs': epoch\n",
        "        }\n",
        "\n",
        "print(\"Best Configuration:\")\n",
        "print(best_params)\n",
        "print(\"Best Evaluation Results:\")\n",
        "print(best_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUAae6juqj7c"
      },
      "source": [
        "#### **3.3.2 Hyperparameter Tuning M2M100 with LoRA**\n",
        "**Best configuration: r = 8, lora_alpha = 64, lora_dropout = 0.0, validation loss = 2.791430950164795**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0MOTVOphqJVO",
        "outputId": "35efbd1d-29a4-4cad-fb56-aff338b97b5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing LoRA configuration: r=8, alpha=16, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.132100</td>\n",
              "      <td>2.899754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.035600</td>\n",
              "      <td>2.852401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.061000</td>\n",
              "      <td>2.842442</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8424415588378906, 'eval_runtime': 3.6997, 'eval_samples_per_second': 270.291, 'eval_steps_per_second': 17.028, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=8, alpha=16, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:27, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.138800</td>\n",
              "      <td>2.899434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.030700</td>\n",
              "      <td>2.852629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.059800</td>\n",
              "      <td>2.842457</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.842456579208374, 'eval_runtime': 3.7066, 'eval_samples_per_second': 269.786, 'eval_steps_per_second': 16.997, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=8, alpha=16, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.142300</td>\n",
              "      <td>2.902096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.033000</td>\n",
              "      <td>2.854503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.061900</td>\n",
              "      <td>2.844099</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.844099283218384, 'eval_runtime': 3.7644, 'eval_samples_per_second': 265.644, 'eval_steps_per_second': 16.736, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=8, alpha=32, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.085700</td>\n",
              "      <td>2.859414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.005700</td>\n",
              "      <td>2.825516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.031200</td>\n",
              "      <td>2.816317</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.816316843032837, 'eval_runtime': 3.7895, 'eval_samples_per_second': 263.89, 'eval_steps_per_second': 16.625, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=8, alpha=32, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.094200</td>\n",
              "      <td>2.860676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.002000</td>\n",
              "      <td>2.827018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.032200</td>\n",
              "      <td>2.817689</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8176894187927246, 'eval_runtime': 3.7152, 'eval_samples_per_second': 269.164, 'eval_steps_per_second': 16.957, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=8, alpha=32, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.097100</td>\n",
              "      <td>2.862704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.004100</td>\n",
              "      <td>2.828652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.034500</td>\n",
              "      <td>2.819312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8193116188049316, 'eval_runtime': 3.6922, 'eval_samples_per_second': 270.842, 'eval_steps_per_second': 17.063, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=8, alpha=64, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.055000</td>\n",
              "      <td>2.830839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.976900</td>\n",
              "      <td>2.801368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.999900</td>\n",
              "      <td>2.791431</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.791430950164795, 'eval_runtime': 3.7583, 'eval_samples_per_second': 266.079, 'eval_steps_per_second': 16.763, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=8, alpha=64, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.064200</td>\n",
              "      <td>2.832751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.975200</td>\n",
              "      <td>2.804096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.003500</td>\n",
              "      <td>2.794098</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.794098138809204, 'eval_runtime': 3.7299, 'eval_samples_per_second': 268.103, 'eval_steps_per_second': 16.891, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=8, alpha=64, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.067200</td>\n",
              "      <td>2.834798</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.978000</td>\n",
              "      <td>2.806042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.006600</td>\n",
              "      <td>2.796152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.796152353286743, 'eval_runtime': 3.8743, 'eval_samples_per_second': 258.108, 'eval_steps_per_second': 16.261, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=16, alpha=16, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:25, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.136900</td>\n",
              "      <td>2.903526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.036600</td>\n",
              "      <td>2.853177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.061600</td>\n",
              "      <td>2.842710</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.842710494995117, 'eval_runtime': 3.7039, 'eval_samples_per_second': 269.989, 'eval_steps_per_second': 17.009, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=16, alpha=16, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.145100</td>\n",
              "      <td>2.904814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.032700</td>\n",
              "      <td>2.854481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.062200</td>\n",
              "      <td>2.844057</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.844057083129883, 'eval_runtime': 3.7589, 'eval_samples_per_second': 266.038, 'eval_steps_per_second': 16.76, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=16, alpha=16, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.148200</td>\n",
              "      <td>2.907256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.034800</td>\n",
              "      <td>2.855808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.063800</td>\n",
              "      <td>2.845312</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8453116416931152, 'eval_runtime': 3.8189, 'eval_samples_per_second': 261.856, 'eval_steps_per_second': 16.497, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=16, alpha=32, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:24, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.086600</td>\n",
              "      <td>2.859956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.005500</td>\n",
              "      <td>2.825960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.032000</td>\n",
              "      <td>2.816813</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8168129920959473, 'eval_runtime': 3.6945, 'eval_samples_per_second': 270.675, 'eval_steps_per_second': 17.053, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=16, alpha=32, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.094500</td>\n",
              "      <td>2.861740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.002500</td>\n",
              "      <td>2.828042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.033200</td>\n",
              "      <td>2.818718</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.818718433380127, 'eval_runtime': 3.7555, 'eval_samples_per_second': 266.273, 'eval_steps_per_second': 16.775, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=16, alpha=32, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:32, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.097200</td>\n",
              "      <td>2.863936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.004700</td>\n",
              "      <td>2.829329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.035100</td>\n",
              "      <td>2.820055</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8200550079345703, 'eval_runtime': 3.7998, 'eval_samples_per_second': 263.174, 'eval_steps_per_second': 16.58, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=16, alpha=64, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.056200</td>\n",
              "      <td>2.831883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.976600</td>\n",
              "      <td>2.802105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.000700</td>\n",
              "      <td>2.792005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7920048236846924, 'eval_runtime': 3.6739, 'eval_samples_per_second': 272.191, 'eval_steps_per_second': 17.148, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=16, alpha=64, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.064500</td>\n",
              "      <td>2.833652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.974800</td>\n",
              "      <td>2.804765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.003600</td>\n",
              "      <td>2.794752</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7947518825531006, 'eval_runtime': 3.7311, 'eval_samples_per_second': 268.018, 'eval_steps_per_second': 16.885, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=16, alpha=64, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.066900</td>\n",
              "      <td>2.835549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.977600</td>\n",
              "      <td>2.806368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.006200</td>\n",
              "      <td>2.796460</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7964603900909424, 'eval_runtime': 3.6864, 'eval_samples_per_second': 271.265, 'eval_steps_per_second': 17.09, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=32, alpha=16, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:23, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.137400</td>\n",
              "      <td>2.904030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.037300</td>\n",
              "      <td>2.853239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.061600</td>\n",
              "      <td>2.842674</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8426735401153564, 'eval_runtime': 3.6849, 'eval_samples_per_second': 271.376, 'eval_steps_per_second': 17.097, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=32, alpha=16, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.146000</td>\n",
              "      <td>2.905088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.033100</td>\n",
              "      <td>2.854301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.061800</td>\n",
              "      <td>2.843768</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8437676429748535, 'eval_runtime': 3.8144, 'eval_samples_per_second': 262.166, 'eval_steps_per_second': 16.516, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=32, alpha=16, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.149100</td>\n",
              "      <td>2.907387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.034900</td>\n",
              "      <td>2.855451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.063400</td>\n",
              "      <td>2.844864</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8448636531829834, 'eval_runtime': 3.7557, 'eval_samples_per_second': 266.263, 'eval_steps_per_second': 16.775, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=32, alpha=32, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:22, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.088600</td>\n",
              "      <td>2.861762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.006400</td>\n",
              "      <td>2.826118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.032100</td>\n",
              "      <td>2.816821</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.8168208599090576, 'eval_runtime': 3.6515, 'eval_samples_per_second': 273.863, 'eval_steps_per_second': 17.253, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=32, alpha=32, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:30, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.096300</td>\n",
              "      <td>2.862282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.003200</td>\n",
              "      <td>2.827423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.032900</td>\n",
              "      <td>2.818080</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.818079710006714, 'eval_runtime': 3.7697, 'eval_samples_per_second': 265.276, 'eval_steps_per_second': 16.712, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=32, alpha=32, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.098700</td>\n",
              "      <td>2.864204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.005300</td>\n",
              "      <td>2.828893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.034800</td>\n",
              "      <td>2.819568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.819567918777466, 'eval_runtime': 3.7805, 'eval_samples_per_second': 264.518, 'eval_steps_per_second': 16.665, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=32, alpha=64, dropout=0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:27, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.056200</td>\n",
              "      <td>2.831829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.977600</td>\n",
              "      <td>2.802418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.001200</td>\n",
              "      <td>2.792586</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.792585611343384, 'eval_runtime': 3.6608, 'eval_samples_per_second': 273.167, 'eval_steps_per_second': 17.21, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=32, alpha=64, dropout=0.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.064900</td>\n",
              "      <td>2.833368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.976100</td>\n",
              "      <td>2.804645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.003700</td>\n",
              "      <td>2.794915</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7949154376983643, 'eval_runtime': 3.7462, 'eval_samples_per_second': 266.937, 'eval_steps_per_second': 16.817, 'epoch': 3.0}\n",
            "Testing LoRA configuration: r=32, alpha=64, dropout=0.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-66-3d85924d9212>:63: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 03:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.067600</td>\n",
              "      <td>2.835190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.978700</td>\n",
              "      <td>2.806263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.006200</td>\n",
              "      <td>2.796576</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7965757846832275, 'eval_runtime': 3.7868, 'eval_samples_per_second': 264.076, 'eval_steps_per_second': 16.637, 'epoch': 3.0}\n",
            "Best LoRA Configuration:\n",
            "{'r': 8, 'lora_alpha': 64, 'lora_dropout': 0.0}\n",
            "Best Evaluation Results:\n",
            "{'eval_loss': 2.791430950164795, 'eval_runtime': 3.7583, 'eval_samples_per_second': 266.079, 'eval_steps_per_second': 16.763, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameter grid for LoRA\n",
        "r_values = [8, 16, 32]  # Low-rank dimensions\n",
        "alpha_values = [16, 32, 64]  # Scaling factors\n",
        "dropout_values = [0.0, 0.1, 0.2]  # Dropout rates\n",
        "\n",
        "# Fixed learning rate, batch size, and epochs (from prior tuning)\n",
        "learning_rate = 2e-5\n",
        "batch_size = 16\n",
        "epochs = 3\n",
        "\n",
        "# Store the best result for LoRA tuning\n",
        "best_lora_result = None\n",
        "best_lora_params = {}\n",
        "\n",
        "# Iterate over all combinations of LoRA hyperparameters\n",
        "for r, alpha, dropout in itertools.product(r_values, alpha_values, dropout_values):\n",
        "    print(f\"Testing LoRA configuration: r={r}, alpha={alpha}, dropout={dropout}\")\n",
        "\n",
        "    # Create a deep copy of the original model for LoRA\n",
        "    model_mtm_lora_tune = copy.deepcopy(model_mtm)\n",
        "\n",
        "    # Configure LoRA\n",
        "    mtm_lora_config = LoraConfig(\n",
        "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "        inference_mode=False,\n",
        "        r=r,\n",
        "        lora_alpha=alpha,\n",
        "        lora_dropout=dropout,\n",
        "        target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    )\n",
        "\n",
        "    # Apply LoRA to M2M100 model\n",
        "    model_mtm_lora_tune = get_peft_model(model_mtm_lora_tune, mtm_lora_config)\n",
        "\n",
        "    # Ensure only LoRA layers are trainable\n",
        "    for name, param in model_mtm_lora.named_parameters():\n",
        "        if any(layer in name for layer in mtm_lora_config.target_modules):\n",
        "            param.requires_grad = True  # LoRA layers\n",
        "        else:\n",
        "            param.requires_grad = False  # Non-LoRA layers\n",
        "\n",
        "    # Set a unique output directory for each LoRA configuration\n",
        "    output_dir = f\"./results/lora_r_{r}_alpha_{alpha}_dropout_{dropout}\"\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=100,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        remove_unused_columns=False,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model_mtm_lora_tune,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data_mtm_train,\n",
        "        eval_dataset=tokenized_data_mtm_dev,\n",
        "        tokenizer=tokenizer_mtm,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "    # Track the best configuration\n",
        "    if best_lora_result is None or eval_results['eval_loss'] < best_lora_result['eval_loss']:\n",
        "        best_lora_result = eval_results\n",
        "        best_lora_params = {\n",
        "            'r': r,\n",
        "            'lora_alpha': alpha,\n",
        "            'lora_dropout': dropout,\n",
        "        }\n",
        "\n",
        "# Print the best LoRA configuration and results\n",
        "print(\"Best LoRA Configuration:\")\n",
        "print(best_lora_params)\n",
        "print(\"Best Evaluation Results:\")\n",
        "print(best_lora_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtbitlEbDHdJ"
      },
      "source": [
        "#### **3.3.3 Hyperparameter Tuning M2M with Layer Freezing**\n",
        "**Best Configuration: freeze_encoder_layers =  8, freeze_decoder_layers = 8, eval_loss = 2.7465786933898926**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aCrFMZXEE8mz",
        "outputId": "21bd348d-29f1-4b61-9776-14d2861ddfb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing Layer Freezing configuration: encoder_layers=8, decoder_layers=8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-c7a80e31ea6b>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 05:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.999300</td>\n",
              "      <td>2.774960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.758300</td>\n",
              "      <td>2.745686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.579800</td>\n",
              "      <td>2.746579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7465786933898926, 'eval_runtime': 2.7368, 'eval_samples_per_second': 365.386, 'eval_steps_per_second': 23.019, 'epoch': 3.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=8, decoder_layers=9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-c7a80e31ea6b>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 05:10, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.012900</td>\n",
              "      <td>2.788489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.788600</td>\n",
              "      <td>2.760466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.624200</td>\n",
              "      <td>2.759859</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7598588466644287, 'eval_runtime': 2.7138, 'eval_samples_per_second': 368.485, 'eval_steps_per_second': 23.215, 'epoch': 3.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=8, decoder_layers=10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-c7a80e31ea6b>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 04:58, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.025700</td>\n",
              "      <td>2.799694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.819700</td>\n",
              "      <td>2.772981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.666000</td>\n",
              "      <td>2.771988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7719881534576416, 'eval_runtime': 2.7139, 'eval_samples_per_second': 368.477, 'eval_steps_per_second': 23.214, 'epoch': 3.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=9, decoder_layers=8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-c7a80e31ea6b>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 05:14, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.007400</td>\n",
              "      <td>2.780797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.776000</td>\n",
              "      <td>2.750747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.606400</td>\n",
              "      <td>2.750795</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.7507948875427246, 'eval_runtime': 2.7219, 'eval_samples_per_second': 367.391, 'eval_steps_per_second': 23.146, 'epoch': 3.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=9, decoder_layers=9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-c7a80e31ea6b>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 05:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.021400</td>\n",
              "      <td>2.795025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.807500</td>\n",
              "      <td>2.766944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.652100</td>\n",
              "      <td>2.765029</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.765029191970825, 'eval_runtime': 2.6979, 'eval_samples_per_second': 370.661, 'eval_steps_per_second': 23.352, 'epoch': 3.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=9, decoder_layers=10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-c7a80e31ea6b>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 04:48, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.035400</td>\n",
              "      <td>2.806822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.840000</td>\n",
              "      <td>2.779782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.695300</td>\n",
              "      <td>2.778074</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.778073787689209, 'eval_runtime': 2.6997, 'eval_samples_per_second': 370.411, 'eval_steps_per_second': 23.336, 'epoch': 3.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=10, decoder_layers=8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-c7a80e31ea6b>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 05:04, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.017400</td>\n",
              "      <td>2.790179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.796100</td>\n",
              "      <td>2.757497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.637000</td>\n",
              "      <td>2.757425</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.757425308227539, 'eval_runtime': 2.7256, 'eval_samples_per_second': 366.895, 'eval_steps_per_second': 23.114, 'epoch': 3.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=10, decoder_layers=9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-c7a80e31ea6b>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 04:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.034400</td>\n",
              "      <td>2.805163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.828900</td>\n",
              "      <td>2.775208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.684400</td>\n",
              "      <td>2.772708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.77270770072937, 'eval_runtime': 2.7284, 'eval_samples_per_second': 366.511, 'eval_steps_per_second': 23.09, 'epoch': 3.0}\n",
            "Testing Layer Freezing configuration: encoder_layers=10, decoder_layers=10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-16-c7a80e31ea6b>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1500/1500 04:39, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.050900</td>\n",
              "      <td>2.818181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.863100</td>\n",
              "      <td>2.789517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.729200</td>\n",
              "      <td>2.787092</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation results: {'eval_loss': 2.787092447280884, 'eval_runtime': 2.7197, 'eval_samples_per_second': 367.693, 'eval_steps_per_second': 23.165, 'epoch': 3.0}\n",
            "Best Layer Freezing Configuration:\n",
            "{'freeze_encoder_layers': 8, 'freeze_decoder_layers': 8}\n",
            "Best Evaluation Results:\n",
            "{'eval_loss': 2.7465786933898926, 'eval_runtime': 2.7368, 'eval_samples_per_second': 365.386, 'eval_steps_per_second': 23.019, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter grid for Layer Freezing\n",
        "freeze_encoder_layers_values = [8, 9, 10]  # Number of encoder layers to freeze\n",
        "freeze_decoder_layers_values = [8, 9, 10]   # Number of decoder layers to freeze\n",
        "\n",
        "# Fixed learning rate, batch size, and epochs (from prior tuning)\n",
        "learning_rate = 2e-5\n",
        "batch_size = 16\n",
        "epochs = 3\n",
        "\n",
        "# Store the best result for Layer Freezing tuning\n",
        "best_freeze_result = None\n",
        "best_freeze_params = {}\n",
        "\n",
        "# Iterate over all combinations of Layer Freezing hyperparameters\n",
        "for freeze_encoder_layers, freeze_decoder_layers in itertools.product(freeze_encoder_layers_values, freeze_decoder_layers_values):\n",
        "    print(f\"Testing Layer Freezing configuration: encoder_layers={freeze_encoder_layers}, decoder_layers={freeze_decoder_layers}\")\n",
        "\n",
        "    # Create a deep copy of the original model for Layer Freezing\n",
        "    model_mtm_freeze_tune = copy.deepcopy(model_mtm)\n",
        "\n",
        "    # Apply Layer Freezing\n",
        "    for name, param in model_mtm_freeze_tune.named_parameters():\n",
        "        # Freeze encoder layers\n",
        "        if \"encoder.layers\" in name:\n",
        "            layer_num = int(name.split(\".\")[3])  # Extract the layer index\n",
        "            if layer_num < freeze_encoder_layers:\n",
        "                param.requires_grad = False\n",
        "        # Freeze decoder layers\n",
        "        elif \"decoder.layers\" in name:\n",
        "            layer_num = int(name.split(\".\")[3])  # Extract the layer index\n",
        "            if layer_num < freeze_decoder_layers:\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            param.requires_grad = True  # Keep other parameters trainable\n",
        "\n",
        "    # Set a unique output directory for each Layer Freezing configuration\n",
        "    output_dir = f\"./results/freeze_encoder_{freeze_encoder_layers}_decoder_{freeze_decoder_layers}\"\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_strategy=\"steps\",\n",
        "        logging_steps=100,\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=0.01,\n",
        "        fp16=True,\n",
        "        remove_unused_columns=False,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model_mtm_freeze_tune,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_data_mtm_train,\n",
        "        eval_dataset=tokenized_data_mtm_dev,\n",
        "        tokenizer=tokenizer_mtm,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(f\"Evaluation results: {eval_results}\")\n",
        "\n",
        "    # Track the best configuration\n",
        "    if best_freeze_result is None or eval_results['eval_loss'] < best_freeze_result['eval_loss']:\n",
        "        best_freeze_result = eval_results\n",
        "        best_freeze_params = {\n",
        "            'freeze_encoder_layers': freeze_encoder_layers,\n",
        "            'freeze_decoder_layers': freeze_decoder_layers,\n",
        "        }\n",
        "\n",
        "# Print the best Layer Freezing configuration and results\n",
        "print(\"Best Layer Freezing Configuration:\")\n",
        "print(best_freeze_params)\n",
        "print(\"Best Evaluation Results:\")\n",
        "print(best_freeze_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sv8OTsV4GXX"
      },
      "source": [
        "## **4. Fine-Tune mBART (50,000 Samples)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsn31bI2zpkF"
      },
      "source": [
        "### **4.1 Load Dataset (50,000)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "UUGmvMB6zwrv",
        "outputId": "9b114ea5-7cb6-4448-9c79-91278a061051"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded train data with 40000 samples.\n",
            "Loaded dev data with 5000 samples.\n",
            "Loaded test data with 5000 samples.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0df55ac979dd496b9881660dbce773d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb262d8c3d43456d9ae0fa93d4a88a7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84c4626088ae48229776fe1934f2f501",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             english_text  \\\n",
            "234806  Second, even when the stock market as a whole ...   \n",
            "41549   The upfront work is the secret to a million-do...   \n",
            "169822  Bill :Actually, my blood pressure isn't that h...   \n",
            "313751  Kylie accepted the advice and went on to recor...   \n",
            "32640   All the examiners commented that Jack had an i...   \n",
            "\n",
            "                                        chinese_text  \n",
            "234806  第二，即使股票市场是个公司整体，有时，即使好的公司在走完这10年之前也会出现不好的时候。  \n",
            "41549                           这是渐渐变成拥有百万资产的大企业的秘密。  \n",
            "169822             比尔：事实上我的血压也没有那么高，但据说血压会随年龄的增长而增高。  \n",
            "313751                          剀莉接受了这个提议，录制了这首歌的单曲。  \n",
            "32640              所有考官们评论说杰克思路清晰，给人印象深刻，独立判断能力也相当强。  \n",
            "                                             english_text  \\\n",
            "173397  They spend a lifetime fighting a losing battle...   \n",
            "296479  As regards inferior quality of your goods, we ...   \n",
            "24319   His lectures will cover some of the topics pec...   \n",
            "371615  In this paper, an incentive mechanism for impl...   \n",
            "309857  Miguel: I can't turn my back on Pedro. We're f...   \n",
            "\n",
            "                                       chinese_text  \n",
            "173397  他们花了一生的时间来做一场不会胜利的战斗，他们的对手是下垂的屁股，凸起的肚子和双下巴。  \n",
            "296479                     由於你方产品质量低劣，我方要求你方赔偿十万台币。  \n",
            "24319                           他的课会涉及计算机科学特有的一些话题。  \n",
            "371615     应用激励机制设计理论设计了有限信息下应用排污收费对排污者进行管理的一种激励机制。  \n",
            "309857                        我不能不理睬佩德罗，我们是朋友，我喜欢他。  \n",
            "                                             english_text  \\\n",
            "449881  \"O yes, certainly,\"said the girl with a sort o...   \n",
            "233204  The invention relates to a manufacturing metho...   \n",
            "13482   Activities in the gym might include the treadm...   \n",
            "320630  He was so lazy that he rusted out when he was ...   \n",
            "111437  This year's curators understood that no matter...   \n",
            "\n",
            "                                             chinese_text  \n",
            "449881              “呵，好，当然，”那女孩说，她表现的那种认真的，灵敏的样子，看来有点好笑。  \n",
            "233204                                 本发明涉及适形调强准直器的制造方法。  \n",
            "13482   健身房里的运动可能包括原地蹬脚踏车、划圆圈、爬楼梯，以及参与人数不等的团体活动如有氧运动、跆...  \n",
            "320630                                       他懒极了，因此未老先衰。  \n",
            "111437  今年的策展人很清楚，无论怎样第75届双年展定为什么主题，人们一看即知这属于惠特尼，所以他们只...  \n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the dataset\n",
        "train_data, dev_data, test_data = load_data_from_txt(data_file_path, sample_size = 50000)\n",
        "\n",
        "# Proceed with tokenization\n",
        "tokenized_data_mbart_train = preprocess_data(train_data, tokenizer_mbart)\n",
        "tokenized_data_mbart_dev = preprocess_data(dev_data, tokenizer_mbart)\n",
        "tokenized_data_mbart_test = preprocess_data(test_data, tokenizer_mbart)\n",
        "\n",
        "# Print the sampled data for verification\n",
        "print(train_data.head())\n",
        "print(dev_data.head())\n",
        "print(test_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_GMPXVmiZj6"
      },
      "source": [
        "### **4.2 Fine-Tune mBart with Regular Technique**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "4q-V9mZntFPD",
        "outputId": "7ecd7152-263d-4b22-f6a4-ba8cb89bf6e7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-e4fd4acbfa13>:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 50:41, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.922700</td>\n",
              "      <td>1.880119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=1.952279345703125, metrics={'train_runtime': 3043.1991, 'train_samples_per_second': 13.144, 'train_steps_per_second': 0.822, 'total_flos': 4232675328000000.0, 'train_loss': 1.952279345703125, 'epoch': 1.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "model_mbart_finetune = copy.deepcopy(model_mbart)\n",
        "\n",
        "# Log file\n",
        "# log_file = \"/content/drive/MyDrive/CIS5800/Log/mBART/Education/memory_usage.log\"\n",
        "# analysis_output_file = \"/content/drive/MyDrive/CIS5800/Log/mBART/Education/memory_analysis.csv\"\n",
        "\n",
        "# log_dir = os.path.dirname(log_file)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.makedirs(log_dir)\n",
        "\n",
        "# Config pad token id\n",
        "model_mbart_finetune.config.pad_token_id = tokenizer_mbart.pad_token_id\n",
        "\n",
        "# Configure TrainingArguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",       # Evaluate once per epoch\n",
        "    logging_strategy=\"steps\",          # Log training loss every few steps\n",
        "    logging_steps=100,                 # Adjust based on desired logging frequency\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"                   # Disable W&B integration\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model_mbart_finetune,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data_mbart_train,\n",
        "    eval_dataset=tokenized_data_mbart_dev,\n",
        "    tokenizer=tokenizer_mbart,\n",
        "    # callbacks=[MemoryUsageLogger(log_file)]  # Add the callback here\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Analyze logs\n",
        "# analyze_logs(log_file, analysis_output_file)\n",
        "\n",
        "# Indicate completion\n",
        "# print(\"Normal fine-tuning completed. Memory usage logged in:\", log_file)\n",
        "# print(\"Top memory usage analysis saved in:\", analysis_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZR2-pfYPaHaG",
        "outputId": "e4c17456-234f-429d-8961-53d7dadf4798"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Scores: 100%|██████████| 5000/5000 [46:24<00:00,  1.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score for the test set: 0.1192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "average_bleu = evaluate_bleu_score(model=model_mbart_finetune, tokenizer=tokenizer_mbart, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8bak1OXrKqp",
        "outputId": "e9b9b242-36c3-4057-cf33-4554d5ba085e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating BLEU: 100%|██████████| 5000/5000 [50:17<00:00,  1.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All Results:\n",
            "                                          chinese_input  \\\n",
            "0                         她接受采访时称,在失业水平下降之前,可能不会出现实质改善.   \n",
            "1     但，让我们面对它，生命中充满了各种各样的考试——有一些你第一而有一些你却不及格——因此，从某...   \n",
            "2          由于因一块著名的景点遭到破坏而羞愧不已，政府投入了数亿美元建造排污设施以及采取其他措施。   \n",
            "3            结果SRY、SOX9、WT1、SF1、AMH及DAX1等基因都参与哺乳动物性别决定。   \n",
            "4                     您没有办法来表明这个方法是否也应该或不应该对该JAR之外的类可用。   \n",
            "...                                                 ...   \n",
            "4995                     在此场合，我们可以看到变态行为是以原始的、渐进的方式完成的。   \n",
            "4996                                  我住的地方离拉萨坐车需一周的路程。   \n",
            "4997  本文最后还针对招商银行外汇业务差异化营销战略的选择方向提出合理化建议,指出招商银行重庆分行可...   \n",
            "4998                                   天生此人，注定他一辈子庸庸碌碌。   \n",
            "4999                    苏联的许多经验是好的，但是如果采取教条主义的方法去学习就坏了。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     No real improvement is possible until unemploy...   \n",
            "1     But let ’s face it, life is filled with all ki...   \n",
            "2     Shamed by damage to a renowned beauty spot, th...   \n",
            "3     Results The SRY, SOX 9, WT 1, SF 1, AMH and DA...   \n",
            "4     You have no way to indicate whether this metho...   \n",
            "...                                                 ...   \n",
            "4995  In this case we see the act of metamorphosis p...   \n",
            "4996  The place I live about one week by bus from lh...   \n",
            "4997  In the light of the direction of choice about ...   \n",
            "4998  He was a man designed by Nature for an unevent...   \n",
            "4999  A lot of the experience of the Soviet Union is...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     She said there may be no substantive improveme...       0.213     True  \n",
            "1     But, let's face it, life is full of all kinds ...       0.274     True  \n",
            "2     The government has spent billions of dollars o...       0.053    False  \n",
            "3     Results The genes SRY, SOX 9, WT 1, SF 1, AMF ...       0.388     True  \n",
            "4     You have no way of indicating whether or not t...       0.479     True  \n",
            "...                                                 ...         ...      ...  \n",
            "4995  In this situation, we can see that the deforma...       0.041    False  \n",
            "4996  It takes a week to drive from where I live to ...       0.043    False  \n",
            "4997  At the end of this paper, we also put forward ...       0.051    False  \n",
            "4998  This man, born from the womb, is doomed to do ...       0.014    False  \n",
            "4999  Much of the experience of the Soviet Union is ...       0.523     True  \n",
            "\n",
            "[5000 rows x 5 columns]\n",
            "\n",
            "Error Examples:\n",
            "                                          chinese_input  \\\n",
            "2          由于因一块著名的景点遭到破坏而羞愧不已，政府投入了数亿美元建造排污设施以及采取其他措施。   \n",
            "5                                  全美国，有成千上万的大学生参加体育运动。   \n",
            "6              我一直梦想着像一个美丽的芭蕾演员一样跳舞，轻盈地旋来转去，耳边是人们的掌声喝彩。   \n",
            "10                自始至终我们坚持一个策略：成为世界领先的为多个市场提供高性能产品的供应商。   \n",
            "11                                他虽然15岁了，但他的智力年龄却低于5岁。   \n",
            "...                                                 ...   \n",
            "4994                         一团咀嚼过的食物嘴中或消化道中的一团软的咀嚼过的食物   \n",
            "4995                     在此场合，我们可以看到变态行为是以原始的、渐进的方式完成的。   \n",
            "4996                                  我住的地方离拉萨坐车需一周的路程。   \n",
            "4997  本文最后还针对招商银行外汇业务差异化营销战略的选择方向提出合理化建议,指出招商银行重庆分行可...   \n",
            "4998                                   天生此人，注定他一辈子庸庸碌碌。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "2     Shamed by damage to a renowned beauty spot, th...   \n",
            "5     Thousands of college students around the Unite...   \n",
            "6     I always had the dream to dance like a beautif...   \n",
            "10    And few companies offer more products for the ...   \n",
            "11    Though he is fifteen, he has a mental age of l...   \n",
            "...                                                 ...   \n",
            "4994  A soft mass of chewed food within the mouth or...   \n",
            "4995  In this case we see the act of metamorphosis p...   \n",
            "4996  The place I live about one week by bus from lh...   \n",
            "4997  In the light of the direction of choice about ...   \n",
            "4998  He was a man designed by Nature for an unevent...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "2     The government has spent billions of dollars o...       0.053    False  \n",
            "5     Across the United States, there are thousands ...       0.086    False  \n",
            "6     I've always dreamt of dancing like a beautiful...       0.078    False  \n",
            "10    From the beginning to the end we adhere to one...       0.020    False  \n",
            "11    Although he is 15 years old, his intellectual ...       0.042    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4994  A slice of chewed food a slice of soft chewed ...       0.091    False  \n",
            "4995  In this situation, we can see that the deforma...       0.041    False  \n",
            "4996  It takes a week to drive from where I live to ...       0.043    False  \n",
            "4997  At the end of this paper, we also put forward ...       0.051    False  \n",
            "4998  This man, born from the womb, is doomed to do ...       0.014    False  \n",
            "\n",
            "[3250 rows x 5 columns]\n",
            "Error examples saved to '/content/drive/MyDrive/CIS5800/Error/mBart_finetune'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                                          chinese_input  \\\n",
              " 0                         她接受采访时称,在失业水平下降之前,可能不会出现实质改善.   \n",
              " 1     但，让我们面对它，生命中充满了各种各样的考试——有一些你第一而有一些你却不及格——因此，从某...   \n",
              " 2          由于因一块著名的景点遭到破坏而羞愧不已，政府投入了数亿美元建造排污设施以及采取其他措施。   \n",
              " 3            结果SRY、SOX9、WT1、SF1、AMH及DAX1等基因都参与哺乳动物性别决定。   \n",
              " 4                     您没有办法来表明这个方法是否也应该或不应该对该JAR之外的类可用。   \n",
              " ...                                                 ...   \n",
              " 4995                     在此场合，我们可以看到变态行为是以原始的、渐进的方式完成的。   \n",
              " 4996                                  我住的地方离拉萨坐车需一周的路程。   \n",
              " 4997  本文最后还针对招商银行外汇业务差异化营销战略的选择方向提出合理化建议,指出招商银行重庆分行可...   \n",
              " 4998                                   天生此人，注定他一辈子庸庸碌碌。   \n",
              " 4999                    苏联的许多经验是好的，但是如果采取教条主义的方法去学习就坏了。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     No real improvement is possible until unemploy...   \n",
              " 1     But let ’s face it, life is filled with all ki...   \n",
              " 2     Shamed by damage to a renowned beauty spot, th...   \n",
              " 3     Results The SRY, SOX 9, WT 1, SF 1, AMH and DA...   \n",
              " 4     You have no way to indicate whether this metho...   \n",
              " ...                                                 ...   \n",
              " 4995  In this case we see the act of metamorphosis p...   \n",
              " 4996  The place I live about one week by bus from lh...   \n",
              " 4997  In the light of the direction of choice about ...   \n",
              " 4998  He was a man designed by Nature for an unevent...   \n",
              " 4999  A lot of the experience of the Soviet Union is...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     She said there may be no substantive improveme...       0.213     True  \n",
              " 1     But, let's face it, life is full of all kinds ...       0.274     True  \n",
              " 2     The government has spent billions of dollars o...       0.053    False  \n",
              " 3     Results The genes SRY, SOX 9, WT 1, SF 1, AMF ...       0.388     True  \n",
              " 4     You have no way of indicating whether or not t...       0.479     True  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4995  In this situation, we can see that the deforma...       0.041    False  \n",
              " 4996  It takes a week to drive from where I live to ...       0.043    False  \n",
              " 4997  At the end of this paper, we also put forward ...       0.051    False  \n",
              " 4998  This man, born from the womb, is doomed to do ...       0.014    False  \n",
              " 4999  Much of the experience of the Soviet Union is ...       0.523     True  \n",
              " \n",
              " [5000 rows x 5 columns],\n",
              "                                           chinese_input  \\\n",
              " 2          由于因一块著名的景点遭到破坏而羞愧不已，政府投入了数亿美元建造排污设施以及采取其他措施。   \n",
              " 5                                  全美国，有成千上万的大学生参加体育运动。   \n",
              " 6              我一直梦想着像一个美丽的芭蕾演员一样跳舞，轻盈地旋来转去，耳边是人们的掌声喝彩。   \n",
              " 10                自始至终我们坚持一个策略：成为世界领先的为多个市场提供高性能产品的供应商。   \n",
              " 11                                他虽然15岁了，但他的智力年龄却低于5岁。   \n",
              " ...                                                 ...   \n",
              " 4994                         一团咀嚼过的食物嘴中或消化道中的一团软的咀嚼过的食物   \n",
              " 4995                     在此场合，我们可以看到变态行为是以原始的、渐进的方式完成的。   \n",
              " 4996                                  我住的地方离拉萨坐车需一周的路程。   \n",
              " 4997  本文最后还针对招商银行外汇业务差异化营销战略的选择方向提出合理化建议,指出招商银行重庆分行可...   \n",
              " 4998                                   天生此人，注定他一辈子庸庸碌碌。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 2     Shamed by damage to a renowned beauty spot, th...   \n",
              " 5     Thousands of college students around the Unite...   \n",
              " 6     I always had the dream to dance like a beautif...   \n",
              " 10    And few companies offer more products for the ...   \n",
              " 11    Though he is fifteen, he has a mental age of l...   \n",
              " ...                                                 ...   \n",
              " 4994  A soft mass of chewed food within the mouth or...   \n",
              " 4995  In this case we see the act of metamorphosis p...   \n",
              " 4996  The place I live about one week by bus from lh...   \n",
              " 4997  In the light of the direction of choice about ...   \n",
              " 4998  He was a man designed by Nature for an unevent...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 2     The government has spent billions of dollars o...       0.053    False  \n",
              " 5     Across the United States, there are thousands ...       0.086    False  \n",
              " 6     I've always dreamt of dancing like a beautiful...       0.078    False  \n",
              " 10    From the beginning to the end we adhere to one...       0.020    False  \n",
              " 11    Although he is 15 years old, his intellectual ...       0.042    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4994  A slice of chewed food a slice of soft chewed ...       0.091    False  \n",
              " 4995  In this situation, we can see that the deforma...       0.041    False  \n",
              " 4996  It takes a week to drive from where I live to ...       0.043    False  \n",
              " 4997  At the end of this paper, we also put forward ...       0.051    False  \n",
              " 4998  This man, born from the womb, is doomed to do ...       0.014    False  \n",
              " \n",
              " [3250 rows x 5 columns])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_evaluate(model_mbart_finetune, tokenizer_mbart, test_data, \"/content/drive/MyDrive/CIS5800/Error/mBart_finetune\", max_length=50, device=\"cuda\", bleu_threshold=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zIGniBfnGnPB"
      },
      "outputs": [],
      "source": [
        "del model_mbart_finetune\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvhxsNrG4-4o"
      },
      "source": [
        "### **4.3 Fine-tune mBart with LoRA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "rT0A-EvQ5HvA",
        "outputId": "e355bbb4-c505-496d-9a2e-042b775fa660"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-8bf0403d4c35>:44: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 27:21, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.960100</td>\n",
              "      <td>1.942648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=2.016888739013672, metrics={'train_runtime': 1643.4707, 'train_samples_per_second': 24.339, 'train_steps_per_second': 1.521, 'total_flos': 4246831104000000.0, 'train_loss': 2.016888739013672, 'epoch': 1.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "model_mbart_lora_finetune = copy.deepcopy(model_mbart)\n",
        "\n",
        "# LoRA Fine-tuning with memory monitoring\n",
        "\n",
        "# Initialize monitoring thread\n",
        "# log_file = \"/content/drive/MyDrive/CIS5800/Log/mBART/Education/memory_usage_lora.log\"\n",
        "# analysis_output_file = \"/content/drive/MyDrive/CIS5800/Log/mBART/Education/memory_analysis_lora.csv\"\n",
        "\n",
        "# log_dir = os.path.dirname(log_file)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.makedirs(log_dir)\n",
        "\n",
        "# LoRA model configuration\n",
        "mbart_lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,  # Low-rank dimension\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.0,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "model_mbart_lora_finetune = get_peft_model(model_mbart_lora_finetune, mbart_lora_config)\n",
        "model_mbart_lora_finetune.config.pad_token_id = tokenizer_mbart.pad_token_id\n",
        "\n",
        "# TrainingArguments with best params\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,  # Best learning rate\n",
        "    per_device_train_batch_size=16,  # Best batch size\n",
        "    per_device_eval_batch_size=16,  # Best batch size\n",
        "    num_train_epochs=1,  # Best epoch\n",
        "    weight_decay=0.01,\n",
        "    remove_unused_columns=False,\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model_mbart_lora_finetune,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data_mbart_train,\n",
        "    eval_dataset=tokenized_data_mbart_dev,\n",
        "    tokenizer=tokenizer_mbart,\n",
        "    # callbacks=[MemoryUsageLogger(log_file)]\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Analyze logs\n",
        "# analyze_logs(log_file, analysis_output_file)\n",
        "\n",
        "# Indicate completion\n",
        "# print(\"LoRA fine-tuning completed. Memory usage logged in:\", log_file)\n",
        "# print(\"Top memory usage analysis saved in:\", analysis_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VCT8wsySASz8",
        "outputId": "8583aa17-3d81-44fd-bbe4-4e46fd3183fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Scores: 100%|██████████| 5000/5000 [58:07<00:00,  1.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score for the test set: 0.1153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "average_bleu = evaluate_bleu_score(model=model_mbart_lora_finetune, tokenizer=tokenizer_mbart, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lx7p0Q5ZAYqr",
        "outputId": "093315a3-eaae-4eb6-d86f-441f63c9e7cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating BLEU:   0%|          | 0/5000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Evaluating BLEU: 100%|██████████| 5000/5000 [1:03:59<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All Results:\n",
            "                                          chinese_input  \\\n",
            "0                                    将生面团一切为二，再分别揉成长条形。   \n",
            "1                                    可邮寄汇票或填写信用卡付款表给我们。   \n",
            "2                                 他总是先收买一个对继承遗产最没有希望的人。   \n",
            "3     另外，这年的开局不错：冬天天气很理想，2段长时间的低温有利于植株，2，3月雨水充足，葡萄出芽...   \n",
            "4                              每当按某个新的信息改写记录，就叫作更新这一记录。   \n",
            "...                                                 ...   \n",
            "4995  独立完成办公室日常的行政管理工作，并协助全国的办公用品采购工作。配合人事行政经理起草、改相关...   \n",
            "4996              密码给予某一任意意义的符号、字母和单词的系统，用于传输需要保密或简结的信息   \n",
            "4997           目的探讨棘阿米巴属自由生活阿米巴引起的肉芽肿性脑膜脑炎的临床病理特点及鉴别诊断。   \n",
            "4998                    \"不如趁势连我们一齐撵了,我们也好,你也不愁再有好的来伏侍你\"   \n",
            "4999                        他预定今晚从八点到十点将在布郎克斯区的‘晴空月’饭店。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     Cut the dough in half and shape each half into...   \n",
            "1     By mailing bank draft in US funds OR fill in o...   \n",
            "2     It was his way to buy out the most unlikely cl...   \n",
            "3     In addition, the year had got off to a good st...   \n",
            "4     Whenever a record is rewritten with some new i...   \n",
            "...                                                 ...   \n",
            "4995  Complete daily office administration managemen...   \n",
            "4996  A system of symbols, letters, or words given c...   \n",
            "4997  Objective To study the clinicopathological cha...   \n",
            "4998  \"Why not take this chance to get rid of the lo...   \n",
            "4999  From eight to ten tonight he'll be at the Luna...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     Divide the dough into two pieces and rub them ...       0.043    False  \n",
            "1     You can mail us a bill of lading or fill in th...       0.026    False  \n",
            "2     He always buys first a person who has the leas...       0.016    False  \n",
            "3     In addition, this year's starting point is goo...       0.017    False  \n",
            "4     Every time a record is rewritten with a new in...       0.494     True  \n",
            "...                                                 ...         ...      ...  \n",
            "4995  Completes the daily administrative management ...       0.039    False  \n",
            "4996  A system of symbols, letters, and words of arb...       0.171     True  \n",
            "4997  Objective To investigate the clinical characte...       0.249     True  \n",
            "4998  \"It's better to let us go, and we'll be all ri...       0.010    False  \n",
            "4999  He's scheduled to be at the Broke Moon Hotel i...       0.065    False  \n",
            "\n",
            "[5000 rows x 5 columns]\n",
            "\n",
            "Error Examples:\n",
            "                                          chinese_input  \\\n",
            "0                                    将生面团一切为二，再分别揉成长条形。   \n",
            "1                                    可邮寄汇票或填写信用卡付款表给我们。   \n",
            "2                                 他总是先收买一个对继承遗产最没有希望的人。   \n",
            "3     另外，这年的开局不错：冬天天气很理想，2段长时间的低温有利于植株，2，3月雨水充足，葡萄出芽...   \n",
            "5          无故缺席三次（含）以上者，成绩不予通过。病假、事假等合理原因，请依学校规定办理请假手续。   \n",
            "...                                                 ...   \n",
            "4991                            整点警报整点警报功能可在每小时整点时发出鸣响。   \n",
            "4992                       英格兰西南部和威尔士之间首次轮渡的初步设计方案已经起草。   \n",
            "4995  独立完成办公室日常的行政管理工作，并协助全国的办公用品采购工作。配合人事行政经理起草、改相关...   \n",
            "4998                    \"不如趁势连我们一齐撵了,我们也好,你也不愁再有好的来伏侍你\"   \n",
            "4999                        他预定今晚从八点到十点将在布郎克斯区的‘晴空月’饭店。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     Cut the dough in half and shape each half into...   \n",
            "1     By mailing bank draft in US funds OR fill in o...   \n",
            "2     It was his way to buy out the most unlikely cl...   \n",
            "3     In addition, the year had got off to a good st...   \n",
            "5     Absence three times and above will flunk. Sick...   \n",
            "...                                                 ...   \n",
            "4991  Hourly Alarm The hourly alarm causes the watch...   \n",
            "4992  Initial plans are drawn up for the first car f...   \n",
            "4995  Complete daily office administration managemen...   \n",
            "4998  \"Why not take this chance to get rid of the lo...   \n",
            "4999  From eight to ten tonight he'll be at the Luna...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     Divide the dough into two pieces and rub them ...       0.043    False  \n",
            "1     You can mail us a bill of lading or fill in th...       0.026    False  \n",
            "2     He always buys first a person who has the leas...       0.016    False  \n",
            "3     In addition, this year's starting point is goo...       0.017    False  \n",
            "5     If you are absent for more than three (includi...       0.007    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4991  Full-point alert The full-point alert function...       0.018    False  \n",
            "4992  A preliminary design plan for the first rounda...       0.078    False  \n",
            "4995  Completes the daily administrative management ...       0.039    False  \n",
            "4998  \"It's better to let us go, and we'll be all ri...       0.010    False  \n",
            "4999  He's scheduled to be at the Broke Moon Hotel i...       0.065    False  \n",
            "\n",
            "[3262 rows x 5 columns]\n",
            "Error examples saved to '/content/drive/MyDrive/CIS5800/Error/mBart_lora'\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                                          chinese_input  \\\n",
              " 0                                    将生面团一切为二，再分别揉成长条形。   \n",
              " 1                                    可邮寄汇票或填写信用卡付款表给我们。   \n",
              " 2                                 他总是先收买一个对继承遗产最没有希望的人。   \n",
              " 3     另外，这年的开局不错：冬天天气很理想，2段长时间的低温有利于植株，2，3月雨水充足，葡萄出芽...   \n",
              " 4                              每当按某个新的信息改写记录，就叫作更新这一记录。   \n",
              " ...                                                 ...   \n",
              " 4995  独立完成办公室日常的行政管理工作，并协助全国的办公用品采购工作。配合人事行政经理起草、改相关...   \n",
              " 4996              密码给予某一任意意义的符号、字母和单词的系统，用于传输需要保密或简结的信息   \n",
              " 4997           目的探讨棘阿米巴属自由生活阿米巴引起的肉芽肿性脑膜脑炎的临床病理特点及鉴别诊断。   \n",
              " 4998                    \"不如趁势连我们一齐撵了,我们也好,你也不愁再有好的来伏侍你\"   \n",
              " 4999                        他预定今晚从八点到十点将在布郎克斯区的‘晴空月’饭店。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     Cut the dough in half and shape each half into...   \n",
              " 1     By mailing bank draft in US funds OR fill in o...   \n",
              " 2     It was his way to buy out the most unlikely cl...   \n",
              " 3     In addition, the year had got off to a good st...   \n",
              " 4     Whenever a record is rewritten with some new i...   \n",
              " ...                                                 ...   \n",
              " 4995  Complete daily office administration managemen...   \n",
              " 4996  A system of symbols, letters, or words given c...   \n",
              " 4997  Objective To study the clinicopathological cha...   \n",
              " 4998  \"Why not take this chance to get rid of the lo...   \n",
              " 4999  From eight to ten tonight he'll be at the Luna...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     Divide the dough into two pieces and rub them ...       0.043    False  \n",
              " 1     You can mail us a bill of lading or fill in th...       0.026    False  \n",
              " 2     He always buys first a person who has the leas...       0.016    False  \n",
              " 3     In addition, this year's starting point is goo...       0.017    False  \n",
              " 4     Every time a record is rewritten with a new in...       0.494     True  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4995  Completes the daily administrative management ...       0.039    False  \n",
              " 4996  A system of symbols, letters, and words of arb...       0.171     True  \n",
              " 4997  Objective To investigate the clinical characte...       0.249     True  \n",
              " 4998  \"It's better to let us go, and we'll be all ri...       0.010    False  \n",
              " 4999  He's scheduled to be at the Broke Moon Hotel i...       0.065    False  \n",
              " \n",
              " [5000 rows x 5 columns],\n",
              "                                           chinese_input  \\\n",
              " 0                                    将生面团一切为二，再分别揉成长条形。   \n",
              " 1                                    可邮寄汇票或填写信用卡付款表给我们。   \n",
              " 2                                 他总是先收买一个对继承遗产最没有希望的人。   \n",
              " 3     另外，这年的开局不错：冬天天气很理想，2段长时间的低温有利于植株，2，3月雨水充足，葡萄出芽...   \n",
              " 5          无故缺席三次（含）以上者，成绩不予通过。病假、事假等合理原因，请依学校规定办理请假手续。   \n",
              " ...                                                 ...   \n",
              " 4991                            整点警报整点警报功能可在每小时整点时发出鸣响。   \n",
              " 4992                       英格兰西南部和威尔士之间首次轮渡的初步设计方案已经起草。   \n",
              " 4995  独立完成办公室日常的行政管理工作，并协助全国的办公用品采购工作。配合人事行政经理起草、改相关...   \n",
              " 4998                    \"不如趁势连我们一齐撵了,我们也好,你也不愁再有好的来伏侍你\"   \n",
              " 4999                        他预定今晚从八点到十点将在布郎克斯区的‘晴空月’饭店。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     Cut the dough in half and shape each half into...   \n",
              " 1     By mailing bank draft in US funds OR fill in o...   \n",
              " 2     It was his way to buy out the most unlikely cl...   \n",
              " 3     In addition, the year had got off to a good st...   \n",
              " 5     Absence three times and above will flunk. Sick...   \n",
              " ...                                                 ...   \n",
              " 4991  Hourly Alarm The hourly alarm causes the watch...   \n",
              " 4992  Initial plans are drawn up for the first car f...   \n",
              " 4995  Complete daily office administration managemen...   \n",
              " 4998  \"Why not take this chance to get rid of the lo...   \n",
              " 4999  From eight to ten tonight he'll be at the Luna...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     Divide the dough into two pieces and rub them ...       0.043    False  \n",
              " 1     You can mail us a bill of lading or fill in th...       0.026    False  \n",
              " 2     He always buys first a person who has the leas...       0.016    False  \n",
              " 3     In addition, this year's starting point is goo...       0.017    False  \n",
              " 5     If you are absent for more than three (includi...       0.007    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4991  Full-point alert The full-point alert function...       0.018    False  \n",
              " 4992  A preliminary design plan for the first rounda...       0.078    False  \n",
              " 4995  Completes the daily administrative management ...       0.039    False  \n",
              " 4998  \"It's better to let us go, and we'll be all ri...       0.010    False  \n",
              " 4999  He's scheduled to be at the Broke Moon Hotel i...       0.065    False  \n",
              " \n",
              " [3262 rows x 5 columns])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_evaluate(model_mbart_lora_finetune, tokenizer_mbart, test_data, \"/content/drive/MyDrive/CIS5800/Error/mBart_lora\", max_length=50, device=\"cuda\", bleu_threshold=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2grIf3BtJqxK"
      },
      "outputs": [],
      "source": [
        "del model_mbart_lora_finetune\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I5TYlxXPVhk"
      },
      "source": [
        "### **4.4 Fine-Tune mBart with Layer Freezing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "3ECRKvjgPob-",
        "outputId": "ecd3fe80-26ff-445b-9262-4ae4ccd0088b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-19-f487a4015949>:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2500/2500 39:59, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.919100</td>\n",
              "      <td>1.931081</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2500, training_loss=1.9690359008789062, metrics={'train_runtime': 2401.6029, 'train_samples_per_second': 16.656, 'train_steps_per_second': 1.041, 'total_flos': 4232675328000000.0, 'train_loss': 1.9690359008789062, 'epoch': 1.0})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "model_mbart_freeze_finetune = copy.deepcopy(model_mbart)\n",
        "\n",
        "# Layer Freezing Fine-tuning with memory monitoring\n",
        "\n",
        "# Initialize monitoring thread\n",
        "# log_file = \"/content/drive/MyDrive/CIS5800/Log/mBART/Education/memory_usage_freeze.log\"\n",
        "# analysis_output_file = \"/content/drive/MyDrive/CIS5800/Log/mBART/Education/memory_analysis_freeze.csv\"\n",
        "\n",
        "# log_dir = os.path.dirname(log_file)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.makedirs(log_dir)\n",
        "\n",
        "freeze_encoder_layers = 8\n",
        "freeze_decoder_layers = 8\n",
        "\n",
        "# Apply Layer Freezing\n",
        "for name, param in model_mbart_freeze_finetune.named_parameters():\n",
        "    # Freeze encoder layers\n",
        "    if \"encoder.layers\" in name:\n",
        "        layer_num = int(name.split(\".\")[3])  # Extract layer index\n",
        "        if layer_num < freeze_encoder_layers:\n",
        "            param.requires_grad = False\n",
        "    # Freeze decoder layers\n",
        "    elif \"decoder.layers\" in name:\n",
        "        layer_num = int(name.split(\".\")[3])  # Extract layer index\n",
        "        if layer_num < freeze_decoder_layers:\n",
        "            param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True  # Keep other parameters trainable\n",
        "\n",
        "model_mbart_freeze_finetune.config.pad_token_id = tokenizer_mbart.pad_token_id\n",
        "\n",
        "# TrainingArguments with best params\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,  # Best learning rate\n",
        "    per_device_train_batch_size=16,  # Best batch size\n",
        "    per_device_eval_batch_size=16,  # Best batch size\n",
        "    num_train_epochs=1,  # Best epoch\n",
        "    weight_decay=0.01,\n",
        "    remove_unused_columns=False,\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model_mbart_freeze_finetune,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data_mbart_train,\n",
        "    eval_dataset=tokenized_data_mbart_dev,\n",
        "    tokenizer=tokenizer_mbart,\n",
        "    # callbacks=[MemoryUsageLogger(log_file)]\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# # Analyze logs\n",
        "# analyze_logs(log_file, analysis_output_file)\n",
        "\n",
        "# # Indicate completion\n",
        "# print(\"LoRA fine-tuning completed. Memory usage logged in:\", log_file)\n",
        "# print(\"Top memory usage analysis saved in:\", analysis_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kw25897GQmwn",
        "outputId": "a0fd2b92-8c0f-4674-dc91-2f22f4d00ce5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Scores: 100%|██████████| 5000/5000 [46:38<00:00,  1.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score for the test set: 0.1178\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "average_bleu = evaluate_bleu_score(model=model_mbart_freeze_finetune, tokenizer=tokenizer_mbart, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3X4MSdTYk22G",
        "outputId": "6c8189ce-b7a2-4e3e-dd42-0afd83a4db94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating BLEU:   0%|          | 0/5000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "Evaluating BLEU: 100%|██████████| 5000/5000 [48:10<00:00,  1.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All Results:\n",
            "                                          chinese_input  \\\n",
            "0                 “呵，好，当然，”那女孩说，她表现的那种认真的，灵敏的样子，看来有点好笑。   \n",
            "1                                    本发明涉及适形调强准直器的制造方法。   \n",
            "2     健身房里的运动可能包括原地蹬脚踏车、划圆圈、爬楼梯，以及参与人数不等的团体活动如有氧运动、跆...   \n",
            "3                                          他懒极了，因此未老先衰。   \n",
            "4     今年的策展人很清楚，无论怎样第75届双年展定为什么主题，人们一看即知这属于惠特尼，所以他们只...   \n",
            "...                                                 ...   \n",
            "4995                            目的研究玻璃体视网膜手术术后高 眼压 的原因。   \n",
            "4996                                  你一会感觉良好但是却开始失去控制。   \n",
            "4997                      本合同将以交钥匙工程为基础，由一家英国/瑞典合资企业承包。   \n",
            "4998                     提供有效的媒介计划和购买，以及长期及稳定的媒体合作关系资源。   \n",
            "4999                   拉格比球比赛19世纪初创立于沃尔威克郡的拉格比学校，因此而得名。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     \"O yes, certainly,\"said the girl with a sort o...   \n",
            "1     The invention relates to a manufacturing metho...   \n",
            "2     Activities in the gym might include the treadm...   \n",
            "3     He was so lazy that he rusted out when he was ...   \n",
            "4     This year's curators understood that no matter...   \n",
            "...                                                 ...   \n",
            "4995  Objective To evaluate the cause of intraocular...   \n",
            "4996  You're fine for a while but you start to lose ...   \n",
            "4997  The contract's going to a British / Swedish jo...   \n",
            "4998  To provide media planning and buying, and some...   \n",
            "4999  The game \" Rugby \" was invented at Rugby Schoo...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     \"Oh, yes, of course,\" said the girl, with the ...       0.012    False  \n",
            "1     The present invention relates to the manufactu...       0.076    False  \n",
            "2     Exercises in gyms may include pedaling, circli...       0.044    False  \n",
            "3     He was so lazy that he didn't give in to old age.       0.384     True  \n",
            "4     This year's curators are clear that whatever t...       0.042    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4995  Objective To investigate the causes of high op...       0.037    False  \n",
            "4996  As soon as you feel good, you start to lose co...       0.351     True  \n",
            "4997  This contract will be contracted by a UK/Swedi...       0.017    False  \n",
            "4998  Provide effective media planning and purchasin...       0.078    False  \n",
            "4999  Rugby soccer was founded in the early 19th cen...       0.084    False  \n",
            "\n",
            "[5000 rows x 5 columns]\n",
            "\n",
            "Error Examples:\n",
            "                                          chinese_input  \\\n",
            "0                 “呵，好，当然，”那女孩说，她表现的那种认真的，灵敏的样子，看来有点好笑。   \n",
            "1                                    本发明涉及适形调强准直器的制造方法。   \n",
            "2     健身房里的运动可能包括原地蹬脚踏车、划圆圈、爬楼梯，以及参与人数不等的团体活动如有氧运动、跆...   \n",
            "4     今年的策展人很清楚，无论怎样第75届双年展定为什么主题，人们一看即知这属于惠特尼，所以他们只...   \n",
            "6                        生命是由快乐时光组成的一个链条，而绝不仅仅是一种简单的存在。   \n",
            "...                                                 ...   \n",
            "4994                           她的目标：宰掉21个北方佬，21是她情郎的年龄。   \n",
            "4995                            目的研究玻璃体视网膜手术术后高 眼压 的原因。   \n",
            "4997                      本合同将以交钥匙工程为基础，由一家英国/瑞典合资企业承包。   \n",
            "4998                     提供有效的媒介计划和购买，以及长期及稳定的媒体合作关系资源。   \n",
            "4999                   拉格比球比赛19世纪初创立于沃尔威克郡的拉格比学校，因此而得名。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     \"O yes, certainly,\"said the girl with a sort o...   \n",
            "1     The invention relates to a manufacturing metho...   \n",
            "2     Activities in the gym might include the treadm...   \n",
            "4     This year's curators understood that no matter...   \n",
            "6     Life is a chian of moment of enjoyment, not on...   \n",
            "...                                                 ...   \n",
            "4994  Her goal: to slay 21 Yankees, one for each yea...   \n",
            "4995  Objective To evaluate the cause of intraocular...   \n",
            "4997  The contract's going to a British / Swedish jo...   \n",
            "4998  To provide media planning and buying, and some...   \n",
            "4999  The game \" Rugby \" was invented at Rugby Schoo...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     \"Oh, yes, of course,\" said the girl, with the ...       0.012    False  \n",
            "1     The present invention relates to the manufactu...       0.076    False  \n",
            "2     Exercises in gyms may include pedaling, circli...       0.044    False  \n",
            "4     This year's curators are clear that whatever t...       0.042    False  \n",
            "6     Life is a chain of happy moments, and not just...       0.087    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4994  Her goal is to kill 21 Yankees, 21 is her love...       0.035    False  \n",
            "4995  Objective To investigate the causes of high op...       0.037    False  \n",
            "4997  This contract will be contracted by a UK/Swedi...       0.017    False  \n",
            "4998  Provide effective media planning and purchasin...       0.078    False  \n",
            "4999  Rugby soccer was founded in the early 19th cen...       0.084    False  \n",
            "\n",
            "[3204 rows x 5 columns]\n",
            "Error examples saved to '/content/drive/MyDrive/CIS5800/Error/mBart_freeze'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                                          chinese_input  \\\n",
              " 0                 “呵，好，当然，”那女孩说，她表现的那种认真的，灵敏的样子，看来有点好笑。   \n",
              " 1                                    本发明涉及适形调强准直器的制造方法。   \n",
              " 2     健身房里的运动可能包括原地蹬脚踏车、划圆圈、爬楼梯，以及参与人数不等的团体活动如有氧运动、跆...   \n",
              " 3                                          他懒极了，因此未老先衰。   \n",
              " 4     今年的策展人很清楚，无论怎样第75届双年展定为什么主题，人们一看即知这属于惠特尼，所以他们只...   \n",
              " ...                                                 ...   \n",
              " 4995                            目的研究玻璃体视网膜手术术后高 眼压 的原因。   \n",
              " 4996                                  你一会感觉良好但是却开始失去控制。   \n",
              " 4997                      本合同将以交钥匙工程为基础，由一家英国/瑞典合资企业承包。   \n",
              " 4998                     提供有效的媒介计划和购买，以及长期及稳定的媒体合作关系资源。   \n",
              " 4999                   拉格比球比赛19世纪初创立于沃尔威克郡的拉格比学校，因此而得名。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     \"O yes, certainly,\"said the girl with a sort o...   \n",
              " 1     The invention relates to a manufacturing metho...   \n",
              " 2     Activities in the gym might include the treadm...   \n",
              " 3     He was so lazy that he rusted out when he was ...   \n",
              " 4     This year's curators understood that no matter...   \n",
              " ...                                                 ...   \n",
              " 4995  Objective To evaluate the cause of intraocular...   \n",
              " 4996  You're fine for a while but you start to lose ...   \n",
              " 4997  The contract's going to a British / Swedish jo...   \n",
              " 4998  To provide media planning and buying, and some...   \n",
              " 4999  The game \" Rugby \" was invented at Rugby Schoo...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     \"Oh, yes, of course,\" said the girl, with the ...       0.012    False  \n",
              " 1     The present invention relates to the manufactu...       0.076    False  \n",
              " 2     Exercises in gyms may include pedaling, circli...       0.044    False  \n",
              " 3     He was so lazy that he didn't give in to old age.       0.384     True  \n",
              " 4     This year's curators are clear that whatever t...       0.042    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4995  Objective To investigate the causes of high op...       0.037    False  \n",
              " 4996  As soon as you feel good, you start to lose co...       0.351     True  \n",
              " 4997  This contract will be contracted by a UK/Swedi...       0.017    False  \n",
              " 4998  Provide effective media planning and purchasin...       0.078    False  \n",
              " 4999  Rugby soccer was founded in the early 19th cen...       0.084    False  \n",
              " \n",
              " [5000 rows x 5 columns],\n",
              "                                           chinese_input  \\\n",
              " 0                 “呵，好，当然，”那女孩说，她表现的那种认真的，灵敏的样子，看来有点好笑。   \n",
              " 1                                    本发明涉及适形调强准直器的制造方法。   \n",
              " 2     健身房里的运动可能包括原地蹬脚踏车、划圆圈、爬楼梯，以及参与人数不等的团体活动如有氧运动、跆...   \n",
              " 4     今年的策展人很清楚，无论怎样第75届双年展定为什么主题，人们一看即知这属于惠特尼，所以他们只...   \n",
              " 6                        生命是由快乐时光组成的一个链条，而绝不仅仅是一种简单的存在。   \n",
              " ...                                                 ...   \n",
              " 4994                           她的目标：宰掉21个北方佬，21是她情郎的年龄。   \n",
              " 4995                            目的研究玻璃体视网膜手术术后高 眼压 的原因。   \n",
              " 4997                      本合同将以交钥匙工程为基础，由一家英国/瑞典合资企业承包。   \n",
              " 4998                     提供有效的媒介计划和购买，以及长期及稳定的媒体合作关系资源。   \n",
              " 4999                   拉格比球比赛19世纪初创立于沃尔威克郡的拉格比学校，因此而得名。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     \"O yes, certainly,\"said the girl with a sort o...   \n",
              " 1     The invention relates to a manufacturing metho...   \n",
              " 2     Activities in the gym might include the treadm...   \n",
              " 4     This year's curators understood that no matter...   \n",
              " 6     Life is a chian of moment of enjoyment, not on...   \n",
              " ...                                                 ...   \n",
              " 4994  Her goal: to slay 21 Yankees, one for each yea...   \n",
              " 4995  Objective To evaluate the cause of intraocular...   \n",
              " 4997  The contract's going to a British / Swedish jo...   \n",
              " 4998  To provide media planning and buying, and some...   \n",
              " 4999  The game \" Rugby \" was invented at Rugby Schoo...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     \"Oh, yes, of course,\" said the girl, with the ...       0.012    False  \n",
              " 1     The present invention relates to the manufactu...       0.076    False  \n",
              " 2     Exercises in gyms may include pedaling, circli...       0.044    False  \n",
              " 4     This year's curators are clear that whatever t...       0.042    False  \n",
              " 6     Life is a chain of happy moments, and not just...       0.087    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4994  Her goal is to kill 21 Yankees, 21 is her love...       0.035    False  \n",
              " 4995  Objective To investigate the causes of high op...       0.037    False  \n",
              " 4997  This contract will be contracted by a UK/Swedi...       0.017    False  \n",
              " 4998  Provide effective media planning and purchasin...       0.078    False  \n",
              " 4999  Rugby soccer was founded in the early 19th cen...       0.084    False  \n",
              " \n",
              " [3204 rows x 5 columns])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_evaluate(model_mbart_freeze_finetune, tokenizer_mbart, test_data, \"/content/drive/MyDrive/CIS5800/Error/mBart_freeze\", max_length=50, device=\"cuda\", bleu_threshold=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LjMXvn3ZKJpt"
      },
      "outputs": [],
      "source": [
        "del model_mbart_freeze_finetune\n",
        "del tokenized_data_mbart_train\n",
        "del tokenized_data_mbart_dev\n",
        "del tokenized_data_mbart_test\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0heTFBaBoyX"
      },
      "source": [
        "## **5. Fine-Tune M2M100 (50,000 Samples)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Wuhnfza0boW"
      },
      "source": [
        "### **5.1 Load Dataset (50,000)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "id": "BKzde-Q10eAL",
        "outputId": "56546b9d-8e18-4e9e-9314-12b9cbb4eba8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded train data with 40000 samples.\n",
            "Loaded dev data with 5000 samples.\n",
            "Loaded test data with 5000 samples.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c37cd6fecb1f4a3091f86d7bcb2617f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/40000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef9af1cf8b48457bb2d19d40eb447c32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "556df846c929405d829d466d359beda1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             english_text  \\\n",
            "234806  They tried to sell me a bill of goods about a ...   \n",
            "41549   But then, Willowville folks don't know to this...   \n",
            "169822  Her sociable manner is really a mask for a ver...   \n",
            "313751  They have been exhibited in London's V A museu...   \n",
            "32640   Your phone will be unable to accurately determ...   \n",
            "\n",
            "                                             chinese_text  \n",
            "234806                                      他们试图骗我买一辆二手车。  \n",
            "41549   然而,威罗维利镇的人们至今也想不到的是,埃米莉·霍顿其实是马克·梅切尔的女儿,而马克却从来没...  \n",
            "169822                             她那好交际的作风，实际上是她腼腆天性的伪装。  \n",
            "313751  这些设计品在伦敦维多利亚·阿尔伯特博物馆展出过，也入选过《世界时尚》杂志(Voguemaga...  \n",
            "32640                                “:”你的手机不能准确定位目前你的位置。  \n",
            "                                             english_text  \\\n",
            "173397  Katsushika City Museum is the museum of local ...   \n",
            "296479  But, maybe we need to throw out the idea of vi...   \n",
            "24319   Now that the baby can hear, some moms like to ...   \n",
            "371615  A survey composed of multiple choice and trans...   \n",
            "309857  If you have exactly the same one - same color,...   \n",
            "\n",
            "                                   chinese_text  \n",
            "173397                  葛饰区乡土天文馆是一所地方历史博物馆和天文馆。  \n",
            "296479  但是，也许我们需要完全丢掉处女这个概念，不要有从一个行为中你会失去什么的想法。  \n",
            "24319                既然婴儿能听到了，一些妈妈喜欢读书或放音乐给宝贝听。  \n",
            "371615     调查采用能力测试的形式，结合定性和定量分析探求词块能力和语言能力的关系。  \n",
            "309857       如果你们有完全一样的 —— 同样颜色，同样款式，同样尺寸，当然愿意。  \n",
            "                                             english_text  \\\n",
            "449881  The upper end of a stay 10 is mounted on a cei...   \n",
            "233204  An Environmental Law and Policy Center represe...   \n",
            "13482   What about you, Mike? Do you think we should t...   \n",
            "320630  But since the blockade began, he has had to re...   \n",
            "111437  Some pulled a long face as they glanced at the...   \n",
            "\n",
            "                                     chinese_text  \n",
            "449881                      支架10的上端安装在车内空间中的天花板上。  \n",
            "233204               一个环境法律和政策中心的代表说，风能类似的基准已经工作。  \n",
            "13482                你呢，迈克? 你认为我们应该把CP21告诉凯先生 吗 ?  \n",
            "320630  以前他直接从中国进口货物，但是封锁开始后，他只得求助于走私犯并且得多付40%的钱。  \n",
            "111437                           有几位可就拉长了脸瞧着会场门口。  \n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the dataset\n",
        "train_data, dev_data, test_data = load_data_from_txt(data_file_path, sample_size = 50000)\n",
        "\n",
        "tokenized_data_mtm_train = preprocess_data(train_data, tokenizer_mtm, max_length=20, src_lang=\"zh\", tgt_lang=\"en\")\n",
        "tokenized_data_mtm_dev = preprocess_data(dev_data, tokenizer_mtm, max_length=20, src_lang=\"zh\", tgt_lang=\"en\")\n",
        "tokenized_data_mtm_test = preprocess_data(test_data, tokenizer_mtm, max_length=20, src_lang=\"zh\", tgt_lang=\"en\")\n",
        "\n",
        "print(train_data.head())\n",
        "print(dev_data.head())\n",
        "print(test_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TKJqLOrebzH"
      },
      "source": [
        "### **5.2 Fine-Tuned M2M100 with Regular Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "i3B5LItQYL8z",
        "outputId": "966fe584-e61f-4825-b91f-2d4482d51970"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-17-0fb7a7a7977a>:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='936' max='936' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [936/936 45:46, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.742000</td>\n",
              "      <td>2.572316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.458700</td>\n",
              "      <td>2.527643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=936, training_loss=2.619048306065747, metrics={'train_runtime': 2750.5289, 'train_samples_per_second': 43.628, 'train_steps_per_second': 0.34, 'total_flos': 5071024768942080.0, 'train_loss': 2.619048306065747, 'epoch': 2.9952})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "model_mtm_finetune = copy.deepcopy(model_mtm)\n",
        "\n",
        "# Fine-tune the model on a small subset for testing\n",
        "# Initialize monitoring thread\n",
        "# log_file = \"/content/drive/MyDrive/CIS5800/Log/m2m/Education/memory_usage.log\"\n",
        "# analysis_output_file = \"/content/drive/MyDrive/CIS5800/Log/m2m/Education/memory_analysis.csv\"\n",
        "\n",
        "# log_dir = os.path.dirname(log_file)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.makedirs(log_dir)\n",
        "\n",
        "# Config pad token id\n",
        "model_mtm_finetune.config.pad_token_id = tokenizer_mtm.pad_token_id\n",
        "\n",
        "# Configure TrainingArguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    gradient_accumulation_steps=8,\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model_mtm_finetune,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data_mtm_train,\n",
        "    eval_dataset=tokenized_data_mtm_dev,\n",
        "    tokenizer=tokenizer_mtm,\n",
        "    # callbacks=[MemoryUsageLogger(log_file)]\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Analyze logs\n",
        "# analyze_logs(log_file, analysis_output_file)\n",
        "\n",
        "# Indicate completion\n",
        "# print(\"Normal fine-tuning completed. Memory usage logged in:\", log_file)\n",
        "# print(\"Top memory usage analysis saved in:\", analysis_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CGnzJYoXgoNg",
        "outputId": "d83c5a03-e3be-4e24-9a7e-b9c5dd5f4c31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Scores: 100%|██████████| 5000/5000 [26:54<00:00,  3.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score for the test set: 0.0809\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "average_bleu = evaluate_bleu_score(model=model_mtm_finetune, tokenizer=tokenizer_mtm, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ozqp5ofWM3c-",
        "outputId": "e1ded2a3-1024-40ae-9ee8-dd2c547990dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating BLEU: 100%|██████████| 5000/5000 [27:09<00:00,  3.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All Results:\n",
            "                                          chinese_input  \\\n",
            "0                                 支架10的上端安装在车内空间中的天花板上。   \n",
            "1                          一个环境法律和政策中心的代表说，风能类似的基准已经工作。   \n",
            "2                          你呢，迈克? 你认为我们应该把CP21告诉凯先生 吗 ?   \n",
            "3             以前他直接从中国进口货物，但是封锁开始后，他只得求助于走私犯并且得多付40%的钱。   \n",
            "4                                      有几位可就拉长了脸瞧着会场门口。   \n",
            "...                                                 ...   \n",
            "4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
            "4996                            例如，您可能安装了服务包，但没有安装处理器包。   \n",
            "4997  实际上，没有一个善于思考的人会拒绝接受他们的第一个观点：即情感生活的巨大变化要求表达方式也随...   \n",
            "4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
            "4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     The upper end of a stay 10 is mounted on a cei...   \n",
            "1     An Environmental Law and Policy Center represe...   \n",
            "2     What about you, Mike? Do you think we should t...   \n",
            "3     But since the blockade began, he has had to re...   \n",
            "4     Some pulled a long face as they glanced at the...   \n",
            "...                                                 ...   \n",
            "4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
            "4996  For example, you may have installed a service ...   \n",
            "4997  All the same, no thinking man can refuse to ac...   \n",
            "4998  Contrary to some stereotypes, Americans don't ...   \n",
            "4999  \"Do you know ,\" went on the father, \"that it's...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     The upper end of the shelf 10 is installed on ...       0.197     True  \n",
            "1     A representative of the Center for Environment...       0.177     True  \n",
            "2     What about you, Mike? Do you think we should t...       0.727     True  \n",
            "3     He had imported goods directly from China, but...       0.102     True  \n",
            "4     A few of them pulled their face and looked at ...       0.033    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4995  Change fuel cost to fuel cost by adding a spac...       0.023    False  \n",
            "4996  For example, you may have installed a Service ...       0.437     True  \n",
            "4997  In fact, no one who is good in thinking will r...       0.138     True  \n",
            "4998  Americans, unlike the typical people, are not ...       0.014    False  \n",
            "4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
            "\n",
            "[5000 rows x 5 columns]\n",
            "\n",
            "Error Examples:\n",
            "                                          chinese_input  \\\n",
            "4                                      有几位可就拉长了脸瞧着会场门口。   \n",
            "5     中国抗战八年，她还算侥幸，自始至终，鬼子都不曾去惹她，假如也像马来亚沦陷了若干年月，我不知她...   \n",
            "7                              由于水位不断上升，随都能看到搜救被困居民的场景。   \n",
            "8                               梯形导向浮阀塔板具有良好的流体力学和传质性能。   \n",
            "10                            这些新观点、新命题，对党史研究有着重大的指导作用。   \n",
            "...                                                 ...   \n",
            "4993                 保险公司也是获益者，他们可以通过更好的途径获得最新资料用于保险精算。   \n",
            "4994                              至少有14人死于这场暴雨，上千人流离失所。   \n",
            "4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
            "4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
            "4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "4     Some pulled a long face as they glanced at the...   \n",
            "5     In China's eight years of fighting a war, she ...   \n",
            "7     Scenes like this one have been playing out all...   \n",
            "8     The trapezoidal directed valve trays have exce...   \n",
            "10    These new point of views, new proposition, to ...   \n",
            "...                                                 ...   \n",
            "4993  Insurance companies benefit fromthe informatio...   \n",
            "4994  At least 1deaths are being blamed on the massi...   \n",
            "4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
            "4998  Contrary to some stereotypes, Americans don't ...   \n",
            "4999  \"Do you know ,\" went on the father, \"that it's...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "4     A few of them pulled their face and looked at ...       0.033    False  \n",
            "5     After eight years of China's resistance, she w...       0.002    False  \n",
            "7     As the water level continues to rise, the scen...       0.009    False  \n",
            "8     The staircase-oriented floating valve panel ha...       0.021    False  \n",
            "10    These new points of view and new arguments hav...       0.031    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4993  Insurance firms are also profitable, and they ...       0.013    False  \n",
            "4994  At least 14 people were killed in the storm an...       0.031    False  \n",
            "4995  Change fuel cost to fuel cost by adding a spac...       0.023    False  \n",
            "4998  Americans, unlike the typical people, are not ...       0.014    False  \n",
            "4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
            "\n",
            "[3884 rows x 5 columns]\n",
            "Error examples saved to '/content/drive/MyDrive/CIS5800/Error/m2m_finetune'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                                          chinese_input  \\\n",
              " 0                                 支架10的上端安装在车内空间中的天花板上。   \n",
              " 1                          一个环境法律和政策中心的代表说，风能类似的基准已经工作。   \n",
              " 2                          你呢，迈克? 你认为我们应该把CP21告诉凯先生 吗 ?   \n",
              " 3             以前他直接从中国进口货物，但是封锁开始后，他只得求助于走私犯并且得多付40%的钱。   \n",
              " 4                                      有几位可就拉长了脸瞧着会场门口。   \n",
              " ...                                                 ...   \n",
              " 4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
              " 4996                            例如，您可能安装了服务包，但没有安装处理器包。   \n",
              " 4997  实际上，没有一个善于思考的人会拒绝接受他们的第一个观点：即情感生活的巨大变化要求表达方式也随...   \n",
              " 4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
              " 4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     The upper end of a stay 10 is mounted on a cei...   \n",
              " 1     An Environmental Law and Policy Center represe...   \n",
              " 2     What about you, Mike? Do you think we should t...   \n",
              " 3     But since the blockade began, he has had to re...   \n",
              " 4     Some pulled a long face as they glanced at the...   \n",
              " ...                                                 ...   \n",
              " 4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
              " 4996  For example, you may have installed a service ...   \n",
              " 4997  All the same, no thinking man can refuse to ac...   \n",
              " 4998  Contrary to some stereotypes, Americans don't ...   \n",
              " 4999  \"Do you know ,\" went on the father, \"that it's...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     The upper end of the shelf 10 is installed on ...       0.197     True  \n",
              " 1     A representative of the Center for Environment...       0.177     True  \n",
              " 2     What about you, Mike? Do you think we should t...       0.727     True  \n",
              " 3     He had imported goods directly from China, but...       0.102     True  \n",
              " 4     A few of them pulled their face and looked at ...       0.033    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4995  Change fuel cost to fuel cost by adding a spac...       0.023    False  \n",
              " 4996  For example, you may have installed a Service ...       0.437     True  \n",
              " 4997  In fact, no one who is good in thinking will r...       0.138     True  \n",
              " 4998  Americans, unlike the typical people, are not ...       0.014    False  \n",
              " 4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
              " \n",
              " [5000 rows x 5 columns],\n",
              "                                           chinese_input  \\\n",
              " 4                                      有几位可就拉长了脸瞧着会场门口。   \n",
              " 5     中国抗战八年，她还算侥幸，自始至终，鬼子都不曾去惹她，假如也像马来亚沦陷了若干年月，我不知她...   \n",
              " 7                              由于水位不断上升，随都能看到搜救被困居民的场景。   \n",
              " 8                               梯形导向浮阀塔板具有良好的流体力学和传质性能。   \n",
              " 10                            这些新观点、新命题，对党史研究有着重大的指导作用。   \n",
              " ...                                                 ...   \n",
              " 4993                 保险公司也是获益者，他们可以通过更好的途径获得最新资料用于保险精算。   \n",
              " 4994                              至少有14人死于这场暴雨，上千人流离失所。   \n",
              " 4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
              " 4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
              " 4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 4     Some pulled a long face as they glanced at the...   \n",
              " 5     In China's eight years of fighting a war, she ...   \n",
              " 7     Scenes like this one have been playing out all...   \n",
              " 8     The trapezoidal directed valve trays have exce...   \n",
              " 10    These new point of views, new proposition, to ...   \n",
              " ...                                                 ...   \n",
              " 4993  Insurance companies benefit fromthe informatio...   \n",
              " 4994  At least 1deaths are being blamed on the massi...   \n",
              " 4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
              " 4998  Contrary to some stereotypes, Americans don't ...   \n",
              " 4999  \"Do you know ,\" went on the father, \"that it's...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 4     A few of them pulled their face and looked at ...       0.033    False  \n",
              " 5     After eight years of China's resistance, she w...       0.002    False  \n",
              " 7     As the water level continues to rise, the scen...       0.009    False  \n",
              " 8     The staircase-oriented floating valve panel ha...       0.021    False  \n",
              " 10    These new points of view and new arguments hav...       0.031    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4993  Insurance firms are also profitable, and they ...       0.013    False  \n",
              " 4994  At least 14 people were killed in the storm an...       0.031    False  \n",
              " 4995  Change fuel cost to fuel cost by adding a spac...       0.023    False  \n",
              " 4998  Americans, unlike the typical people, are not ...       0.014    False  \n",
              " 4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
              " \n",
              " [3884 rows x 5 columns])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_evaluate(model_mtm_finetune, tokenizer_mtm, test_data, \"/content/drive/MyDrive/CIS5800/Error/m2m_finetune\", max_length=50, device=\"cuda\", bleu_threshold=0.1, model_type = \"m2m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wi3U5v1kMaUx"
      },
      "outputs": [],
      "source": [
        "del model_mtm_finetune\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POk2iB0V5ULw"
      },
      "source": [
        "### **5.3 Fine-Tine M2M100 with LoRA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1SWoXpHB5kn0",
        "outputId": "1f043f13-2125-4c18-980b-68cabf4fb437"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-20-7c973f6ea38a>:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7500/7500 29:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.946400</td>\n",
              "      <td>2.757318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.881300</td>\n",
              "      <td>2.688100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.833100</td>\n",
              "      <td>2.674909</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7500, training_loss=2.9112531982421874, metrics={'train_runtime': 1742.3146, 'train_samples_per_second': 68.874, 'train_steps_per_second': 4.305, 'total_flos': 5096138342400000.0, 'train_loss': 2.9112531982421874, 'epoch': 3.0})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "model_mtm_lora_finetune = copy.deepcopy(model_mtm)\n",
        "\n",
        "# LoRA Fine-tuning with memory monitoring\n",
        "\n",
        "# # Initialize monitoring thread\n",
        "# log_file = \"/content/drive/MyDrive/CIS5800/Log/m2m/Education/memory_usage_lora.log\"\n",
        "# analysis_output_file = \"/content/drive/MyDrive/CIS5800/Log/m2m/Education/memory_analysis_lora.csv\"\n",
        "\n",
        "# log_dir = os.path.dirname(log_file)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.makedirs(log_dir)\n",
        "\n",
        "mtm_lora_config = LoraConfig( # constructor\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.0, # change\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        ")\n",
        "\n",
        "model_mtm_lora_finetune = get_peft_model(model_mtm_lora_finetune, mtm_lora_config)\n",
        "model_mtm_lora_finetune.config.pad_token_id = tokenizer_mtm.pad_token_id # tokenizer as the config for tokenizer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    remove_unused_columns=False,\n",
        "    logging_steps=100,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_mtm_lora_finetune,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data_mtm_train,\n",
        "    eval_dataset=tokenized_data_mtm_dev,\n",
        "    tokenizer=tokenizer_mtm,\n",
        "    # callbacks=[MemoryUsageLogger(log_file)]\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Analyze logs\n",
        "# analyze_logs(log_file, analysis_output_file)\n",
        "\n",
        "# # Indicate completion\n",
        "# print(\"LoRA fine-tuning completed. Memory usage logged in:\", log_file)\n",
        "# print(\"Top memory usage analysis saved in:\", analysis_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "61wkfD8JBk8S",
        "outputId": "8b672e94-3cb9-4e5e-e67f-9f84771a72dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Scores: 100%|██████████| 5000/5000 [35:59<00:00,  2.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score for the test set: 0.0782\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "average_bleu = evaluate_bleu_score(model=model_mtm_lora_finetune, tokenizer=tokenizer_mtm, test_data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ceYN6PkNNwe0",
        "outputId": "4008edb3-fd60-41a1-f8b7-7beb5b3289d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating BLEU: 100%|██████████| 5000/5000 [36:23<00:00,  2.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "All Results:\n",
            "                                          chinese_input  \\\n",
            "0                                 支架10的上端安装在车内空间中的天花板上。   \n",
            "1                          一个环境法律和政策中心的代表说，风能类似的基准已经工作。   \n",
            "2                          你呢，迈克? 你认为我们应该把CP21告诉凯先生 吗 ?   \n",
            "3             以前他直接从中国进口货物，但是封锁开始后，他只得求助于走私犯并且得多付40%的钱。   \n",
            "4                                      有几位可就拉长了脸瞧着会场门口。   \n",
            "...                                                 ...   \n",
            "4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
            "4996                            例如，您可能安装了服务包，但没有安装处理器包。   \n",
            "4997  实际上，没有一个善于思考的人会拒绝接受他们的第一个观点：即情感生活的巨大变化要求表达方式也随...   \n",
            "4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
            "4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     The upper end of a stay 10 is mounted on a cei...   \n",
            "1     An Environmental Law and Policy Center represe...   \n",
            "2     What about you, Mike? Do you think we should t...   \n",
            "3     But since the blockade began, he has had to re...   \n",
            "4     Some pulled a long face as they glanced at the...   \n",
            "...                                                 ...   \n",
            "4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
            "4996  For example, you may have installed a service ...   \n",
            "4997  All the same, no thinking man can refuse to ac...   \n",
            "4998  Contrary to some stereotypes, Americans don't ...   \n",
            "4999  \"Do you know ,\" went on the father, \"that it's...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     The upper end of the shell 10 is installed on ...       0.197     True  \n",
            "1     A representative of the Environmental Law and ...       0.240     True  \n",
            "2     What about you, Mike? Do you think we should t...       0.655     True  \n",
            "3     He had previously imported goods directly from...       0.041    False  \n",
            "4       Some of them looked at the door of the meeting.       0.045    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4995  Change fuel costs into fuel costs by adding a ...       0.022    False  \n",
            "4996  For example, you may have a service pack insta...       0.376     True  \n",
            "4997  In fact, no one who is good at thinking will r...       0.134     True  \n",
            "4998  The Americans are different from the typical p...       0.020    False  \n",
            "4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
            "\n",
            "[5000 rows x 5 columns]\n",
            "\n",
            "Error Examples:\n",
            "                                          chinese_input  \\\n",
            "3             以前他直接从中国进口货物，但是封锁开始后，他只得求助于走私犯并且得多付40%的钱。   \n",
            "4                                      有几位可就拉长了脸瞧着会场门口。   \n",
            "5     中国抗战八年，她还算侥幸，自始至终，鬼子都不曾去惹她，假如也像马来亚沦陷了若干年月，我不知她...   \n",
            "6                                       他跌倒时, 头碰到箱子的一角。   \n",
            "7                              由于水位不断上升，随都能看到搜救被困居民的场景。   \n",
            "...                                                 ...   \n",
            "4993                 保险公司也是获益者，他们可以通过更好的途径获得最新资料用于保险精算。   \n",
            "4994                              至少有14人死于这场暴雨，上千人流离失所。   \n",
            "4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
            "4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
            "4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "3     But since the blockade began, he has had to re...   \n",
            "4     Some pulled a long face as they glanced at the...   \n",
            "5     In China's eight years of fighting a war, she ...   \n",
            "6      He fell and hit his head on the corner of a box.   \n",
            "7     Scenes like this one have been playing out all...   \n",
            "...                                                 ...   \n",
            "4993  Insurance companies benefit fromthe informatio...   \n",
            "4994  At least 1deaths are being blamed on the massi...   \n",
            "4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
            "4998  Contrary to some stereotypes, Americans don't ...   \n",
            "4999  \"Do you know ,\" went on the father, \"that it's...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "3     He had previously imported goods directly from...       0.041    False  \n",
            "4       Some of them looked at the door of the meeting.       0.045    False  \n",
            "5     For eight years of China's resistance, she was...       0.004    False  \n",
            "6       When he fell, his head hit a corner of the box.       0.061    False  \n",
            "7     As the water level continues to rise, the scen...       0.010    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4993  Insurance companies are also profitable, and t...       0.025    False  \n",
            "4994  At least 14 people were killed in the storm, a...       0.031    False  \n",
            "4995  Change fuel costs into fuel costs by adding a ...       0.022    False  \n",
            "4998  The Americans are different from the typical p...       0.020    False  \n",
            "4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
            "\n",
            "[3960 rows x 5 columns]\n",
            "Error examples saved to '/content/drive/MyDrive/CIS5800/Error/m2m_lora'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(                                          chinese_input  \\\n",
              " 0                                 支架10的上端安装在车内空间中的天花板上。   \n",
              " 1                          一个环境法律和政策中心的代表说，风能类似的基准已经工作。   \n",
              " 2                          你呢，迈克? 你认为我们应该把CP21告诉凯先生 吗 ?   \n",
              " 3             以前他直接从中国进口货物，但是封锁开始后，他只得求助于走私犯并且得多付40%的钱。   \n",
              " 4                                      有几位可就拉长了脸瞧着会场门口。   \n",
              " ...                                                 ...   \n",
              " 4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
              " 4996                            例如，您可能安装了服务包，但没有安装处理器包。   \n",
              " 4997  实际上，没有一个善于思考的人会拒绝接受他们的第一个观点：即情感生活的巨大变化要求表达方式也随...   \n",
              " 4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
              " 4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     The upper end of a stay 10 is mounted on a cei...   \n",
              " 1     An Environmental Law and Policy Center represe...   \n",
              " 2     What about you, Mike? Do you think we should t...   \n",
              " 3     But since the blockade began, he has had to re...   \n",
              " 4     Some pulled a long face as they glanced at the...   \n",
              " ...                                                 ...   \n",
              " 4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
              " 4996  For example, you may have installed a service ...   \n",
              " 4997  All the same, no thinking man can refuse to ac...   \n",
              " 4998  Contrary to some stereotypes, Americans don't ...   \n",
              " 4999  \"Do you know ,\" went on the father, \"that it's...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     The upper end of the shell 10 is installed on ...       0.197     True  \n",
              " 1     A representative of the Environmental Law and ...       0.240     True  \n",
              " 2     What about you, Mike? Do you think we should t...       0.655     True  \n",
              " 3     He had previously imported goods directly from...       0.041    False  \n",
              " 4       Some of them looked at the door of the meeting.       0.045    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4995  Change fuel costs into fuel costs by adding a ...       0.022    False  \n",
              " 4996  For example, you may have a service pack insta...       0.376     True  \n",
              " 4997  In fact, no one who is good at thinking will r...       0.134     True  \n",
              " 4998  The Americans are different from the typical p...       0.020    False  \n",
              " 4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
              " \n",
              " [5000 rows x 5 columns],\n",
              "                                           chinese_input  \\\n",
              " 3             以前他直接从中国进口货物，但是封锁开始后，他只得求助于走私犯并且得多付40%的钱。   \n",
              " 4                                      有几位可就拉长了脸瞧着会场门口。   \n",
              " 5     中国抗战八年，她还算侥幸，自始至终，鬼子都不曾去惹她，假如也像马来亚沦陷了若干年月，我不知她...   \n",
              " 6                                       他跌倒时, 头碰到箱子的一角。   \n",
              " 7                              由于水位不断上升，随都能看到搜救被困居民的场景。   \n",
              " ...                                                 ...   \n",
              " 4993                 保险公司也是获益者，他们可以通过更好的途径获得最新资料用于保险精算。   \n",
              " 4994                              至少有14人死于这场暴雨，上千人流离失所。   \n",
              " 4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
              " 4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
              " 4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 3     But since the blockade began, he has had to re...   \n",
              " 4     Some pulled a long face as they glanced at the...   \n",
              " 5     In China's eight years of fighting a war, she ...   \n",
              " 6      He fell and hit his head on the corner of a box.   \n",
              " 7     Scenes like this one have been playing out all...   \n",
              " ...                                                 ...   \n",
              " 4993  Insurance companies benefit fromthe informatio...   \n",
              " 4994  At least 1deaths are being blamed on the massi...   \n",
              " 4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
              " 4998  Contrary to some stereotypes, Americans don't ...   \n",
              " 4999  \"Do you know ,\" went on the father, \"that it's...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 3     He had previously imported goods directly from...       0.041    False  \n",
              " 4       Some of them looked at the door of the meeting.       0.045    False  \n",
              " 5     For eight years of China's resistance, she was...       0.004    False  \n",
              " 6       When he fell, his head hit a corner of the box.       0.061    False  \n",
              " 7     As the water level continues to rise, the scen...       0.010    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4993  Insurance companies are also profitable, and t...       0.025    False  \n",
              " 4994  At least 14 people were killed in the storm, a...       0.031    False  \n",
              " 4995  Change fuel costs into fuel costs by adding a ...       0.022    False  \n",
              " 4998  The Americans are different from the typical p...       0.020    False  \n",
              " 4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
              " \n",
              " [3960 rows x 5 columns])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "error_evaluate(model_mtm_lora_finetune, tokenizer_mtm, test_data, \"/content/drive/MyDrive/CIS5800/Error/m2m_lora\", max_length=50, device=\"cuda\", bleu_threshold=0.1, model_type = \"m2m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AForYtL9NMwG"
      },
      "outputs": [],
      "source": [
        "del model_mtm_lora_finetune\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K12gQZseTFVQ"
      },
      "source": [
        "### **5.4 Fine-Tune M2M with Layer Freezing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "01aWpe-iTOh5",
        "outputId": "0324eec2-4e17-41be-b796-40776c46695f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-23-80796311a631>:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5289' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5289/7500 33:28 < 13:59, 2.63 it/s, Epoch 2.12/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.863800</td>\n",
              "      <td>2.681758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.653900</td>\n",
              "      <td>2.632754</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7500/7500 47:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.863800</td>\n",
              "      <td>2.681758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.653900</td>\n",
              "      <td>2.632754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.507400</td>\n",
              "      <td>2.625175</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7500, training_loss=2.690735758463542, metrics={'train_runtime': 2855.0678, 'train_samples_per_second': 42.031, 'train_steps_per_second': 2.627, 'total_flos': 5079151411200000.0, 'train_loss': 2.690735758463542, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "model_mtm_freeze_finetune = copy.deepcopy(model_mtm)\n",
        "\n",
        "# Layer Freezing Fine-tuning with memory monitoring\n",
        "\n",
        "# Initialize monitoring thread\n",
        "# log_file = \"/content/drive/MyDrive/CIS5800/Log/m2m/Education/memory_usage_freeze.log\"\n",
        "# analysis_output_file = \"/content/drive/MyDrive/CIS5800/Log/m2m/Education/memory_analysis_freeze.csv\"\n",
        "\n",
        "# log_dir = os.path.dirname(log_file)\n",
        "# if not os.path.exists(log_dir):\n",
        "#     os.makedirs(log_dir)\n",
        "\n",
        "freeze_encoder_layers = 8\n",
        "freeze_decoder_layers = 8\n",
        "\n",
        "# Apply Layer Freezing\n",
        "for name, param in model_mtm_freeze_finetune.named_parameters():\n",
        "    # Freeze encoder layers\n",
        "    if \"encoder.layers\" in name:\n",
        "        layer_num = int(name.split(\".\")[3])  # Extract the layer index\n",
        "        if layer_num < freeze_encoder_layers:\n",
        "            param.requires_grad = False\n",
        "    # Freeze decoder layers\n",
        "    elif \"decoder.layers\" in name:\n",
        "        layer_num = int(name.split(\".\")[3])  # Extract the layer index\n",
        "        if layer_num < freeze_decoder_layers:\n",
        "            param.requires_grad = False\n",
        "    else:\n",
        "        param.requires_grad = True  # Keep other parameters trainable\n",
        "\n",
        "\n",
        "model_mtm_freeze_finetune.config.pad_token_id = tokenizer_mtm.pad_token_id # tokenizer as the config for tokenizer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "        remove_unused_columns=False,\n",
        "        logging_steps=100,\n",
        "        save_steps=500,\n",
        "        save_total_limit=2,\n",
        "        save_strategy=\"no\",\n",
        "        report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_mtm_freeze_finetune,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_data_mtm_train,\n",
        "    eval_dataset=tokenized_data_mtm_dev,\n",
        "    tokenizer=tokenizer_mtm,\n",
        "    # callbacks=[MemoryUsageLogger(log_file)]\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Analyze logs\n",
        "# analyze_logs(log_file, analysis_output_file)\n",
        "\n",
        "# # Indicate completion\n",
        "# print(\"LoRA fine-tuning completed. Memory usage logged in:\", log_file)\n",
        "# print(\"Top memory usage analysis saved in:\", analysis_output_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9lvB5JzT9SW"
      },
      "outputs": [],
      "source": [
        "average_bleu = evaluate_bleu_score(model=model_mtm_freeze_finetune, tokenizer=tokenizer_mtm, test_data=test_data, model_type = \"m2m\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "error_evaluate(model_mtm_freeze_finetune, tokenizer_mtm, test_data, \"/content/drive/MyDrive/CIS5800/Error/m2m_freeze\", max_length=50, device=\"cuda\", bleu_threshold=0.1, model_type = \"m2m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbRLc-GN1i1S",
        "outputId": "28ce313f-deb4-422e-e226-2570bbe09d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating BLEU: 100%|██████████| 5000/5000 [29:06<00:00,  2.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All Results:\n",
            "                                          chinese_input  \\\n",
            "0                                 支架10的上端安装在车内空间中的天花板上。   \n",
            "1                          一个环境法律和政策中心的代表说，风能类似的基准已经工作。   \n",
            "2                          你呢，迈克? 你认为我们应该把CP21告诉凯先生 吗 ?   \n",
            "3             以前他直接从中国进口货物，但是封锁开始后，他只得求助于走私犯并且得多付40%的钱。   \n",
            "4                                      有几位可就拉长了脸瞧着会场门口。   \n",
            "...                                                 ...   \n",
            "4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
            "4996                            例如，您可能安装了服务包，但没有安装处理器包。   \n",
            "4997  实际上，没有一个善于思考的人会拒绝接受他们的第一个观点：即情感生活的巨大变化要求表达方式也随...   \n",
            "4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
            "4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "0     The upper end of a stay 10 is mounted on a cei...   \n",
            "1     An Environmental Law and Policy Center represe...   \n",
            "2     What about you, Mike? Do you think we should t...   \n",
            "3     But since the blockade began, he has had to re...   \n",
            "4     Some pulled a long face as they glanced at the...   \n",
            "...                                                 ...   \n",
            "4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
            "4996  For example, you may have installed a service ...   \n",
            "4997  All the same, no thinking man can refuse to ac...   \n",
            "4998  Contrary to some stereotypes, Americans don't ...   \n",
            "4999  \"Do you know ,\" went on the father, \"that it's...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "0     The upper end of the shelf 10 is installed on ...       0.197     True  \n",
            "1     A representative of the Center for Environment...       0.177     True  \n",
            "2     What about you, Mike? Do you think we should t...       0.682     True  \n",
            "3     Before he had imported goods directly from Chi...       0.109     True  \n",
            "4     Several of them looked at the door of the meet...       0.040    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4995  Change fuel cost to fuel cost by adding a spac...       0.034    False  \n",
            "4996  For example, you may have installed a Service ...       0.426     True  \n",
            "4997  In fact, none of those who think well will ref...       0.146     True  \n",
            "4998  Americans, unlike the typical people, are not ...       0.014    False  \n",
            "4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
            "\n",
            "[5000 rows x 5 columns]\n",
            "\n",
            "Error Examples:\n",
            "                                          chinese_input  \\\n",
            "4                                      有几位可就拉长了脸瞧着会场门口。   \n",
            "5     中国抗战八年，她还算侥幸，自始至终，鬼子都不曾去惹她，假如也像马来亚沦陷了若干年月，我不知她...   \n",
            "7                              由于水位不断上升，随都能看到搜救被困居民的场景。   \n",
            "8                               梯形导向浮阀塔板具有良好的流体力学和传质性能。   \n",
            "10                            这些新观点、新命题，对党史研究有着重大的指导作用。   \n",
            "...                                                 ...   \n",
            "4993                 保险公司也是获益者，他们可以通过更好的途径获得最新资料用于保险精算。   \n",
            "4994                              至少有14人死于这场暴雨，上千人流离失所。   \n",
            "4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
            "4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
            "4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
            "\n",
            "                                           ground_truth  \\\n",
            "4     Some pulled a long face as they glanced at the...   \n",
            "5     In China's eight years of fighting a war, she ...   \n",
            "7     Scenes like this one have been playing out all...   \n",
            "8     The trapezoidal directed valve trays have exce...   \n",
            "10    These new point of views, new proposition, to ...   \n",
            "...                                                 ...   \n",
            "4993  Insurance companies benefit fromthe informatio...   \n",
            "4994  At least 1deaths are being blamed on the massi...   \n",
            "4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
            "4998  Contrary to some stereotypes, Americans don't ...   \n",
            "4999  \"Do you know ,\" went on the father, \"that it's...   \n",
            "\n",
            "                                             prediction  bleu_score  correct  \n",
            "4     Several of them looked at the door of the meet...       0.040    False  \n",
            "5     After eight years of China's resistance, she w...       0.005    False  \n",
            "7     As the water level is rising, there is a scene...       0.010    False  \n",
            "8     The staircase-oriented floating valve panel ha...       0.021    False  \n",
            "10    These new points of view and new arguments hav...       0.031    False  \n",
            "...                                                 ...         ...      ...  \n",
            "4993  Insurance companies are also profitable, they ...       0.024    False  \n",
            "4994  At least 14 people were killed in the storm an...       0.031    False  \n",
            "4995  Change fuel cost to fuel cost by adding a spac...       0.034    False  \n",
            "4998  Americans, unlike the typical people, are not ...       0.014    False  \n",
            "4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
            "\n",
            "[3813 rows x 5 columns]\n",
            "Error examples saved to '/content/drive/MyDrive/CIS5800/Error/m2m_freeze'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                          chinese_input  \\\n",
              " 0                                 支架10的上端安装在车内空间中的天花板上。   \n",
              " 1                          一个环境法律和政策中心的代表说，风能类似的基准已经工作。   \n",
              " 2                          你呢，迈克? 你认为我们应该把CP21告诉凯先生 吗 ?   \n",
              " 3             以前他直接从中国进口货物，但是封锁开始后，他只得求助于走私犯并且得多付40%的钱。   \n",
              " 4                                      有几位可就拉长了脸瞧着会场门口。   \n",
              " ...                                                 ...   \n",
              " 4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
              " 4996                            例如，您可能安装了服务包，但没有安装处理器包。   \n",
              " 4997  实际上，没有一个善于思考的人会拒绝接受他们的第一个观点：即情感生活的巨大变化要求表达方式也随...   \n",
              " 4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
              " 4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 0     The upper end of a stay 10 is mounted on a cei...   \n",
              " 1     An Environmental Law and Policy Center represe...   \n",
              " 2     What about you, Mike? Do you think we should t...   \n",
              " 3     But since the blockade began, he has had to re...   \n",
              " 4     Some pulled a long face as they glanced at the...   \n",
              " ...                                                 ...   \n",
              " 4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
              " 4996  For example, you may have installed a service ...   \n",
              " 4997  All the same, no thinking man can refuse to ac...   \n",
              " 4998  Contrary to some stereotypes, Americans don't ...   \n",
              " 4999  \"Do you know ,\" went on the father, \"that it's...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 0     The upper end of the shelf 10 is installed on ...       0.197     True  \n",
              " 1     A representative of the Center for Environment...       0.177     True  \n",
              " 2     What about you, Mike? Do you think we should t...       0.682     True  \n",
              " 3     Before he had imported goods directly from Chi...       0.109     True  \n",
              " 4     Several of them looked at the door of the meet...       0.040    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4995  Change fuel cost to fuel cost by adding a spac...       0.034    False  \n",
              " 4996  For example, you may have installed a Service ...       0.426     True  \n",
              " 4997  In fact, none of those who think well will ref...       0.146     True  \n",
              " 4998  Americans, unlike the typical people, are not ...       0.014    False  \n",
              " 4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
              " \n",
              " [5000 rows x 5 columns],\n",
              "                                           chinese_input  \\\n",
              " 4                                      有几位可就拉长了脸瞧着会场门口。   \n",
              " 5     中国抗战八年，她还算侥幸，自始至终，鬼子都不曾去惹她，假如也像马来亚沦陷了若干年月，我不知她...   \n",
              " 7                              由于水位不断上升，随都能看到搜救被困居民的场景。   \n",
              " 8                               梯形导向浮阀塔板具有良好的流体力学和传质性能。   \n",
              " 10                            这些新观点、新命题，对党史研究有着重大的指导作用。   \n",
              " ...                                                 ...   \n",
              " 4993                 保险公司也是获益者，他们可以通过更好的途径获得最新资料用于保险精算。   \n",
              " 4994                              至少有14人死于这场暴雨，上千人流离失所。   \n",
              " 4995                        把燃料成本改变为燃料成本，通过在词组中间添加一个空格。   \n",
              " 4998               美国人与一般人所认定的典型不同，他们并不会到处拥抱、亲吻遇见的每一个人。   \n",
              " 4999                      “你们知道，”父亲接着说，“在这鬼窝窝洞里，冷得象狗一样。   \n",
              " \n",
              "                                            ground_truth  \\\n",
              " 4     Some pulled a long face as they glanced at the...   \n",
              " 5     In China's eight years of fighting a war, she ...   \n",
              " 7     Scenes like this one have been playing out all...   \n",
              " 8     The trapezoidal directed valve trays have exce...   \n",
              " 10    These new point of views, new proposition, to ...   \n",
              " ...                                                 ...   \n",
              " 4993  Insurance companies benefit fromthe informatio...   \n",
              " 4994  At least 1deaths are being blamed on the massi...   \n",
              " 4995  Change FuelCost to Fuel Cost by placing a spac...   \n",
              " 4998  Contrary to some stereotypes, Americans don't ...   \n",
              " 4999  \"Do you know ,\" went on the father, \"that it's...   \n",
              " \n",
              "                                              prediction  bleu_score  correct  \n",
              " 4     Several of them looked at the door of the meet...       0.040    False  \n",
              " 5     After eight years of China's resistance, she w...       0.005    False  \n",
              " 7     As the water level is rising, there is a scene...       0.010    False  \n",
              " 8     The staircase-oriented floating valve panel ha...       0.021    False  \n",
              " 10    These new points of view and new arguments hav...       0.031    False  \n",
              " ...                                                 ...         ...      ...  \n",
              " 4993  Insurance companies are also profitable, they ...       0.024    False  \n",
              " 4994  At least 14 people were killed in the storm an...       0.031    False  \n",
              " 4995  Change fuel cost to fuel cost by adding a spac...       0.034    False  \n",
              " 4998  Americans, unlike the typical people, are not ...       0.014    False  \n",
              " 4999  \"You know,\" said the father, \"in this hole, it...       0.031    False  \n",
              " \n",
              " [3813 rows x 5 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rHhSwSQNmkR"
      },
      "outputs": [],
      "source": [
        "del model_mtm_freeze_finetune\n",
        "del tokenized_data_mtm_train\n",
        "del tokenized_data_mtm_dev\n",
        "del tokenized_data_mtm_test\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.synchronize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTTh0H4P9kr8"
      },
      "source": [
        "## **6. Parameters and GPU Usage Comparison**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR1e2lboBO6s"
      },
      "source": [
        "### **6.1 Parameter Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf_jVfe0ppWk",
        "outputId": "fa670d1f-549a-4910-ba56-8379ada7cb84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mBART (original) - Total Parameters: 610879488\n",
            "mBART (original) - Trainable Parameters (All): 610879488\n",
            "mBART (original) - Percentage of Parameters to Fine-tune (All): 100.00%\n",
            "\n",
            "mBART (LoRA) - Total Parameters: 612059136\n",
            "mBART (LoRA) - Trainable Parameters (All): 76750848\n",
            "mBART (LoRA) - Percentage of Parameters to Fine-tune (All): 12.54%\n",
            "\n",
            "mBART (Layer Freezing) - Total Parameters: 610879488\n",
            "mBART (Layer Freezing) - Trainable Parameters (All): 375736320\n",
            "mBART (Layer Freezing) - Percentage of Parameters to Fine-tune (All): 61.51%\n",
            "\n",
            "MTM (original) - Total Parameters: 483905536\n",
            "MTM (original) - Trainable Parameters (All): 483905536\n",
            "MTM (original) - Percentage of Parameters to Fine-tune (All): 100.00%\n",
            "\n",
            "MTM with LoRA - Total Parameters: 485085184\n",
            "MTM with LoRA - Trainable Parameters (All): 76750848\n",
            "MTM with LoRA - Percentage of Parameters to Fine-tune (All): 15.82%\n",
            "\n",
            "MTM with Layer Freezing - Total Parameters: 483905536\n",
            "MTM with Layer Freezing - Trainable Parameters (All): 248762368\n",
            "MTM with Layer Freezing - Percentage of Parameters to Fine-tune (All): 51.41%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display parameter summaries for the original and LoRA-enhanced models\n",
        "print_model_parameters(model_mbart, \"mBART (original)\")\n",
        "print_model_parameters(model_mbart_lora, \"mBART (LoRA)\")\n",
        "print_model_parameters(model_mbart_freeze, \"mBART (Layer Freezing)\")\n",
        "\n",
        "# Display parameter summaries for the original and LoRA-enhanced models\n",
        "print_model_parameters(model_mtm, \"MTM (original)\")\n",
        "print_model_parameters(model_mtm_lora, \"MTM with LoRA\")\n",
        "print_model_parameters(model_mtm_freeze, \"MTM with Layer Freezing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFdq9O0OpzW_"
      },
      "source": [
        "### **6.2 GPU Usage Comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVA2oSlDi49D"
      },
      "outputs": [],
      "source": [
        "def plot_memory_usage(normal_file, lora_file, freeze_file, model=\"mbart\"):\n",
        "    \"\"\"\n",
        "    Plots separate graphs comparing GPU and RAM consumption of mBART normal vs mBART LoRA.\n",
        "\n",
        "    Parameters:\n",
        "    - normal_file: Path to the CSV file for mBART normal memory data.\n",
        "    - lora_file: Path to the CSV file for mBART LoRA memory data.\n",
        "    - freeze_file: Path to the CSV file for mBART layer freezing memory data.\n",
        "    - model: Model name to include in the title (default is \"mbart\").\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Load the data\n",
        "    normal_data = pd.read_csv(normal_file)\n",
        "    lora_data = pd.read_csv(lora_file)\n",
        "    freeze_data = pd.read_csv(freeze_file)\n",
        "\n",
        "    # Convert the 'Timestamp' column to datetime for proper plotting\n",
        "    normal_data['Timestamp'] = pd.to_datetime(normal_data['Timestamp'])\n",
        "    lora_data['Timestamp'] = pd.to_datetime(lora_data['Timestamp'])\n",
        "    freeze_data['Timestamp'] = pd.to_datetime(freeze_data['Timestamp'])\n",
        "\n",
        "    # Find the minimum timestamp across all datasets\n",
        "    min_timestamp = min(\n",
        "        normal_data['Timestamp'].min(),\n",
        "        lora_data['Timestamp'].min(),\n",
        "        freeze_data['Timestamp'].min()\n",
        "    )\n",
        "\n",
        "    # Normalize timestamps by subtracting the minimum timestamp\n",
        "    normal_data['Normalized Timestamp'] = (normal_data['Timestamp'] - min_timestamp).dt.total_seconds()\n",
        "    lora_data['Normalized Timestamp'] = (lora_data['Timestamp'] - min_timestamp).dt.total_seconds()\n",
        "    freeze_data['Normalized Timestamp'] = (freeze_data['Timestamp'] - min_timestamp).dt.total_seconds()\n",
        "\n",
        "    # Reset starting timestamp to 0 for all datasets\n",
        "    normal_data['Normalized Timestamp'] -= normal_data['Normalized Timestamp'].min()\n",
        "    lora_data['Normalized Timestamp'] -= lora_data['Normalized Timestamp'].min()\n",
        "    freeze_data['Normalized Timestamp'] -= freeze_data['Normalized Timestamp'].min()\n",
        "\n",
        "    # Plot GPU consumption comparison\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(normal_data['Normalized Timestamp'], normal_data['GPU Memory Usage (GB)'], label='mBART Normal GPU', marker='o')\n",
        "    plt.plot(lora_data['Normalized Timestamp'], lora_data['GPU Memory Usage (GB)'], label='mBART LoRA GPU', marker='x')\n",
        "    plt.plot(freeze_data['Normalized Timestamp'], freeze_data['GPU Memory Usage (GB)'], label='mBART Freeze GPU', marker='*')\n",
        "\n",
        "    title = 'GPU Consumption: {} Normal vs LoRA vs Layer Freezing'.format(model.upper())\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Time (seconds since start)')\n",
        "    plt.ylabel('GPU Memory Usage (GB)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JtM0WW6qi57h",
        "outputId": "2381fc20-414f-4718-df03-e1333490349a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# # Plot for mBart\n",
        "plot_memory_usage(\"/content/drive/MyDrive/CIS5800/Log/mBART/Education/memory_usage.log\",\n",
        "                  \"/content/drive/MyDrive/CIS5800/Log/mBART/Education/memory_usage_lora.log\",\n",
        "                  \"/content/drive/MyDrive/CIS5800/Log/mBART/Education/memory_usage_freeze.log\",\n",
        "                  \"mabrt\")\n",
        "\n",
        "# Plot for m2m\n",
        "plot_memory_usage(\"/content/drive/MyDrive/CIS5800/Log/m2m/Education/memory_usage.log\",\n",
        "                  \"/content/drive/MyDrive/CIS5800/Log/m2m/Education/memory_usage_lora.log\",\n",
        "                  \"/content/drive/MyDrive/CIS5800/Log/m2m/Education/memory_usage_freeze.log\",\n",
        "                  \"mtm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtXP9sOcIg8v"
      },
      "source": [
        "## **7. Extra Exploration: Quantized General-Purpose LLMs and Language-Specific Models**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVx1PC4QKrTH"
      },
      "source": [
        "### **7.1 Pre-Quantized Llamma**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anMFp7x8O7_T",
        "outputId": "43aa8b3d-e320-47f2-ecd8-95f4ebc50d5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at TheBloke/Chinese-Alpaca-2-7B-GPTQ were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n",
            "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "*** Generate:\n",
            "<s> Below is a Chinese sentence. Translate it into English.\n",
            "\n",
            "### Instruction:\n",
            "我爱机器学习\n",
            "\n",
            "### Response:\n",
            "\n",
            "I love machine learning.\n"
          ]
        }
      ],
      "source": [
        "model_name_or_path = \"TheBloke/Chinese-Alpaca-2-7B-GPTQ\"\n",
        "\n",
        "Alpaca = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "Alpaca_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
        "\n",
        "prompt = \"我爱机器学习\"\n",
        "prompt_template=f'''Below is a Chinese sentence. Translate it into English.\n",
        "\n",
        "### Instruction:\n",
        "{prompt}\n",
        "\n",
        "### Response:\n",
        "\n",
        "'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = Alpaca_tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
        "output = Alpaca.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
        "print(Alpaca_tokenizer.decode(output[0]).replace('</s>', ''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjqH35RoLgi-"
      },
      "outputs": [],
      "source": [
        "train_data, dev_data, test_data = load_data_from_txt(data_file_path, sample_size = 50000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Define the model name or path\n",
        "model_name_or_path = \"TheBloke/Chinese-Alpaca-2-7B-GPTQ\"\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    device_map=\"auto\",             # Automatically map layers to available devices\n",
        "    trust_remote_code=True         # Enable remote code if required\n",
        ")\n",
        "\n",
        "# Calculate the number of trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Number of trainable parameters: {trainable_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393,
          "referenced_widgets": [
            "a0910d362d8b45f89af1328f8234a333",
            "399ca725676e493dbbbc2183469eef71",
            "99eee8b9427c450b94eebd322dda9c4e",
            "0fef70d6edc94457bf993cfb4f48bc15",
            "83d741784ef24ae68ad82fb116161417",
            "52bb02281b7f442394f46fcf3a31bc1e",
            "bb85cc5a1fd946f895319f9f34fdc53f",
            "cb5222a9a2f24821bda905493337bd6c",
            "4bd7940e3cd5428785a43ab56f24d8cd",
            "760b7843ece64cb5a64adb6f6f159903",
            "fd2edc248e6847739c45db2331292b26",
            "7a900508396146568e9ee6a713f4b1dc",
            "e5e277a73af34af9bae2274c77aca311",
            "98401dc2a6b7495cbf45e5a2bc58fac0",
            "f83079666ca44480a90a32c69b4c7759",
            "1f7269d700eb45418693479c1b30699c",
            "caaf549bfea840a987fb3e1dd48121d1",
            "1e17b9a93c504e69ade7d3663cab0dc6",
            "5d0458270a864812a260a4a1d091fc92",
            "03a57e79ec544e0e8b32f573d7ecd6f3",
            "f7a1e05d78f541ccbc564bc8cded152f",
            "5ed8d0d7aca34d2d840df9c34de22a7f",
            "0a5d96f62d434580bc8bb8410ce0e6b4",
            "d4446a0fa8d740ceaf8c5f22e34d0c1b",
            "341926154e764eeaafa7c5cc6cdb9912",
            "5df486804e6b4b39bbacaa2b4c0ab12c",
            "6d45526b106945629730eb5c451d3fb3",
            "318c10876d3147eb993f1041b6815507",
            "404b7dce98f840719695cada46afbfc1",
            "150987c5f9d543fd86c961b8c9ce2d4c",
            "c5457173c4d4436c8f3f7867cf3adfcf",
            "a880e430cc5a48d4a9836505c8c1efff",
            "5b091887407e4ca099cc714ffc72f4f3"
          ]
        },
        "id": "HO9OH12N6Zqb",
        "outputId": "3a44e836-2781-4d7e-f0b4-83e50e83f5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/924 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0910d362d8b45f89af1328f8234a333"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda:CUDA extension not installed.\n",
            "WARNING:auto_gptq.nn_modules.qlinear.qlinear_cuda_old:CUDA extension not installed.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.28G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a900508396146568e9ee6a713f4b1dc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:5055: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
            "  warnings.warn(\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "Some weights of the model checkpoint at TheBloke/Chinese-Alpaca-2-7B-GPTQ were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.o_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias']\n",
            "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a5d96f62d434580bc8bb8410ce0e6b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 453251072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx78qCBbYGaD",
        "outputId": "2d05dc8c-530f-4b74-e599-cd64923b15f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Scores: 100%|██████████| 5000/5000 [10:06:58<00:00,  7.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score for the test set: 0.0511\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.051076716055391054"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_bleu_score(Alpaca, Alpaca_tokenizer, test_data, max_length=200, device=\"cuda\", Alpaca = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiAXxQ4RKzQE"
      },
      "source": [
        "### **7.2 MarianMT - Chinese to English Only**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "7d1d73cd809f4f81891339d9b91963b5",
            "725b6dd3fa434e2d910d6e6b2c3f5c02",
            "abcba4d582564f03b09b1434b9d22ace",
            "0f5afbcc40b2460fbee5d7b5fe3cee41",
            "9a5cfc3706da4734b8829e9eeda18453",
            "c5adf56850fb4611b974f970bd46f1ba",
            "1a87084fa0834b4f974b7d91f854538e",
            "69608abae16346349058c52e3ceb8418",
            "85b6388873814ad69ff0fde892767b82",
            "8892b402a972410191c7c58409f6f421",
            "30f68ea6a55a46dab0c9089eae2952ad"
          ]
        },
        "id": "qMtMZnD4lMEP",
        "outputId": "31306479-b259-426b-8247-6a725c59734d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d1d73cd809f4f81891339d9b91963b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/312M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I love machine learning. I love natural language.\n"
          ]
        }
      ],
      "source": [
        "model_id = \"Helsinki-NLP/opus-mt-zh-en\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "MarianMT_tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "MarianMT_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "input_text = \"我爱机器学习，我爱自然语言处理\"\n",
        "inputs = MarianMT_tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = MarianMT_model.generate(**inputs, max_length=50)\n",
        "\n",
        "translated_text = MarianMT_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(translated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50Hy4JmehylJ"
      },
      "source": [
        "**Trainable Parameters for Self-Quantized Mariana MT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPvpIhH5hwbq",
        "outputId": "b678467a-1d70-49f5-cdcb-b1f910560ecd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 77419008\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Load the model\n",
        "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Count the number of trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Number of trainable parameters: {trainable_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_fyijSHpK3p",
        "outputId": "d18be8da-048d-438f-84bd-172b16d81409"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded train data with 40000 samples.\n",
            "Loaded dev data with 5000 samples.\n",
            "Loaded test data with 5000 samples.\n"
          ]
        }
      ],
      "source": [
        "train_data, dev_data, test_data = load_data_from_txt(data_file_path, sample_size = 50000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHNDsbTcpYY6",
        "outputId": "1842e090-9f28-4245-ec6d-219a5db5e563"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Calculating BLEU Scores: 100%|██████████| 5000/5000 [1:20:03<00:00,  1.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average BLEU score for the test set: 0.1029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.10293217807613292"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_bleu_score(MarianMT_model, MarianMT_tokenizer, test_data, max_length=200, device=\"cuda\", Alpaca = False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "prKdB5wTwPme",
        "XklaqI9z8qAy",
        "GXhU2VXTwiwH",
        "ozU3DRKlwtH1",
        "QhCijy_CqYHV",
        "_KdUmj4Ayhkd",
        "uMZQWk2lttXo",
        "-4Urv1Ng5VIN",
        "GILSJNwtBxpr",
        "qx6APgdyXlgH",
        "2VmUXr3R0z3J",
        "dVG55jwJkbtn",
        "mJR8LtahC5m2",
        "ikWMcrgpZT2b",
        "CUAae6juqj7c",
        "EtbitlEbDHdJ",
        "5Sv8OTsV4GXX",
        "Jsn31bI2zpkF",
        "F_GMPXVmiZj6",
        "MvhxsNrG4-4o",
        "n0heTFBaBoyX",
        "6Wuhnfza0boW",
        "9TKJqLOrebzH",
        "POk2iB0V5ULw",
        "K12gQZseTFVQ",
        "qR1e2lboBO6s"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "002770a697294cd2a1e62ad9ebb56f0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0946552c0e9e4d1e8a7dceecbfeebd66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ca9d1a2be2945fe8492088c529f21c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_536d1b905a8b4e328125bfa37aee69c7",
              "IPY_MODEL_25c10fbc8a504e50b6e8859ebe172753",
              "IPY_MODEL_dd57a84a69f149a1ba7ab9feb8d66c3a"
            ],
            "layout": "IPY_MODEL_3491cd83bc924065a32138c4af6f8a6c"
          }
        },
        "0ef3082589fe48e9b5115130f8d8170e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f5afbcc40b2460fbee5d7b5fe3cee41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8892b402a972410191c7c58409f6f421",
            "placeholder": "​",
            "style": "IPY_MODEL_30f68ea6a55a46dab0c9089eae2952ad",
            "value": " 312M/312M [00:01&lt;00:00, 238MB/s]"
          }
        },
        "110a246bcdf74da598f3b58bdadb33b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11f06f842c274672aec268b7b8a1bde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "171cced3c611442784354d9f94f5c278": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17a750d1c77c41609b83822f8297f161": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1893af19547248e5a5c1d8f3263502ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5df9ea3969824c11b6b80583197649c0",
            "placeholder": "​",
            "style": "IPY_MODEL_c29bf138bef446e6bc23bf794ad7e97f",
            "value": "config.json: 100%"
          }
        },
        "1a87084fa0834b4f974b7d91f854538e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b0d3f7633c54f99b2ef3ba24deeae98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e417de88dc94e84a224b85173154e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f48780158e314db5a2f1a0a54751f034",
              "IPY_MODEL_8576425312624e3681aa02e18744428d",
              "IPY_MODEL_a278515e304e439e96873a80009734af"
            ],
            "layout": "IPY_MODEL_9e011d5071cb4136bd358fc6eb4036f9"
          }
        },
        "237bd9946efd4341a6e737f90619415a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8932ac5b234b49b9b16ce7387312d1ca",
            "placeholder": "​",
            "style": "IPY_MODEL_2de78c4b2daf4434b269beb08fedf2b8",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "25c10fbc8a504e50b6e8859ebe172753": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_913c744b886b433ea1ad7c7718b82972",
            "max": 261,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5467128fefd748b7b9c8aebba9b64a16",
            "value": 261
          }
        },
        "293eec9edcad4b02880f2367ca370f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b0da7e5d0ad4af6bae9757dec680f7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c37188e2ad64e4f800b7fade3946d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2de78c4b2daf4434b269beb08fedf2b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30f68ea6a55a46dab0c9089eae2952ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3491cd83bc924065a32138c4af6f8a6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "457d792d667e4e07a5e0b59090b4daaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_110a246bcdf74da598f3b58bdadb33b1",
            "max": 1429,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91f81fe060674705b1d06053069c84a1",
            "value": 1429
          }
        },
        "48608fc7fd874e84a7f09b6eaa1f03d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5068215f603144bf992b9646ce7e7f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1893af19547248e5a5c1d8f3263502ec",
              "IPY_MODEL_457d792d667e4e07a5e0b59090b4daaf",
              "IPY_MODEL_e9cf9a5ccbc543b49417ff8b3908b811"
            ],
            "layout": "IPY_MODEL_0946552c0e9e4d1e8a7dceecbfeebd66"
          }
        },
        "5239a3204f50437ebb9660e77889d360": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9935c83b26de424bb8f16a38514fc549",
            "placeholder": "​",
            "style": "IPY_MODEL_ec0fc12c1e46475fa8db9a2e9079a793",
            "value": " 529/529 [00:00&lt;00:00, 32.8kB/s]"
          }
        },
        "536d1b905a8b4e328125bfa37aee69c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b0da7e5d0ad4af6bae9757dec680f7e",
            "placeholder": "​",
            "style": "IPY_MODEL_e0904ce568ea43939edcea1745630a51",
            "value": "generation_config.json: 100%"
          }
        },
        "5467128fefd748b7b9c8aebba9b64a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bac5314524242719b10b66c56285fb7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5df9ea3969824c11b6b80583197649c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f4d477e70b04a458c9920401aaa4baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62aa05a2bf9c4d00966c87e6d4a04bbe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6418a89bb07449d3986fb0985d0b0d91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64ba765e64f4444cadcada2c98c37424": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69608abae16346349058c52e3ceb8418": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c598d02efab4b74a9eebdfbfc0c77cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "725b6dd3fa434e2d910d6e6b2c3f5c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5adf56850fb4611b974f970bd46f1ba",
            "placeholder": "​",
            "style": "IPY_MODEL_1a87084fa0834b4f974b7d91f854538e",
            "value": "model.safetensors: 100%"
          }
        },
        "7d1d73cd809f4f81891339d9b91963b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_725b6dd3fa434e2d910d6e6b2c3f5c02",
              "IPY_MODEL_abcba4d582564f03b09b1434b9d22ace",
              "IPY_MODEL_0f5afbcc40b2460fbee5d7b5fe3cee41"
            ],
            "layout": "IPY_MODEL_9a5cfc3706da4734b8829e9eeda18453"
          }
        },
        "8442e1702c114f16abb5b2d2bf87c33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8576425312624e3681aa02e18744428d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db51b11fe06c4234b0773f4e354b00b4",
            "max": 2444578688,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aca03c1006944492bebbb331c64c1a41",
            "value": 2444578688
          }
        },
        "85b6388873814ad69ff0fde892767b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85fd8a9632fb4e8294cdfea09c3894ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87e418e793af4804b4de3b745521baa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c085a6ca4c134ca7a141ed9d17c26583",
            "placeholder": "​",
            "style": "IPY_MODEL_11f06f842c274672aec268b7b8a1bde3",
            "value": " 649/649 [00:00&lt;00:00, 54.6kB/s]"
          }
        },
        "8892b402a972410191c7c58409f6f421": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8932ac5b234b49b9b16ce7387312d1ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "913c744b886b433ea1ad7c7718b82972": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f81fe060674705b1d06053069c84a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "940ba0154c42408a95c031dfdedc6fc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b1436bb01a074f5ea26d88d1f3f8ea99",
              "IPY_MODEL_b97c38b90de242979b0f58f63302cbd4",
              "IPY_MODEL_af7df096521e4df89427ead7b509db3c"
            ],
            "layout": "IPY_MODEL_5bac5314524242719b10b66c56285fb7"
          }
        },
        "9935c83b26de424bb8f16a38514fc549": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a5cfc3706da4734b8829e9eeda18453": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9af86396852a41bdb9af03cbc5e6abae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b310f903a104c0689a057b53a1884aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64ba765e64f4444cadcada2c98c37424",
            "placeholder": "​",
            "style": "IPY_MODEL_e128e776b30a485b97bbec619a18edf4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "9e011d5071cb4136bd358fc6eb4036f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a278515e304e439e96873a80009734af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb037255f7ad4907962ebb141696eefe",
            "placeholder": "​",
            "style": "IPY_MODEL_293eec9edcad4b02880f2367ca370f3d",
            "value": " 2.44G/2.44G [00:11&lt;00:00, 215MB/s]"
          }
        },
        "a9bfe5364ca0430390a15f6d8241b333": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abcba4d582564f03b09b1434b9d22ace": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69608abae16346349058c52e3ceb8418",
            "max": 312062468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85b6388873814ad69ff0fde892767b82",
            "value": 312062468
          }
        },
        "aca03c1006944492bebbb331c64c1a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af7df096521e4df89427ead7b509db3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002770a697294cd2a1e62ad9ebb56f0f",
            "placeholder": "​",
            "style": "IPY_MODEL_f33c8631b8a541c496a0cd096c8583a5",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 18.5MB/s]"
          }
        },
        "b1436bb01a074f5ea26d88d1f3f8ea99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48608fc7fd874e84a7f09b6eaa1f03d3",
            "placeholder": "​",
            "style": "IPY_MODEL_2c37188e2ad64e4f800b7fade3946d76",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "b97c38b90de242979b0f58f63302cbd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a750d1c77c41609b83822f8297f161",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f4d477e70b04a458c9920401aaa4baf",
            "value": 5069051
          }
        },
        "c085a6ca4c134ca7a141ed9d17c26583": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1e2ca8d05984c42bef78914d63a0b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c29bf138bef446e6bc23bf794ad7e97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5adf56850fb4611b974f970bd46f1ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2e19031aa804962a444cdf147a61337": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b310f903a104c0689a057b53a1884aa",
              "IPY_MODEL_fe730cae5da84d12ae15061c7dfc6dce",
              "IPY_MODEL_87e418e793af4804b4de3b745521baa9"
            ],
            "layout": "IPY_MODEL_6c598d02efab4b74a9eebdfbfc0c77cf"
          }
        },
        "db51b11fe06c4234b0773f4e354b00b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd57a84a69f149a1ba7ab9feb8d66c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171cced3c611442784354d9f94f5c278",
            "placeholder": "​",
            "style": "IPY_MODEL_1b0d3f7633c54f99b2ef3ba24deeae98",
            "value": " 261/261 [00:00&lt;00:00, 23.8kB/s]"
          }
        },
        "e0904ce568ea43939edcea1745630a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e128e776b30a485b97bbec619a18edf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9cf9a5ccbc543b49417ff8b3908b811": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85fd8a9632fb4e8294cdfea09c3894ab",
            "placeholder": "​",
            "style": "IPY_MODEL_8442e1702c114f16abb5b2d2bf87c33f",
            "value": " 1.43k/1.43k [00:00&lt;00:00, 81.2kB/s]"
          }
        },
        "eb037255f7ad4907962ebb141696eefe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec0fc12c1e46475fa8db9a2e9079a793": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f16c2010a4cc4f078d864e06a29b2705": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f33c8631b8a541c496a0cd096c8583a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f48780158e314db5a2f1a0a54751f034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9af86396852a41bdb9af03cbc5e6abae",
            "placeholder": "​",
            "style": "IPY_MODEL_a9bfe5364ca0430390a15f6d8241b333",
            "value": "model.safetensors: 100%"
          }
        },
        "fe730cae5da84d12ae15061c7dfc6dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62aa05a2bf9c4d00966c87e6d4a04bbe",
            "max": 649,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c1e2ca8d05984c42bef78914d63a0b58",
            "value": 649
          }
        },
        "ff7fc8fad35e4bc4b5af6a66b02eeab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6418a89bb07449d3986fb0985d0b0d91",
            "max": 529,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f16c2010a4cc4f078d864e06a29b2705",
            "value": 529
          }
        },
        "ff9faaa4372d40beaa2a0d0379718a0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_237bd9946efd4341a6e737f90619415a",
              "IPY_MODEL_ff7fc8fad35e4bc4b5af6a66b02eeab0",
              "IPY_MODEL_5239a3204f50437ebb9660e77889d360"
            ],
            "layout": "IPY_MODEL_0ef3082589fe48e9b5115130f8d8170e"
          }
        },
        "a0910d362d8b45f89af1328f8234a333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_399ca725676e493dbbbc2183469eef71",
              "IPY_MODEL_99eee8b9427c450b94eebd322dda9c4e",
              "IPY_MODEL_0fef70d6edc94457bf993cfb4f48bc15"
            ],
            "layout": "IPY_MODEL_83d741784ef24ae68ad82fb116161417"
          }
        },
        "399ca725676e493dbbbc2183469eef71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52bb02281b7f442394f46fcf3a31bc1e",
            "placeholder": "​",
            "style": "IPY_MODEL_bb85cc5a1fd946f895319f9f34fdc53f",
            "value": "config.json: 100%"
          }
        },
        "99eee8b9427c450b94eebd322dda9c4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb5222a9a2f24821bda905493337bd6c",
            "max": 924,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bd7940e3cd5428785a43ab56f24d8cd",
            "value": 924
          }
        },
        "0fef70d6edc94457bf993cfb4f48bc15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_760b7843ece64cb5a64adb6f6f159903",
            "placeholder": "​",
            "style": "IPY_MODEL_fd2edc248e6847739c45db2331292b26",
            "value": " 924/924 [00:00&lt;00:00, 62.5kB/s]"
          }
        },
        "83d741784ef24ae68ad82fb116161417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52bb02281b7f442394f46fcf3a31bc1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb85cc5a1fd946f895319f9f34fdc53f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb5222a9a2f24821bda905493337bd6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bd7940e3cd5428785a43ab56f24d8cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "760b7843ece64cb5a64adb6f6f159903": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2edc248e6847739c45db2331292b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a900508396146568e9ee6a713f4b1dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5e277a73af34af9bae2274c77aca311",
              "IPY_MODEL_98401dc2a6b7495cbf45e5a2bc58fac0",
              "IPY_MODEL_f83079666ca44480a90a32c69b4c7759"
            ],
            "layout": "IPY_MODEL_1f7269d700eb45418693479c1b30699c"
          }
        },
        "e5e277a73af34af9bae2274c77aca311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caaf549bfea840a987fb3e1dd48121d1",
            "placeholder": "​",
            "style": "IPY_MODEL_1e17b9a93c504e69ade7d3663cab0dc6",
            "value": "model.safetensors: 100%"
          }
        },
        "98401dc2a6b7495cbf45e5a2bc58fac0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d0458270a864812a260a4a1d091fc92",
            "max": 4278396392,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_03a57e79ec544e0e8b32f573d7ecd6f3",
            "value": 4278396392
          }
        },
        "f83079666ca44480a90a32c69b4c7759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7a1e05d78f541ccbc564bc8cded152f",
            "placeholder": "​",
            "style": "IPY_MODEL_5ed8d0d7aca34d2d840df9c34de22a7f",
            "value": " 4.28G/4.28G [00:26&lt;00:00, 137MB/s]"
          }
        },
        "1f7269d700eb45418693479c1b30699c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caaf549bfea840a987fb3e1dd48121d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e17b9a93c504e69ade7d3663cab0dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d0458270a864812a260a4a1d091fc92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a57e79ec544e0e8b32f573d7ecd6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7a1e05d78f541ccbc564bc8cded152f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ed8d0d7aca34d2d840df9c34de22a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a5d96f62d434580bc8bb8410ce0e6b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4446a0fa8d740ceaf8c5f22e34d0c1b",
              "IPY_MODEL_341926154e764eeaafa7c5cc6cdb9912",
              "IPY_MODEL_5df486804e6b4b39bbacaa2b4c0ab12c"
            ],
            "layout": "IPY_MODEL_6d45526b106945629730eb5c451d3fb3"
          }
        },
        "d4446a0fa8d740ceaf8c5f22e34d0c1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_318c10876d3147eb993f1041b6815507",
            "placeholder": "​",
            "style": "IPY_MODEL_404b7dce98f840719695cada46afbfc1",
            "value": "generation_config.json: 100%"
          }
        },
        "341926154e764eeaafa7c5cc6cdb9912": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_150987c5f9d543fd86c961b8c9ce2d4c",
            "max": 131,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5457173c4d4436c8f3f7867cf3adfcf",
            "value": 131
          }
        },
        "5df486804e6b4b39bbacaa2b4c0ab12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a880e430cc5a48d4a9836505c8c1efff",
            "placeholder": "​",
            "style": "IPY_MODEL_5b091887407e4ca099cc714ffc72f4f3",
            "value": " 131/131 [00:00&lt;00:00, 9.85kB/s]"
          }
        },
        "6d45526b106945629730eb5c451d3fb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "318c10876d3147eb993f1041b6815507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "404b7dce98f840719695cada46afbfc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "150987c5f9d543fd86c961b8c9ce2d4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5457173c4d4436c8f3f7867cf3adfcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a880e430cc5a48d4a9836505c8c1efff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b091887407e4ca099cc714ffc72f4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}